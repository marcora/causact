---
title: "causact"
format:
  html:
    toc: true
    df-print: kable
---

## Introduction

There are three languages at the core of any data analysis: narrative, math, and code.

-   **Narrative**: This is the language of your real-world understanding.

-   **Math**: This is the language describing a faithful mathematical representation of your real-world narrative whose solution or output might turn your data into data-driven insight.

-   **Code**: This is the programming language, the code required to generate output that exactly or approximately solves your math problem or generates the desired output.

Using Bayesian inference is the provably best method of combining data with domain knowledge to extract interpretable and insightful results that lead us towards better outcomes.

This is the job of the *business analyst*; to drive better outcomes by compelling actions that are aligned with strategy and informed by data.

![](images/clipboard-2076919182.png)

![](images/clipboard-613416536.png)

The business analyst workflow consists of transforming the real-world into a compact mathematical representation that we can use, along with data, to computationally search for insights. These computational insights must then be translated back into the real-world in such a way that they inspire action which leads to improved outcomes. This is your role: transform the real-world into the math/computer world, extract mathematical/statistical/computational insight, then transform that insight into a compelling real-world call for action.

![](images/clipboard-3434935492.png)

## Representing uncertainty

[Real-world uncertainty]{.smallcaps} makes decision making hard. Conversely, without uncertainty decisions would be easier. For example, if a cafe knew exactly 10 customers would want a bagel, then they could make exactly 10 bagels in the morning; if a factory’s machine could make each part exactly the same, then quality control to ensure the part’s fit could be eliminated; if a customer’s future ability to pay back a loan was known, then a bank’s decision to underwrite the loan would be quite simple.

In this section, we learn to represent our real-world uncertainty (e.g. demand for bagels, quality of a manufacturing process , or risk of loan default) in mathematical and computational terms. We start by defining the ideal mathematical way of representing uncertainty, namely by assigning a *probability distribution* to a *random variable*. Subsequently, we learn to describe our uncertainty in a *random variable* using *representative samples* as a pragmatic computational proxy to this mathematical ideal.

Think of a random variable as a mapping from potential outcomes to numerical values representing the probability we assign to each outcome.

| Outcome | Probability |
|---------|-------------|
| Heads   | 0.5         |
| Tails   | 0.5         |

While the table might be adequate to describe the mapping of coin flip outcomes to probability, as we make more complex models of the real-world, we will want to take advantage of the concise (and often terse) notation that mathematicians would use. In addition, we want to gain fluency in *math world* notation so that we can successfully traverse the bridge between *real world* and *math world*.

Above, a random variable was introduced as a mapping of outcomes to probabilities. And, this is how you should think of it most of the time. However, to start gaining fluency in the math-world definition of a random variable, we will also view this mapping process as not just one mapping, but rather a sequence of two mappings: 1) the first mapping is actually the “true” probability-theory definition of a **random variable** - it maps all possible outcomes to real numbers, and 2) the second mapping, known as a **probability distribution** in probability theory, maps the numbers from the first mapping to real numbers representing how plausibility is allocated across all possible outcomes - we often think of this allocation as assigning probability to each outcome.

The terse math-world representation of a mapping process like this is denoted:

$$
X:\Omega \rightarrow \mathbb{R}
$$

, where you interpret it as “random variable $X$ maps each possible outcome in sample space omega to a real number.”

The second mapping process then assigns a probability distribution to the random variable. By convention, lowercase letters, e.g. $x$, represent actual observed outcomes. We call $x$ the *realization* of random variable $X$ and define the mapping of outcomes to probability for every $x \in X$ (read as $x$ “in” $X$ and interpret it to mean “for each possible realization of random variable $X$”).

In this book, we will use $f$, to denote a function that maps each possible realization of a random variable to its corresponding plausibilty measure and use a subscript to disambiguate which random variable is being referred to when necessary. For the coin flip example:

$$
f_X: X \rightarrow [0,1]
$$

Despite all this fancy notation, for small problems it is sometimes the best course of action to think of a random variable as a lookup table as shown here:

| Outcome | Realization ($x$) | $f(x)$ |
|---------|-------------------|--------|
| Heads   | 1                 | 0.5    |
| Tails   | 0                 | 0.5    |

and where $f(x)$ can be interpreted as the plausability assigned to random variable $X$ taking on the value $x$.

The Bernoulli distribution (or a Bernoulli random variable):

| Outcome | Realization ($x$) | $f(x)$  |
|---------|-------------------|---------|
| Success | 1                 | $\pi$   |
| Failure | 0                 | $1-\pi$ |

With all of this background, we are now equipped to model uncertainty in any observable data that has two outcomes. The way we will represent this uncertainty is using two forms: 1) a graphical model and 2) a statistical model.

```{r setup}
#| output: false
library(tidyverse)
library(ggformula)
library(mosaic)
library(posterior)
library(bayesplot)
library(causact) # causact::install_causact_deps()

theme_set(theme_minimal())

set.seed(123)
```

```{r}
dag_create() |>
  dag_node(
    descr = "Coin flip",
    label = "X") |>
  dag_render(shortLabel = TRUE)
```

$$
\begin{aligned}X &\equiv \textrm{Coin flip outcome with heads}=1 \textrm{  and tails}=0.\\X &\sim \textrm{Bernoulli}(p)\end{aligned}
$$

Instead of working with probability distributions directly as *mathematical* objects, we will most often seek a representative sample and treat them as *computational* objects (i.e. **data**). For modelling a coin flip, the representative sample might simply be a list of $0$ and $1$ generated by someone flipping a coin or by a computer simulating someone flipping a coin.

[Turning a mathematical object into a representative sample using R]{.smallcaps} is quite easy as R can be used to generate random outcomes from just about all well-known probability distributions.

```{r}
rbern(7, p = 0.5)
```

```{r}
set.seed(123) 

# Create dataframe of coinflip observations
numFlips = 50 ## flip the coin 50 times
df = data.frame(
  flipNum = 1:numFlips,
  coinFlip = rbern(n=numFlips,prob=0.5)
  ) %>%
  mutate(headsProportion = cummean(coinFlip))
  
# Plot results
ggplot(df, aes(x = flipNum, y = headsProportion)) + 
  geom_point() +
  geom_line() + 
  geom_hline(yintercept = 0.5, color = "red") +
  ggtitle("Running Proportion of Heads") +
  xlab("Flip Number") + 
  ylab("Proportion of Heads") +
  ylim(c(0,1))
```

$$
\begin{aligned}X_i &\equiv \textrm{If passenger } i \textrm{ shows up, then } X=1 \textrm{. Otherwise, } X = 0 \textrm{. Note: } i \in \{1,2,3\}.\\X_i &\sim \textrm{Bernoulli}(p = 0.85)\\Y &\equiv \textrm{Total number of passengers that show up.}\\Y &= X_1 + X_2 + X_3\end{aligned}
$$

```{r}
numFlights = 1000 ## number of simulated flights
probShow = 0.85 ## probability of passenger showing up

# choose random seed so others can
# replicate results
set.seed(111) 

pass1 = rbern(n = numFlights, prob = probShow)
pass2 = rbern(n = numFlights, prob = probShow)
pass3 = rbern(n = numFlights, prob = probShow)

# create data frame (use tibble to from tidyverse) 
flightDF = tibble(
  simNum = 1:numFlights,
  totalPassengers = pass1 + pass2 + pass3
)

# transform data to give proportion
propDF = flightDF %>% 
  group_by(totalPassengers) %>% 
  summarize(numObserved = n()) %>%
  mutate(proportion = numObserved / sum(numObserved))

# plot data with estimates
ggplot(propDF, 
       aes(x = totalPassengers, y = proportion)) +
  geom_col() +
  geom_text(aes(label = proportion), nudge_y = 0.03)
```

Simulation will always be your friend in the sense that if given enough time, a simulation will always give you results that approximate mathematical exactness. The only problem with this friend is it is sometimes slow to yield representative results. In these cases, sometimes mathematics provides a shortcut. The shortcut we study here is to define a probability distibution.

$$
\begin{aligned}Y &\equiv \textrm{Total number of passengers that show up.}\\Y &\sim \textrm{Binomial}(n = 3, p = 0.85)\end{aligned}
$$

```{r}
# transform data to give proportion
propExactDF = tibble(totalPassengers = 0:3) %>%
  mutate(proportion = 
           dbinom(x = totalPassengers,
                  size = 3,
                  prob = 0.85))

# plot data with estimates
ggplot(propExactDF, aes(x = totalPassengers, 
                        y = proportion)) +
  geom_col() +
  geom_text(aes(label = proportion), 
            nudge_y = 0.03)
```

The above code is both simpler and faster than the approximation code run earlier. In addition, it gives exact results. Hence, when we can take mathematical shortcuts, we will to save time and reduce the uncertainty in our results introduced by approximation error.

Our representation of uncertainty takes place in three worlds: 1) the real-world - we use graphical models (i.e. ovals) to convey the story of uncertainty, 2) the math-world - we use statistical models to rigorously define how random outcomes are generated, and 3) the computation-world - we use R functions to answer questions about exact distributions and representative samples to answer questions when the exact distribution is unobtainable.

## Joint distributions tell you everything

The most complete method of reasoning about sets of random variables is by having a *joint probability distribution*. A joint probability distribution, , assigns a probability value to all possible assignments or realizations of sets of random variables. The goal of this section is to 1) introduce you to the notation of joint probability distributions and 2) convince you that if you are given a joint probability distribution, then you would be able to answer some very useful questions using probability.

### Joint distributions

### Marginal distributions

### Conditional distributions

### Limitations of joint distributions

[Why don’t we just use joint probability distributions all the time?]{.smallcaps} Despite the expressive power of having a joint probability distribution, they are not that easy to directly construct due to the *curse of dimensionality*. As the number of random variables being considered in a dataset grows, the number of potential probability assignments grows too. Even in the era of big data, this curse of dimensionality still exists.Generally speaking, an exponential increase is required in the size of the dataset as each new descriptive feature is added.

Let’s assume we have $n$ random variables with each having $k$ values. Thus, the joint distribution requires $k^n$ probabilites. Even if $k=2$ and $n=34$, this leads to 17,179,869,184 possibilities (over 17 billion). To make this concrete, a typical car purchase decision might easily look at 34 different variables (e.g. make, model, color, style, financing, etc.). So, to model this decision would require a very large joint distribution which actually dwarfs the amount of data that is available. As a point of comparison, well under 100 million motor vehicles were sold worldwide in 2019 - i.e. less than one data point per possible combination of features. Despite this “curse”, we will learn to get around it with more compact representations of joint distributions. These representations will require less data, but will still yield the power to answer queries of interest; just as if we had access to the full joint distribution.

## Graphical models tell joint distribution stories

Graphical models serve as compact representations of joint probability distributions.

It’s okay to not draw a perfect model when you are first attacking a problem.

```{mermaid}
graph TD
    X[ X: It is raining ]
    Y[ Y: The curb is wet ]
    X --> Y
```

The diagram above is what mathematicians and computer scientists call a *graph*. To them, a *graph* is a set of related objects where the objects are called *nodes* and any pair of related nodes are known as an *edge*. For our purposes, a *node* is a visual depiction of a random variable (i.e. an oval) and the presence of a visible *edge* indicates a relationship between the connected random variables.

Our first vetting should be to ensure that nodes can be modelled as a random variable with each node representing a mapping of real-world outcomes to numerical values (see the “Representing uncertainty” section above).

Any variable we include in our models needs to be clearly defined and fulfill the requirements of a random variable; afterall, this is our key technique for representing the *real-world* in mathematical terms.

Mathematically, our goal is to have a joint distribution over the random variables of interest. Once we have that, we can answer any probability query we might have using marginal and conditional distributions (see the “Joint distributions tell you everything” section above).

For all but very simple models like the one above, a tabular representation of a joint distribution becomes unmanageable (computationally, cognitively, and statistically).

To overcome this, we use a different, more-compact structure - called a *Bayesian Network* (BN). Bayesian networks are compressed, easier-to-specify recipes for generating a full joint distribution. So, what is a BN? It is a type of *graph* (i.e. nodes and edges), specifically a *directed acyclic graph* (DAG), with the following requirements:

1.  All nodes represent random variables. They are drawn using ovals. By convention, a constant is also considered a random variable.

2.  All edges are pairs of random variables with a specified direction of ordering. By convention, the direction is from parent node to child node. While not always true, it is usually good to have edges reflecting a causal ordering. Edges are drawn using arrows where the tail originates from the parent and the tip points to a child.

3.  Edges are uni-directional, they must only flow in one direction between any two nodes (i.e. *directed*).

4.  The graph is *acyclic* meaning there are no cycles. In other words, if you were to put your finger on any node and trace a path following the direction of the edges, you would not be able to return to the starting node.

5.  Any child node’s probability distribution will be expressed as conditional solely on its parents’ values, i.e. $P(\text{child}\mid \text{parent(s)})$; this assumption is what enables more compact representations of joint distributions.

The one edge, $Y \rightarrow X$, means that our uncertainty in $X$ is measured using a probability function conditional on its parent - i.e. $P(X|Y)$. Since there are no edges leading into $Y$, it has no parents, its probability distribution is $P(Y)$ . With those two probabilities in place, we can recover the joint distribution, $P(X,Y)$, based on the definition of conditional probability $P(X,Y) = P(Y) \times P(X|Y)$.

If there are no edges between $X$ and $Y$, then the joint distribution is recovered via $P(X,Y)=P(X)\cdot P(Y)$. In this case, the two random variables are independent and that is not a model structure consistent with the scenario we are exploring. If the ordering of nodes for $X$ and $Y$ are reversed such that $Y$ is the parent of $X$, then the joint distribution would be recovered via $P(X,Y)=P(Y)\cdot P(X\mid Y)$. This math is not consistent with the story we are trying to tell. The story is that rain causes the curb to be wet, not the other way around.

The general rule for a probabilistic graphical model is that its joint distribution can be factored using the chain rule formula for DAGs where $P(X_1, X_2, \ldots, X_n) = \prod_I P(X_i|Parents(X_i))$. Hence, to model any one random variable, we only have to model its relationship to its parents instead of modelling its relationship to all of the random variables included in the model.

See [mathematicalmonk](www.youtube.com/@mathematicalmonk)’s youtube video on how joint distributions are compactly represented using DAGs here: <https://youtu.be/3XysEf3IQN4>.

## Bayesian inference on graphical models

*Plausible reasoning* - an allocation of credibility/plausibility to all possible explanations of the observations.

Plausible reasoning in the math world is called Bayesian inference - a methodology for updating the credibility/plausibility of the various explanations in our mathematical representation of the world as more data becomes available.

Let’s walk through a hypothetical argument between two scientists as to the effect of a new drug on cognition in Alzheimer's disease (AD).

Assume the two scientists have competing models for what effect the drug will have on the cognition of patients with AD:

1.  *The Optimist Model (Model1)* : This model is from a scientist who is very optimistic about the effect of the drug and argues that 70% of all patients will see at least a 5% increase in cognition.

2.  *The Pessimist Model (Model2)* : This model is from a scientist who is very pessimistic about the effect of the drug and argues that 20% of all patients will see at least a 5% increase in cognition.

These two scientists recognize and respect their differing opinions - they agree to test the new drug in one patient. They hire you as a data analyst and ask you to make the decision as to whose model is more credible in light of the test’s results. Your job is to allocate credibility to the two competing models both before seeing the test results and after seeing the test results. Initially, you might not have any reason to favor one model over another, but as data is collected, your belief in whose model is more plausible/believable/credible will change. For example, if cognition decreases, then the pessimist model would seem more credible. Your task is to allocate credibility (using probability theory) to the various models/explanations, before and after the data becomes available.

### Building a graphical model of the real-world

The first step is to create a graphical model/representation of the real world.

Starting simple, *let’s only imagine that we test the drug in one patient* and our single data point (i.e. whether cognition increases by at least 5% or not) follows a Bernoulli distribution.

The graphical model is simply:

```{r}
dag_create() |>
  dag_node(
    descr = "Cognition increases",
    label = "X") |>
  dag_render(shortLabel = TRUE)
```

And, the statistical model with the mathematical details is represented like this:

$$
\begin{aligned}X \equiv& \textrm{ Cognition increases: } \\        & \textrm{ If cognition increases more than 5}\% \textrm{, then }X=1 \textrm{ otherwise, }X=0.\\X \sim  & \textrm{ Bernoulli}(\theta)\\\end{aligned}
$$

We have seen this model before when representing coin flips. Our data is analogous to heads or tails of a coin flip. The data will be reduced to a zero or one for each patient. If given $\theta$, we could generate data using `rbern(n=1, prob=theta)`, but, we do not know $\theta$; the reason we are looking at data is to answer the question: “what is $\theta$?”

From the story above, $\theta$ can only take on two values. In the optimistic model $\theta = 70\%$ and in the pessimistic model $\theta = 20\%$. So, we have two models of the world and are uncertain as to which one is more plausible. Without data, we have no reason to believe one scientist over another, so $P(\theta=70\%)=50\%$ and $P(\theta=20\%)=50\%$ - i.e. each scientist is equally likely to be correct. This is just like saying $P(\text{model1})=P(\text{model2})=50\%$. Before any data is considered, this allocation of credibility assigning probability to all the models being considered is called the *prior*. The prior is the initial probability allocated among all the possible models.

See <https://youtu.be/nCRTuwCdmP0> for gaining some intuition about prior probabilities.

So now, we can more completely specify our data story using both a graphical and a statistical model with specified prior probabilities. The graphical model is now two ovals representing our uncertainty in the probability of success and the observed cognition increase (random variable math-labels for $\Theta$ and $X$ are included for extra clarity in connecting the graphical model and the statistical model):

```{r}
dag_create() |>
  dag_node(
    descr = "Cognition increases",
    label = "X") |>
  dag_node(
    descr = "Success probability",
    label = "Θ",
    child = "X") |>
  dag_render()
```

And, the statistical model is represented like this:

$$
\begin{aligned}
X \equiv& \textrm{ Cognition increases: } \\
        & \textrm{ If cognition increases more than 5} \% \textrm{, then }X=1 \textrm{ otherwise, }X=0.\\
X \sim  & \textrm{ Bernoulli}(\theta)\\
\Theta \equiv& \textrm{ Success probability: } \\
\end{aligned}
$$ $$
\Theta \sim
\begin{array}{ccc}
  \textrm{Outcome } & \textrm{ Realization }(\theta) & f(\theta) \\
  \hline
  \textrm{Model1}  & \textrm{   70}\% & \textrm{  50}\% \\
  \textrm{Model2}  & \textrm{   20}\% & \textrm{  50}\% \\
\end{array}
$$

where the prior probability distribution for $\Theta$ is given in tabular form.

The graphical model and statistical model are two different ways of representing the same story. The graphical model is more visual and intuitive, while the statistical model is more precise and mathematical. Both are useful for different purposes, but both represents a generative model, i.e., a recipe for simulating real-world data observations. In this case, following the top-down flow of the graphical model, the recipe is:

1.  Simulate a potential success probability by randomly picking, with equal probability, either the optimist or pessimist model.
2.  Simulate a drug’s success by using the probability from (1) and generating a Bernoulli random variable with that probability of success.

We can easily show how to simulate an observation by writing code for the recipe in R:

```{r}
# Step 1: Simulate a potential success probability
theta = sample(size = 1, x = c(0.7, 0.2), prob = c(0.5, 0.5))
# Step 2: Simulate a drug's success
x = rbern(n = 1, prob = theta)
theta
x
```

Much of your subsequent work will use this notion of generative models as recipes. You will 1) *create generative models* that serve as skeleton recipes - recipes with named probability distributions and **unknown** parameters - for how real-world data arises, and 2) *inform models with data* by reallocating plausibility to the recipe parameters that are most consistent with the data.

A guiding principle for creating good generative models is that the generated data should mimic data that you, as a data analyst, believe is plausible. If the generative model outputs implausible data in high frequency, then your model is not capturing the essence of the underlying data story; your modelling assumptions will need work. When the generative model seems correct in the absence of data, then data can feed the updating process which sensibly reallocates prior probability so that model parameters that tend to generate data similar to the observed data are deemed more plausible.

### From model to joint distribution

The graphical model shows that we have uncertainty about two related random variables: 1) *Success Probability* ($\Theta$) *Cognition Increases* ($X$). Our assumption - built into our prior - is that one of our two models, $\theta = 20\%$ or $\theta = 70\%$, is correct.

::: callout-note
This implicit assumption that one of the considered generative models is correct is sometimes referred to as the *small world* assumption. See this comparison by Richard McElreath of *small world* versus *large world* highlighting the implications of this assumption: <https://youtu.be/WFv2vS8ESkk>.
:::

We are also confident about what collecting data from one patient might yield: either a single *success* or a single *failure* in terms of the cognition increase. Prior to collecting data, there are four combinations of model and data that are potential truths. Each combination’s prior plausibility, represented by $a\%$, $b\%$, $c\%$ and $d\%$, are elements of the table:

$$
\begin{array}{cc|cc}         &                  & \textrm{  Possible}                & \textrm{Models  }                  \\         &                  & \theta = 70\% & \theta = 20\% \\         \hline\textrm{Possible} & \textit{Success} & a\%   & b\%                     \\\textrm{Data}     & \textit{Failure} & c\%   & d\%\end{array}
$$

```{r}
# Simulate many samples from the generative model
samples <- map(1:10000, function(x) { theta = sample(size = 1, x = c(0.7, 0.2), prob = c(0.5, 0.5)); x = rbinom(size=1, n=1, prob=theta); tibble(theta, x) }) |> list_rbind()

head(samples)
```

```{r}
# Calculate joint probability distribution before seeing the data, and the marginal of theta (prior)
joint <- samples %>%
  group_by(theta, x) %>%
  summarize(count = n(), .groups = "drop") %>%
  mutate(p = count / sum(count))

joint

joint |> 
  group_by(theta) |> 
  summarize(prior = sum(p))
```

```{r}
tally(~ x + theta, data = samples, format = 'proportion')
```

### Bayesian updating of the joint distribution

$$
P(Model | Data) = \frac{P(Data|Model) \times P(Model)}{P(Data)}
$$

The above formula is called *Bayes' Theorem* and it is the mathematical engine that drives Bayesian inference. It is a simple formula, but it is very powerful. It tells us how to update our beliefs about the plausibility of various models in light of new data. The formula has four components:

1.  $P(Model | Data)$: This is the *posterior* probability of the model given the data. It is what we are trying to compute. It tells us how plausible each model is after seeing the data.
2.  $P(Data|Model)$: This is the *likelihood* of the data given the model. It tells us how likely the observed data is under each model.
3.  $P(Model)$: This is the *prior* probability of the model.
4.  $P(Data)$: This is the *marginal likelihood* of the data. It is a normalizing constant that ensures the posterior probabilities sum to one.

To use Bayes' Theorem, we need to compute the likelihood of the data under each model. This is done using the probability distributions defined in our statistical model.

Assume we observe a success, i.e. the patient’s cognition increases by at least 5%. The likelihood of this data under each model is:

-   Under Model1 ($\theta = 70\%$): $P(X=1|\theta=70\%) = 0.7$
-   Under Model2 ($\theta = 20\%$): $P(X=1|\theta=20\%) = 0.2$

The marginal likelihood of the data is computed by summing the likelihoods weighted by the prior probabilities:

$$
P(Data) = P(X=1) = P(X=1|\theta=70\%) \times P(\theta=70\%) + P(X=1|\theta=20\%) \times P(\theta=20\%) = 0.7 \times 0.5 + 0.2 \times 0.5 = 0.45
$$ Now, we can plug these values into Bayes' Theorem to compute the posterior probabilities:

-   For Model1 ($\theta = 70\%$):

$$
P(\theta=70\%|X=1) = \frac{P(X=1|\theta=70\%) \times P(\theta=70\%)}{P(X=1)} = \frac{0.7 \times 0.5}{0.45} \approx 0.778
$$

-   For Model2 ($\theta = 20\%$):

$$
P(\theta=20\%|X=1) = \frac{P(X=1|\theta=20\%) \times P(\theta=20\%)}{P(X=1)} = \frac{0.2 \times 0.5}{0.45} \approx 0.222
$$

So, after observing a success, the posterior probabilities are approximately 77.8% for Model1 and 22.2% for Model2. This means that after seeing the data, we believe that Model1 is more plausible than Model2.

```{r}
# Calculate joint probability distribution after seeing the data (x=1), and the marginal of theta (posterior)
joint <- samples %>%
  filter(x == 1) %>%
  group_by(theta, x) %>%
  summarize(count = n(), .groups = "drop") %>%
  mutate(p = count / sum(count))

joint

joint |> 
  group_by(theta) |> 
  summarize(prior = sum(p))
```

## Generative DAGs and scientific and statistical models

Up to this point, we specified a prior joint distribution using information from both a graphical model and a statistical model. In this section, we combine the two model types, graphical models and statistical models, into one visualization called the generative DAG.

The advantage of this is that the generative DAG unites real-world measurements (e.g. cognition increases) with their math/computation world counterparts (e.g. $X \sim \textrm{Bernoulli}(\theta)$) without requiring the cognitive load of flipping back-and-forth between the graphical and statistical models.

Moreover, generative DAGs, when combined with data, enable Bayesian inference to be automated (yay! no more manual calculations using Bayes rule).

### Generative DAGs

To build a generative DAG, we combine a graphical model with a *statistical model* into a single unifying representation.

::: callout-note
A statistical model is a mathematically expressed recipe for creating a joint distribution of related random variables. Each variable is defined either using a probability distribution or a function of the other variables. Statistical model and generative DAG will be used interchangeably. The term statistical model is just the mathematically expressed recipe. Generative DAG is the graphically expressed recipe with an embedded statistical model.
:::

The objective in building generative DAGs is to capture how we might go about simulating real-world measurements using the mathematical language we’ve been learning. Good generative DAGs are recipes for simulating a real-world observation in a way that reflects our domain knowledge. Then, with a generative DAG and some actual data, we will ask the computer to do Bayesian inference for us and to output an updated joint distribution of the random variables in our DAG.

The creation of a generative DAG mimics the creation of graphical models with accompanying statistical models. Both the structure of the DAG and the definition of the random variables should reflect what a domain expert knows, or wants to assume, about the scientific narrative being investigated.

```{r}
dag_create() |>
  dag_node(
    descr = "Cognition increases",
    label = "X",
    rhs = bernoulli(theta)) |>
  dag_node(
    descr = "Success probability",
    label = "Θ",
    child = "X",
    rhs = uniform(0,1)) |>
  dag_render()
```

Notice, `theta` is now spelled out phonetically as opposed to using the actual Greek letter math symbol of $\theta$. While this need not be done when drawing a generative DAG by hand, we phonetically spell out the Greek letter $\theta$ as `theta` here because computer code and computer-generated DAG visuals lack easy support for the Greek alphabet. Additionally, and more importantly, we switch to using lowercase notation for both `x` and `theta` as we want to highlight that ovals in generative DAG models represent a single realization of the random variable. Later we will introduce the representation of more than one realization.

### Building generative DAGs

As we build these generative DAGs, it is often easiest to build and read them from bottom to top.

Every node will also have a mathematical line (e.g. `x ~ Bernoulli(theta)` ). The mathematical line will always follow a three-part structure of 1) Left-hand side, 2) Relationship Type, and 3) Right-hand side:

Each line of a statistical model follows one of two forms: or

-   **Left-hand side (LHS)**: a mathematical label for a realization of the node (e.g. $x$ or $y$ or whatever symbol/word you choose). The purpose of the label is to have a very short way of referring to a node. To that end, you cannot include spaces in this label and try to keep it at five letters or less.

-   **Relationship Type**: There are two types of relationships that can be specified:

1.  a *probabilistic* relationship, denoted by the $\sim$ symbol, where the LHS node’s value is determined in a way that has some inherent randomness, such as the outcome of a coin flip. Or,

    1.  a *deterministic* relationship denoted by an $=$ symbol. A deterministic relationship means the LHS node’s value is determined with zero uncertainty as a function of its inputs.

-   **Right-hand side (RHS)**: either a known probability distribution governing the node’s probabilistic outcome (e.g. $\textrm{Bernoulli}(\theta)$) or a mathematical function defining the node’s value (e.g. $x + y$ ). In both cases, any parameters/variables/inputs in the RHS must be represented by a parent node to the current node in your model.
