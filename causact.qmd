---
title: "A Business analyst’s introduction to business analytics (2e)"
author: "Adam Fleischhacker"
date: 2025-01-15
format:
  html:
    toc: true
    df-print: kable
---

```{r setup}
#| output: false
library(tidyverse)
library(ggformula)
library(mosaic)
library(posterior)
library(bayesplot)
library(causact) # causact::install_causact_deps()

theme_set(theme_minimal())

set.seed(123)
```

[https://www.causact.com/](https://www.causact.com/#welcome)

![](images/clipboard-340255815.png){width="50%"}

<http://www.amazon.com/dp/B0CFZMKRGX>

## Introduction

There are three languages at the core of any data analysis: narrative, math, and code.

-   **Narrative**: This is the language of your real-world understanding.

-   **Math**: This is the language describing a faithful mathematical representation of your real-world narrative whose solution or output might turn your data into data-driven insight.

-   **Code**: This is the programming language, the code required to generate output that exactly or approximately solves your math problem or generates the desired output.

Using Bayesian inference is the provably best method of combining data with domain knowledge to extract interpretable and insightful results that lead us towards better outcomes.

This is the job of the *business analyst*; to drive better outcomes by compelling actions that are aligned with strategy and informed by data.

![](images/clipboard-2076919182.png)

![](images/clipboard-613416536.png)

The business analyst workflow consists of transforming the real-world into a compact mathematical representation that we can use, along with data, to computationally search for insights. These computational insights must then be translated back into the real-world in such a way that they inspire action which leads to improved outcomes. This is your role: transform the real-world into the math/computer world, extract mathematical/statistical/computational insight, then transform that insight into a compelling real-world call for action.

![](images/clipboard-3434935492.png)

## Representing uncertainty

[Real-world uncertainty]{.smallcaps} makes decision making hard. Conversely, without uncertainty decisions would be easier. For example, if a cafe knew exactly 10 customers would want a bagel, then they could make exactly 10 bagels in the morning; if a factory’s machine could make each part exactly the same, then quality control to ensure the part’s fit could be eliminated; if a customer’s future ability to pay back a loan was known, then a bank’s decision to underwrite the loan would be quite simple.

In this section, we learn to represent our real-world uncertainty (e.g. demand for bagels, quality of a manufacturing process , or risk of loan default) in mathematical and computational terms. We start by defining the ideal mathematical way of representing uncertainty, namely by assigning a *probability distribution* to a *random variable*. Subsequently, we learn to describe our uncertainty in a *random variable* using *representative samples* as a pragmatic computational proxy to this mathematical ideal.

Think of a random variable as a mapping from potential outcomes to numerical values representing the probability we assign to each outcome.

| Outcome | Probability |
|---------|-------------|
| Heads   | 0.5         |
| Tails   | 0.5         |

While the table might be adequate to describe the mapping of coin flip outcomes to probability, as we make more complex models of the real-world, we will want to take advantage of the concise (and often terse) notation that mathematicians would use. In addition, we want to gain fluency in *math world* notation so that we can successfully traverse the bridge between *real world* and *math world*.

Above, a random variable was introduced as a mapping of outcomes to probabilities. And, this is how you should think of it most of the time. However, to start gaining fluency in the math-world definition of a random variable, we will also view this mapping process as not just one mapping, but rather a sequence of two mappings: 1) the first mapping is actually the “true” probability-theory definition of a **random variable** - it maps all possible outcomes to real numbers, and 2) the second mapping, known as a **probability distribution** in probability theory, maps the numbers from the first mapping to real numbers representing how plausibility is allocated across all possible outcomes - we often think of this allocation as assigning probability to each outcome.

The terse math-world representation of a mapping process like this is denoted:

$$
X:\Omega \rightarrow \mathbb{R}
$$

, where you interpret it as “random variable $X$ maps each possible outcome in sample space omega to a real number.”

The second mapping process then assigns a probability distribution to the random variable. By convention, lowercase letters, e.g. $x$, represent actual observed outcomes. We call $x$ the *realization* of random variable $X$ and define the mapping of outcomes to probability for every $x \in X$ (read as $x$ “in” $X$ and interpret it to mean “for each possible realization of random variable $X$”).

In this book, we will use $f$, to denote a function that maps each possible realization of a random variable to its corresponding plausibilty measure and use a subscript to disambiguate which random variable is being referred to when necessary. For the coin flip example:

$$
f_X: X \rightarrow [0,1]
$$

Despite all this fancy notation, for small problems it is sometimes the best course of action to think of a random variable as a lookup table as shown here:

| Outcome | Realization ($x$) | $f(x)$ |
|---------|-------------------|--------|
| Heads   | 1                 | 0.5    |
| Tails   | 0                 | 0.5    |

and where $f(x)$ can be interpreted as the plausability assigned to random variable $X$ taking on the value $x$.

The Bernoulli distribution (or a Bernoulli random variable):

| Outcome | Realization ($x$) | $f(x)$  |
|---------|-------------------|---------|
| Success | 1                 | $\pi$   |
| Failure | 0                 | $1-\pi$ |

With all of this background, we are now equipped to model uncertainty in any observable data that has two outcomes. The way we will represent this uncertainty is using two forms: 1) a graphical model and 2) a statistical model.

```{r}
dag_create() |>
  dag_node(
    descr = "Coin flip",
    label = "X") |>
  dag_render(shortLabel = TRUE)
```

$$
\begin{aligned}X &\equiv \textrm{Coin flip outcome with heads}=1 \textrm{  and tails}=0.\\X &\sim \textrm{Bernoulli}(p)\end{aligned}
$$

Instead of working with probability distributions directly as *mathematical* objects, we will most often seek a representative sample and treat them as *computational* objects (i.e. **data**). For modelling a coin flip, the representative sample might simply be a list of $0$ and $1$ generated by someone flipping a coin or by a computer simulating someone flipping a coin.

[Turning a mathematical object into a representative sample using R]{.smallcaps} is quite easy as R can be used to generate random outcomes from just about all well-known probability distributions.

```{r}
rbern(7, p = 0.5)
```

```{r}
set.seed(123) 

# Create dataframe of coinflip observations
numFlips = 50 ## flip the coin 50 times
df = data.frame(
  flipNum = 1:numFlips,
  coinFlip = rbern(n=numFlips,prob=0.5)
  ) |>
  mutate(headsProportion = cummean(coinFlip))
  
# Plot results
ggplot(df, aes(x = flipNum, y = headsProportion)) + 
  geom_point() +
  geom_line() + 
  geom_hline(yintercept = 0.5, color = "red") +
  ggtitle("Running Proportion of Heads") +
  xlab("Flip Number") + 
  ylab("Proportion of Heads") +
  ylim(c(0,1))
```

$$
\begin{aligned}X_i &\equiv \textrm{If passenger } i \textrm{ shows up, then } X=1 \textrm{. Otherwise, } X = 0 \textrm{. Note: } i \in \{1,2,3\}.\\X_i &\sim \textrm{Bernoulli}(p = 0.85)\\Y &\equiv \textrm{Total number of passengers that show up.}\\Y &= X_1 + X_2 + X_3\end{aligned}
$$

```{r}
numFlights = 1000 ## number of simulated flights
probShow = 0.85 ## probability of passenger showing up

# choose random seed so others can
# replicate results
set.seed(111) 

pass1 = rbern(n = numFlights, prob = probShow)
pass2 = rbern(n = numFlights, prob = probShow)
pass3 = rbern(n = numFlights, prob = probShow)

# create data frame (use tibble to from tidyverse) 
flightDF = tibble(
  simNum = 1:numFlights,
  totalPassengers = pass1 + pass2 + pass3
)

# transform data to give proportion
propDF = flightDF |> 
  group_by(totalPassengers) |> 
  summarize(numObserved = n()) |>
  mutate(proportion = numObserved / sum(numObserved))

# plot data with estimates
ggplot(propDF, 
       aes(x = totalPassengers, y = proportion)) +
  geom_col() +
  geom_text(aes(label = proportion), nudge_y = 0.03)
```

Simulation will always be your friend in the sense that if given enough time, a simulation will always give you results that approximate mathematical exactness. The only problem with this friend is it is sometimes slow to yield representative results. In these cases, sometimes mathematics provides a shortcut. The shortcut we study here is to define a probability distibution.

$$
\begin{aligned}Y &\equiv \textrm{Total number of passengers that show up.}\\Y &\sim \textrm{Binomial}(n = 3, p = 0.85)\end{aligned}
$$

```{r}
# transform data to give proportion
propExactDF = tibble(totalPassengers = 0:3) |>
  mutate(proportion = 
           dbinom(x = totalPassengers,
                  size = 3,
                  prob = 0.85))

# plot data with estimates
ggplot(propExactDF, aes(x = totalPassengers, 
                        y = proportion)) +
  geom_col() +
  geom_text(aes(label = proportion), 
            nudge_y = 0.03)
```

The above code is both simpler and faster than the approximation code run earlier. In addition, it gives exact results. Hence, when we can take mathematical shortcuts, we will to save time and reduce the uncertainty in our results introduced by approximation error.

Our representation of uncertainty takes place in three worlds: 1) the real-world - we use graphical models (i.e. ovals) to convey the story of uncertainty, 2) the math-world - we use statistical models to rigorously define how random outcomes are generated, and 3) the computation-world - we use R functions to answer questions about exact distributions and representative samples to answer questions when the exact distribution is unobtainable.

## Joint distributions tell you everything

The most complete method of reasoning about sets of random variables is by having a *joint probability distribution*. A joint probability distribution, , assigns a probability value to all possible assignments or realizations of sets of random variables. The goal of this section is to 1) introduce you to the notation of joint probability distributions and 2) convince you that if you are given a joint probability distribution, then you would be able to answer some very useful questions using probability.

### Joint distributions

### Marginal distributions

### Conditional distributions

### Limitations of joint distributions

[Why don’t we just use joint probability distributions all the time?]{.smallcaps} Despite the expressive power of having a joint probability distribution, they are not that easy to directly construct due to the *curse of dimensionality*. As the number of random variables being considered in a dataset grows, the number of potential probability assignments grows too. Even in the era of big data, this curse of dimensionality still exists.Generally speaking, an exponential increase is required in the size of the dataset as each new descriptive feature is added.

Let’s assume we have $n$ random variables with each having $k$ values. Thus, the joint distribution requires $k^n$ probabilites. Even if $k=2$ and $n=34$, this leads to 17,179,869,184 possibilities (over 17 billion). To make this concrete, a typical car purchase decision might easily look at 34 different variables (e.g. make, model, color, style, financing, etc.). So, to model this decision would require a very large joint distribution which actually dwarfs the amount of data that is available. As a point of comparison, well under 100 million motor vehicles were sold worldwide in 2019 - i.e. less than one data point per possible combination of features. Despite this “curse”, we will learn to get around it with more compact representations of joint distributions. These representations will require less data, but will still yield the power to answer queries of interest; just as if we had access to the full joint distribution.

## Graphical models tell joint distribution stories

Graphical models serve as compact representations of joint probability distributions.

It’s okay to not draw a perfect model when you are first attacking a problem.

```{mermaid}
graph TD
    X[ X: It is raining ]
    Y[ Y: The curb is wet ]
    X --> Y
```

The diagram above is what mathematicians and computer scientists call a *graph*. To them, a *graph* is a set of related objects where the objects are called *nodes* and any pair of related nodes are known as an *edge*. For our purposes, a *node* is a visual depiction of a random variable (i.e. an oval) and the presence of a visible *edge* indicates a relationship between the connected random variables.

Our first vetting should be to ensure that nodes can be modelled as a random variable with each node representing a mapping of real-world outcomes to numerical values (see the “Representing uncertainty” section above).

Any variable we include in our models needs to be clearly defined and fulfill the requirements of a random variable; afterall, this is our key technique for representing the *real-world* in mathematical terms.

Mathematically, our goal is to have a joint distribution over the random variables of interest. Once we have that, we can answer any probability query we might have using marginal and conditional distributions (see the “Joint distributions tell you everything” section above).

For all but very simple models like the one above, a tabular representation of a joint distribution becomes unmanageable (computationally, cognitively, and statistically).

To overcome this, we use a different, more-compact structure - called a *Bayesian Network* (BN). Bayesian networks are compressed, easier-to-specify recipes for generating a full joint distribution. So, what is a BN? It is a type of *graph* (i.e. nodes and edges), specifically a *directed acyclic graph* (DAG), with the following requirements:

1.  All nodes represent random variables. They are drawn using ovals. By convention, a constant is also considered a random variable.

2.  All edges are pairs of random variables with a specified direction of ordering. By convention, the direction is from parent node to child node. While not always true, it is usually good to have edges reflecting a causal ordering. Edges are drawn using arrows where the tail originates from the parent and the tip points to a child.

3.  Edges are uni-directional, they must only flow in one direction between any two nodes (i.e. *directed*).

4.  The graph is *acyclic* meaning there are no cycles. In other words, if you were to put your finger on any node and trace a path following the direction of the edges, you would not be able to return to the starting node.

5.  Any child node’s probability distribution will be expressed as conditional solely on its parents’ values, i.e. $P(\text{child}\mid \text{parent(s)})$; this assumption is what enables more compact representations of joint distributions.

The one edge, $Y \rightarrow X$, means that our uncertainty in $X$ is measured using a probability function conditional on its parent - i.e. $P(X|Y)$. Since there are no edges leading into $Y$, it has no parents, its probability distribution is $P(Y)$ . With those two probabilities in place, we can recover the joint distribution, $P(X,Y)$, based on the definition of conditional probability $P(X,Y) = P(Y) \times P(X|Y)$.

If there are no edges between $X$ and $Y$, then the joint distribution is recovered via $P(X,Y)=P(X)\cdot P(Y)$. In this case, the two random variables are independent and that is not a model structure consistent with the scenario we are exploring. If the ordering of nodes for $X$ and $Y$ are reversed such that $Y$ is the parent of $X$, then the joint distribution would be recovered via $P(X,Y)=P(Y)\cdot P(X\mid Y)$. This math is not consistent with the story we are trying to tell. The story is that rain causes the curb to be wet, not the other way around.

The general rule for a probabilistic graphical model is that its joint distribution can be factored using the chain rule formula for DAGs where $P(X_1, X_2, \ldots, X_n) = \prod_I P(X_i|Parents(X_i))$. Hence, to model any one random variable, we only have to model its relationship to its parents instead of modelling its relationship to all of the random variables included in the model.

See [mathematicalmonk](www.youtube.com/@mathematicalmonk)’s youtube video on how joint distributions are compactly represented using DAGs here: <https://youtu.be/3XysEf3IQN4>.

## Bayesian inference on graphical models

*Plausible reasoning* - an allocation of credibility/plausibility to all possible explanations of the observations.

Plausible reasoning in the math world is called Bayesian inference - a methodology for updating the credibility/plausibility of the various explanations in our mathematical representation of the world as more data becomes available.

Let’s walk through a hypothetical argument between two scientists as to the effect of a new drug on cognition in Alzheimer's disease (AD).

Assume the two scientists have competing models for what effect the drug will have on the cognition of patients with AD:

1.  *The Optimist Model (Model1)* : This model is from a scientist who is very optimistic about the effect of the drug and argues that 70% of all patients will see at least a 5% increase in cognition.

2.  *The Pessimist Model (Model2)* : This model is from a scientist who is very pessimistic about the effect of the drug and argues that 20% of all patients will see at least a 5% increase in cognition.

These two scientists recognize and respect their differing opinions - they agree to test the new drug in one patient. They hire you as a data analyst and ask you to make the decision as to whose model is more credible in light of the test’s results. Your job is to allocate credibility to the two competing models both before seeing the test results and after seeing the test results. Initially, you might not have any reason to favor one model over another, but as data is collected, your belief in whose model is more plausible/believable/credible will change. For example, if cognition decreases, then the pessimist model would seem more credible. Your task is to allocate credibility (using probability theory) to the various models/explanations, before and after the data becomes available.

### Building a graphical model of the real-world

The first step is to create a graphical model/representation of the real world.

Starting simple, *let’s only imagine that we test the drug in one patient* and our single data point (i.e. whether cognition increases by at least 5% or not) follows a Bernoulli distribution.

The graphical model is simply:

```{r}
dag_create() |>
  dag_node(
    descr = "Cognition increases",
    label = "X") |>
  dag_render(shortLabel = TRUE)
```

And, the statistical model with the mathematical details is represented like this:

$$
\begin{aligned}X \equiv& \textrm{ Cognition increases: } \\        & \textrm{ If cognition increases more than 5}\% \textrm{, then }X=1 \textrm{ otherwise, }X=0.\\X \sim  & \textrm{ Bernoulli}(\theta)\\\end{aligned}
$$

We have seen this model before when representing coin flips. Our data is analogous to heads or tails of a coin flip. The data will be reduced to a zero or one for each patient. If given $\theta$, we could generate data using `rbern(n=1, prob=theta)`, but, we do not know $\theta$; the reason we are looking at data is to answer the question: “what is $\theta$?”

From the story above, $\theta$ can only take on two values. In the optimistic model $\theta = 70\%$ and in the pessimistic model $\theta = 20\%$. So, we have two models of the world and are uncertain as to which one is more plausible. Without data, we have no reason to believe one scientist over another, so $P(\theta=70\%)=50\%$ and $P(\theta=20\%)=50\%$ - i.e. each scientist is equally likely to be correct. This is just like saying $P(\text{model1})=P(\text{model2})=50\%$. Before any data is considered, this allocation of credibility assigning probability to all the models being considered is called the *prior*. The prior is the initial probability allocated among all the possible models.

See <https://youtu.be/nCRTuwCdmP0> for gaining some intuition about prior probabilities.

So now, we can more completely specify our data story using both a graphical and a statistical model with specified prior probabilities. The graphical model is now two ovals representing our uncertainty in the probability of success and the observed cognition increase (random variable math-labels for $\Theta$ and $X$ are included for extra clarity in connecting the graphical model and the statistical model):

```{r}
dag_create() |>
  dag_node(
    descr = "Cognition increases",
    label = "X") |>
  dag_node(
    descr = "Success probability",
    label = "Θ",
    child = "X") |>
  dag_render()
```

And, the statistical model is represented like this:

$$
\begin{aligned}
X \equiv& \textrm{ Cognition increases: } \\
        & \textrm{ If cognition increases more than 5} \% \textrm{, then }X=1 \textrm{ otherwise, }X=0.\\
X \sim  & \textrm{ Bernoulli}(\theta)\\
\Theta \equiv& \textrm{ Success probability: } \\
\end{aligned}
$$ $$
\Theta \sim
\begin{array}{ccc}
  \textrm{Outcome } & \textrm{ Realization }(\theta) & f(\theta) \\
  \hline
  \textrm{Model1}  & \textrm{   70}\% & \textrm{  50}\% \\
  \textrm{Model2}  & \textrm{   20}\% & \textrm{  50}\% \\
\end{array}
$$

where the prior probability distribution for $\Theta$ is given in tabular form.

The graphical model and statistical model are two different ways of representing the same story. The graphical model is more visual and intuitive, while the statistical model is more precise and mathematical. Both are useful for different purposes, but both represents a generative model, i.e., a recipe for simulating real-world data observations. In this case, following the top-down flow of the graphical model, the recipe is:

1.  Simulate a potential success probability by randomly picking, with equal probability, either the optimist or pessimist model.
2.  Simulate a drug’s success by using the probability from (1) and generating a Bernoulli random variable with that probability of success.

We can easily show how to simulate an observation by writing code for the recipe in R:

```{r}
# Step 1: Simulate a potential success probability
theta = sample(size = 1, x = c(0.7, 0.2), prob = c(0.5, 0.5))
# Step 2: Simulate a drug's success
x = rbern(n = 1, prob = theta)
theta
x
```

Much of your subsequent work will use this notion of generative models as recipes. You will 1) *create generative models* that serve as skeleton recipes - recipes with named probability distributions and **unknown** parameters - for how real-world data arises, and 2) *inform models with data* by reallocating plausibility to the recipe parameters that are most consistent with the data.

A guiding principle for creating good generative models is that the generated data should mimic data that you, as a data analyst, believe is plausible. If the generative model outputs implausible data in high frequency, then your model is not capturing the essence of the underlying data story; your modelling assumptions will need work. When the generative model seems correct in the absence of data, then data can feed the updating process which sensibly reallocates prior probability so that model parameters that tend to generate data similar to the observed data are deemed more plausible.

### From model to joint distribution

The graphical model shows that we have uncertainty about two related random variables: 1) *Success Probability* ($\Theta$) *Cognition Increases* ($X$). Our assumption - built into our prior - is that one of our two models, $\theta = 20\%$ or $\theta = 70\%$, is correct.

::: callout-note
This implicit assumption that one of the considered generative models is correct is sometimes referred to as the *small world* assumption. See this comparison by Richard McElreath of *small world* versus *large world* highlighting the implications of this assumption: <https://youtu.be/WFv2vS8ESkk>.
:::

We are also confident about what collecting data from one patient might yield: either a single *success* or a single *failure* in terms of the cognition increase. Prior to collecting data, there are four combinations of model and data that are potential truths. Each combination’s prior plausibility, represented by $a\%$, $b\%$, $c\%$ and $d\%$, are elements of the table:

$$
\begin{array}{cc|cc}         &                  & \textrm{  Possible}                & \textrm{Models  }                  \\         &                  & \theta = 70\% & \theta = 20\% \\         \hline\textrm{Possible} & \textit{Success} & a\%   & b\%                     \\\textrm{Data}     & \textit{Failure} & c\%   & d\%\end{array}
$$

```{r}
# Simulate many samples from the generative model
samples <- map(1:10000, function(x) { theta = sample(size = 1, x = c(0.7, 0.2), prob = c(0.5, 0.5)); x = rbinom(size=1, n=1, prob=theta); tibble(theta, x) }) |> list_rbind()

head(samples)
```

```{r}
# Calculate joint probability distribution before seeing the data, and the marginal of theta (prior)
joint <- samples |>
  group_by(theta, x) |>
  summarize(count = n(), .groups = "drop") |>
  mutate(p = count / sum(count))

joint

joint |> 
  group_by(theta) |> 
  summarize(prior = sum(p))
```

```{r}
tally(~ x + theta, data = samples, format = 'proportion')
```

### Bayesian updating of the joint distribution

$$
P(Model | Data) = \frac{P(Data|Model) \times P(Model)}{P(Data)}
$$

The above formula is called *Bayes' Theorem* and it is the mathematical engine that drives Bayesian inference. It is a simple formula, but it is very powerful. It tells us how to update our beliefs about the plausibility of various models in light of new data. The formula has four components:

1.  $P(Model | Data)$: This is the *posterior* probability of the model given the data. It is what we are trying to compute. It tells us how plausible each model is after seeing the data.
2.  $P(Data|Model)$: This is the *likelihood* of the data given the model. It tells us how likely the observed data is under each model.
3.  $P(Model)$: This is the *prior* probability of the model.
4.  $P(Data)$: This is the *marginal likelihood* of the data. It is a normalizing constant that ensures the posterior probabilities sum to one.

To use Bayes' Theorem, we need to compute the likelihood of the data under each model. This is done using the probability distributions defined in our statistical model.

Assume we observe a success, i.e. the patient’s cognition increases by at least 5%. The likelihood of this data under each model is:

-   Under Model1 ($\theta = 70\%$): $P(X=1|\theta=70\%) = 0.7$
-   Under Model2 ($\theta = 20\%$): $P(X=1|\theta=20\%) = 0.2$

The marginal likelihood of the data is computed by summing the likelihoods weighted by the prior probabilities:

$$
P(Data) = P(X=1) = P(X=1|\theta=70\%) \times P(\theta=70\%) + P(X=1|\theta=20\%) \times P(\theta=20\%) = 0.7 \times 0.5 + 0.2 \times 0.5 = 0.45
$$ Now, we can plug these values into Bayes' Theorem to compute the posterior probabilities:

-   For Model1 ($\theta = 70\%$):

$$
P(\theta=70\%|X=1) = \frac{P(X=1|\theta=70\%) \times P(\theta=70\%)}{P(X=1)} = \frac{0.7 \times 0.5}{0.45} \approx 0.778
$$

-   For Model2 ($\theta = 20\%$):

$$
P(\theta=20\%|X=1) = \frac{P(X=1|\theta=20\%) \times P(\theta=20\%)}{P(X=1)} = \frac{0.2 \times 0.5}{0.45} \approx 0.222
$$

So, after observing a success, the posterior probabilities are approximately 77.8% for Model1 and 22.2% for Model2. This means that after seeing the data, we believe that Model1 is more plausible than Model2.

```{r}
# Calculate joint probability distribution after seeing the data (x=1), and the marginal of theta (posterior)
joint <- samples |>
  filter(x == 1) |>
  group_by(theta, x) |>
  summarize(count = n(), .groups = "drop") |>
  mutate(p = count / sum(count))

joint

joint |> 
  group_by(theta) |> 
  summarize(prior = sum(p))
```

## Generative DAGs as scientific and statistical models

Up to this point, we specified a prior joint distribution using information from both a graphical model and a statistical model. In this section, we combine the two model types, graphical models and statistical models, into one visualization called the generative DAG.

The advantage of this is that the generative DAG unites real-world measurements (e.g. cognition increases) with their math/computation world counterparts (e.g. $X \sim \textrm{Bernoulli}(\theta)$) without requiring the cognitive load of flipping back-and-forth between the graphical and statistical models.

Moreover, generative DAGs, when combined with data, enable Bayesian inference to be automated (yay! no more manual calculations using Bayes rule).

### Generative DAGs

To build a generative DAG, we combine a graphical model with a *statistical model* into a single unifying representation.

::: callout-note
A statistical model is a mathematically expressed recipe for creating a joint distribution of related random variables. Each variable is defined either using a probability distribution or a function of the other variables. Statistical model and generative DAG will be used interchangeably. The term statistical model is just the mathematically expressed recipe. Generative DAG is the graphically expressed recipe with an embedded statistical model.
:::

The objective in building generative DAGs is to capture how we might go about simulating real-world measurements using the mathematical language we’ve been learning. Good generative DAGs are recipes for simulating a real-world observation in a way that reflects our domain knowledge. Then, with a generative DAG and some actual data, we will ask the computer to do Bayesian inference for us and to output an updated joint distribution of the random variables in our DAG.

The creation of a generative DAG mimics the creation of graphical models with accompanying statistical models. Both the structure of the DAG and the definition of the random variables should reflect what a domain expert knows, or wants to assume, about the data generating process being investigated.

Here is the generative DAG for the scenario described before. We have seen some elements of this DAG before. For example, the *Cognition increase*s $X$ node, whose prominent feature is that it can only take on two-values, *yes* or *no*. We model these binary-valued real-world outcomes with the probabilistic mapping (Bernoulli PMF) $x \sim \textrm{Bernoulli}(\theta)$.

```{r}
dag_create() |>
  dag_node(
    descr = "Cognition increases",
    label = "x",
    rhs = bernoulli(theta)) |>
  dag_node(
    descr = "Success probability",
    label = "theta",
    child = "x",
    rhs = uniform(0,1)) |>
  dag_render()
```

Other elements of the DAG are new. Notice, `theta` is now spelled out phonetically as opposed to using the actual Greek letter math symbol of $\theta$. While this need not be done when drawing a generative DAG by hand, we phonetically spell out the Greek letter $\theta$ as `theta` here because computer code and computer-generated DAG visuals lack easy support for the Greek alphabet. Additionally, and more importantly, we switch to using lowercase notation for both `x` and `theta` as we want to highlight that ovals in generative DAG models represent a single realization of the random variable. Later we will introduce the representation of more than one realization.

### Building generative models

As we build these generative DAGs, it is often easiest to build and read them from bottom to top; start by converting your target measurement, in this case whether the cognition of patients with AD increases after treatment with a new drug, to a brief real-world description (e.g. *Cognition increases*). This description is the top-line of the oval. For rigor, a more formal definition should be stored outside of the generative DAG in a simple text document that can be referenced by others.

Every node will also have a mathematical line (e.g. $x \sim \textrm{Bernoulli}(theta)$). The mathematical line will always follow a three-part structure of 1) Left-hand side, 2) Relationship Type, and 3) Right-hand side:

Each line of a statistical model follows one of two forms: or

-   **Left-hand side (LHS)**: a mathematical label for a realization of the node (e.g. $x$ or $y$ or whatever symbol/word you choose). The purpose of the label is to have a very short way of referring to a node. To that end, you cannot include spaces in this label and try to keep it at five letters or less.

-   **Relationship Type**: There are two types of relationships that can be specified:

1.  a *probabilistic* relationship, denoted by the $\sim$ symbol, where the LHS node’s value is determined with some uncertainty/randomness as a function of its inputs, such as the outcome of a coin flip. $LHS \sim \textrm{Probability mass/density function}$ Or,

2.  a *deterministic* relationship denoted by an $=$ symbol. A deterministic relationship means the LHS node’s value is determined with zero uncertainty as a function of its inputs. $LHS \sim \textrm{Deterministic function}$

-   **Right-hand side (RHS)**: either a known probability distribution (i.e., probability mass or density function) governing the node’s random outcome (e.g. $\textrm{Bernoulli}(\theta)$) or a deterministic mathematical function defining the node’s value (e.g. $x + y$ ). In both cases, any parameters/variables/inputs in the RHS must be represented by a parent node to the current node in your model.

To model subsequent nodes, consider any parameters or variables on the right-hand side of the mathematical lines that remain undefined; all of these undefined parameters must be defined on the LHS of a parent node. Hence, for $x \sim \textrm{Bernoulli}(theta)$, $\theta$ should be (and is) the LHS of a parent node of $x$.

Now to have a complete statistical model, we need a recipe for how all the random variables of interest can be generated. Previously, when doing Bayes rule calculations by hand, we considered a recipe for with only two possible values. However, as we move towards using the computer to do Bayesian inference for us, we now consider all of the possibilities - any value between 0 and 1.

The generative DAG above introduces us to the uniform distribution. The $\textrm{uniform}(a, b)$ distribution is a two-parameter probability distribution with the property that all real numbers between $a$ and $b$ are generated with equal probability. Hence, for our application where represents a probability restricted to be between 0 and 1, we use $\textrm{uniform}(0, 1)$ to represent our consideration of the infinite possible values of success probability ranging from 0% to 100%.

Since all nodes/variables ($x$ and $\theta$) have a mathematical line, and all RHS parameters/variables are defined, the DAG above represents a generative model. The top line of each oval is a meaningful real-world description of the random variable and the bottom line gives the math.

Notice that the generative DAG is a recipe for simulating or generating a single sample/realization from the joint distribution - read it from top-to-bottom: 1) first, simulate a random sample value of $\theta$, say the realization is 10%, and then 2) use that sample value to simulate $x$, which assuming $\theta = 10\%$, is either 0 with 90% probability or 1 with 10% probability.

For illustrating the simulation aspect of a generative DAG, we can use simple R functions to get a joint realization of the two variables. We model $\theta$ as $\textrm{uniform}(0, 1)$ and $x$ as $\textrm{bernoulli}(\theta)$ computationally using the functions `runif` and `rbern` (i.e. `rfoo` for the uniform distribution and Bernoulli distribution, respectively). Hence, a random sample from the joint distribution of $\Theta$ and $X$ can be generated with the following code:

```{r}
set.seed(1234)
# generate random theta: n is # of samples we want
theta = runif(n=1,min=0,max=1)
theta # print value

# generate random X
x = rbern(n = 1, prob = theta)
x # print value
```

For the particular model above, the recipe picked a random $\theta$ of 11.4% and then, using that small probability of success, generated a random $x$ with 11.4% chance of giving a one. Unsurprisingly, the random sample from $x$ was zero.

A generative models is a mathematical abstraction of a real-world data generating process (DGP); they represent, in the formal language of probability theory, our knowledge of the DGP. We will use these abstractions and Bayesian inference to combine prior information in the form of a generative DAG with observed data. The combination will yield us a posterior distribution; a joint distribution over our RV’s of interest that we can use to answer questions and generate insight. For most generative DAGs, Bayes rule is not analytically tractable (i.e. it can’t be reduced to simple math), and we need to define a computational model using a probabilistic programming language.

### Representing observed data with fill and a plate

A generative DAG represents a family of recipes (i.e., hypothesis space, where each hypothesis/recipe is indexed by a parameter value or combination of parameter values) for simulating one real-world observation. We can then use data to inform us about which of the recipes seem more consistent with the data. For example, we know that observing cognition increase not in one but three patients with AD after drug treatment would be less consistent with the recipe based on the pessimist model $\theta = 20\%$ and more consistent with the recipe based on the optimist model $\theta = 70\%$. Thus, reallocating plausibility in light of this type of data becomes a milestone during our data analysis.

In the generative DAG above, the reallocation of probability in light of the data will be done in such a way as to give more plausibility to $\theta$ values consistent with the observations and less plausibility to the other $\theta$ values.

```{r}
dag_create() |>
  dag_node(
    descr = "Cognition increases",
    label = "x",
    rhs = bernoulli(theta),
    obs = TRUE
    ) |>
  dag_node(
    descr = "Success probability",
    label = "theta",
    child = "x",
    rhs = uniform(0,1)) |>
  dag_render()
```

```{r}
dag_create() |>
  dag_node(
    descr = "Cognition increases",
    label = "x",
    rhs = bernoulli(theta),
    obs = TRUE) |>
  dag_plate(
    descr = "Observation",
    label = "i",
    nodeLabels = "x") |>
  dag_node(
    descr = "Success probability",
    label = "theta",
    child = "x",
    rhs = uniform(0,1)) |>
  dag_render()
```

In the generative DAG above, there are some additional elements. *Observed* data - in contrast to latent parameters/variables or unobserved data - gets represented by using fill shading of the ovals (e.g. the darker fill of *Cognition increases*). Also, rectangles called *plates* are used to indicate repetition of the enclosed random variable (or group/subgraph of random variables). In this case, the plate indicates we have observed multiple \[modeled as independent in the corresponding statistical model\] outcomes/realizations of $x$ from treating more than one patient with the drug.

Text in the lower right-hand corner of the plate indicates how variables inside the plate are repeated and indexed. In this case, there will be one realization of $x$ for each observation. The letter $i$ represents a short-hand label used to index the observations. For example, `x[i]` is the $i^{\text{th}}$ observation of `x` and therefore, `x[2]` would the $2^{\text{nd}}$ observation of `x`. The `[3]` in the lower-right hand corner represents the number of repetitions of the RV, in this case there are 3 observations of stores: `x[1]`, `x[2]`, and `x[3]`.

Since the $\theta$ node lacks a plate, the generative recipe implied by this generative DAG calls for just one realization of $\theta$ to be used in generating all 3 observations of $x$. In other words, our recipe assumes there is just one "true" $\theta$ value and all observations are Bernoulli trials with the same $\theta$ value. The question we will soon answer computationally is “how to reallocate our plausibility among all possible "true" values of $\theta$ given that we have observed 3 observations of $x$?”

More rigorous mathematical notation and definitions can be found in the lucid recommendations of Michael Betancourt. See his work *Towards a Principled Bayesian Workflow (Rstan)* for a more in-depth treatment of how generative models and prior distributions are only models of real-world processes. <https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html>. Additionally, Betancourt’s *Generative Modelling* (<https://betanalpha.github.io/assets/case_studies/generative_modeling.html>) is excellent in explaining how models with narrative interpretations tell a set of stories on how data are generated. For us, we will combine our generative DAGs with observed data to refine our beliefs about which stories (i.e. which parameters) seem more plausible than others.

## Computational Bayesian inference workflows

This section illustrates how to use the `causact` package to specify and run computational Bayesian inference workflows. The package is an easy-to-use, intuitive, and visual interface to `numpyro`. In fact, it automates the creation of `numpyro` code based on a user creating a generative DAG. `causact` advocates and enables generative DAGs to serve as a business analytics workflow (BAW) platform upon which to launch business discussions, create statistical models, and automate computational Bayesian inference.

### Getting started with `dag_foo()` functions

In the code below, the two lines beginning with `dag_` output R list objects consisting of six data frames. Let’s not worry too much about the details, but notice that one of the data frames is for storing node information (i.e. `nodesDF`) and one for edge information (i.e. `edgesDF`).

```{r}
## returns a list of data frames to store DAG info
dag_create()
```

```{r}
## adds a node to the list with given description
dag_create() |> dag_node("BernoulliRV")
```

The function dag_create() is used to create an empty list object that we will refer to as a *causact_graph*. Subsequently functions like `dag_node()` and `dag_edge()` will take a *causact_graph* object as input, modify them, and provide a *causact_graph* object as output. This feature allows for the chaining (i.e. using `|>`) of these functions to build up your observational model. Once done building with `dag_create()`, `dag_node()`, and `dag_edge()`, a call to the `dag_render()` function outputs a visual depiction of your *causact graph*.

```{r}
dag_create() |> 
  dag_node("BernoulliRV") |> 
  dag_render()   ## visualize graph
```

### Four steps to creating graphical models

Let’s use the `dag_foo()` functions to make a generative DAG with Bernoulli data, a uniform prior, and some observed data:

$$
\begin{aligned}X &\sim \textrm{Bernoulli}(\theta) \\\theta &\sim \textrm{uniform}(0,1)\end{aligned}
$$

Assume two successes and one failure such that:

$$
x_1 = 1, x_2 = 1, x_3 = 0
$$

The following code uses the `descr` and `label` arguments of the `dag_node()` function to, respectively, provide a long label for your random variable that is easily understood by business stakeholders and a short label that is more for notational/mathematical convenience:

```{r}
dag_create() |>  # just get one node to show
  dag_node(descr = "Cognition increases", label = "x") |>
  dag_render()
```

Since, we actually observe this node as data, we use the `data` argument of the `dag_node()` function to specify the observed values:

```{r}
dag_create() |>  # make it an observed node by adding data
  dag_node(descr = "Cognition increases", label = "x", 
           data = c(1,1,0)) |>
  dag_render()
```

The node’s darker fill is automated because of the supplied observed data and the `[3]` means there are three observed realizations in the supplied data vector. You could also use a plate to represent the multiple realizations, but I recommend plate creation to be the last step in creating generative DAGs via the `causact` package.

One goal we often have is to take a sample of data and find plausible parameters within a distribution family (e.g. normal, gamma, etc.) governing how that data might have been generated - at its core, Bayesian inference is a search for plausible parameters of a specified generative DAG. To specify the distribution family governing the observed data, use the `rhs` argument of the `dag_node()` function.

::: callout-note
`rhs` stands for right-hand side. The `rhs` of a node is reserved for indicating how the node is generated as a function of its parent nodes’ values. Alternatively, when not used as a distribution specification, the `rhs` is an algebraic expression or other function of data converting parent node values into a realization of the node’s value.
:::

There is an implicit assumption that all unknown variables will eventually be defined by parent nodes - the `causact` package will not check for this. In the case below, we are specifying a parameter `theta` that will need to eventually be added as a parent node.

```{r}
dag_create() |>  # specify data generating distribution
  dag_node(descr = "Cognition increases", label = "x", 
           rhs = bernoulli(theta),  ##add distribution 
           data = c(1,1,0)) |>
  dag_render()
```

Note, a random variable’s distribution must be part of `causact`. The full list of `causact` distributions can be found by running the following line:

```{r}
#| eval: false
?causact::distributions
```

For each unknown argument remaining on the `rhs` of any nodes in your causact graph, one must eventually All unknown rhs arguments for nodes must be defined prior to running any Bayesain computations.define how the unknown argument can be generated. In our data node above, the only unknown argument on the `rhs` is `theta`. We define the generative model for `theta` by making an additional node representing its value:

```{r}
dag_create() |>  
  dag_node(descr = "Cognition increases", 
           label = "x",
           rhs = bernoulli(theta), # no quotation marks
           data = c(1,1,0)) |>
  dag_node(descr = "Success probability", 
           label = "theta",  # labels needs quotes
           rhs = uniform(0,1)) |>
  dag_render()
```

Notice, the parameters of the `rhs` distribution for `theta` are not unknown - they are actually constants (i.e. 0 and 1) and hence, no additional parent nodes are required to be created. Take note that `theta` on the `rhs` for the *Cognition increases* node does not require quotes as it refers to an R object; specifically, this refers to the R object created by:

```{r}
#| eval: false
dag_node(descr = "Success probability", 
         label = "theta",  # label needs quotes
         rhs = uniform(0,1))
```

where the `theta` object is given its name by the `label = "theta"` argument; when creating node labels via this argument, quotes are typically required.

Without a link between `theta` and `x`, one is not creating a properly factorizable directed acyclic graph as is required for specifying a joint distribution. Using the `child` argument of `dag_node`, one can now create the required link between `theta` as a parent node and `theta` as an argument to the distribution of its child node `x`:

```{r}
dag_create() |>  # connect parent to child
  dag_node(descr = "Cognition increases", label = "x",
           rhs = bernoulli(theta),
           data = c(1,1,0)) |>
  dag_node(descr = "Success probability", label = "theta",
           rhs = uniform(0,1),
           child = "x") |>  ## ADDED LINE TO CREATE EDGE
  dag_render()
```

::: callout-note
When using the child argument, please ensure the child was previously defined as a node. Generative DAGs are designed to be built from bottom-to-top reflecting the way a business analyst would create these DAGs. Note that computer code for Bayesian inference, like numpyro code, requires the exact opposite order - parent nodes get defined prior to their children. One way the causact package accelerates the BAW is by facilitating a bottom-up workflow that can be automatically translated to top-down computer code.
:::

For more complicated modelling, repeat this process of adding parent nodes until there are no uncertain parameters on the right hand side of the top nodes.

### Running Bayesian inference on a generative DAG

Now we use `dag_numpyro()` function to get a posterior distribution based solely on the `causact_graph` that we just made. A call to `dag_numpyro()` expects a `causact_graph` as the first argument, so make sure you are not passing the output of the `dag_render()` function by mistake. Use `dag_render()` to get a picture and `dag_numpyro()` for Bayesian inference - just do not mix them in the same chain of functions. The second argument of `dag_numpyro()` is the `mcmc` argument and its default value is `TRUE`. When `mcmc = FALSE`, `dag_numpyro(mcmc = FALSE)` gives a print-out of the `numpyro` code that would be run by `causact` to get a posterior without running the code. You can cut and paste this code into a new R-script if you wish and run it there (ESPECIALLY USEFUL FOR DEBUGGING WHEN YOU GET AN ERROR AT THIS STAGE). Alternatively and preferably, the `numpyro` code should run automatically in the background by setting `mcmc = TRUE` or simply omitting this argument because it is the default value. Usage is shown below:

```{r}
# running Bayesian inference
# remove dag_render() and save graph object
graph = dag_create() |>
  dag_node(descr = "Cognition increases", 
           label = "x",
           rhs = bernoulli(theta),
           data = c(1,1,0)) |>
  dag_node(descr = "Success probability", 
           label = "theta",
           rhs = uniform(0,1),
           child = "x")
# pass graph to dag_numpyro
drawsDF = graph |> dag_numpyro()
```

After some time, during which `numpyro` computes our posterior Bayesian distribution, we get that distribution as representative sample in an object I typically name `drawsDF` - a data frame of posterior draws that is now useful for computation and plotting. For a quick visual-check that all went well, pass the `drawsDF` data frame to the `dagp_plot()` function to get a quick look at the credible (posterior) values of `theta`:

```{r}
drawsDF |> dagp_plot()  # eyeball P(theta>0.65)
```

the lighter fill indicates a 90% percentile interval where 5% of plausible values are excluded from the left- and right-sides of the colored interval. Consider this range your credilble values for `theta`; hence, our posterior belief is that `theta` is somewhere in the 27% to 90% range. The darker fill within the colored interval indicates a 10% percentile interval; hence, the most likely values of `theta` are centered around 60%. For more customized graphs, please use `bayesplot` or `ggplot2` with `ggdist` with `drawsDF` as the input data.

### Investigating the posterior distribution

The object we named `drawsDF` is our posterior distribution after `numpyro` automated Bayes rule for us. The posterior distribution is expressed as a representative sample of all unobserved nodes/parameters; it is not a named probability distribution. Each row of `drawsDF` is a single individual sample or realization of the posterior distribution. Each row is referred to as a **draw** from the posterior.

To see a representative sample of the posterior distribution, we access the R object, `drawsDF`, created above. The `theta` column of `drawsDF` contains our representative sample of the posterior distribution. In this case, that representative sample includes 4,000 samples of `theta`. Recall that our prior belief about `theta`, the probability of success, was uniformly distributed between 0 and 1 - all values equally likely. Now, after observing 2 successes and 1 failure, our plausibility beliefs should favor $\theta$ values away from 0 and 1 and more towards middle values as we learned both success and failure are possible; but we let Bayes rule (via `numpyro`) do this plausibility updating for us.

```{r}
head(drawsDF)
```

Using `bayesplot`, we can visualize how plausibility was reallocated from our uniform prior of $\theta$ in light of the observed data; in other words, we can see the posterior distribution for $\theta$ after observing 2 out of 3 stores increase sales:

```{r}
mcmc_areas(drawsDF)
```

Notice how values of $\theta$ are no longer uniformly distributed; values from 50% to 75% are represented more frequently.

Some other takeaways from the plot regarding plausibility reallocation:

-   Low values of $\theta$ are now deemed less plausible; two out of three successes is simply inconsistent with low values of $\theta$.

-   Very high values of $\theta$ are less plausbile; one failure in three tries is not likely to occur if $\theta$ were to be super-high.

-   Our best guess of $\theta$ went from 0.5 (i.e. the mean of theta when uniformly distributed) to `mean(drawsDF$theta) =` 0.60.

-   We are still very unsure about the "true" value of $\theta$ after only 3 observations. We can see this by looking at quantiles of the posterior distribution suggesting a 90% percentile interval from 0.25 to 0.91:

```{r}
summarize_draws(drawsDF)
```

This continued large band of uncertainty is a good thing. We only have three data points, we should not be very confident in our point estimate of $\theta$. If we want a tighter interval of uncertainty, then we simply need to get more data.

### Making probabilistic statements with indicator functions

With a representative sample of the posterior joint distribution available to us, namely `drawsDF`, we can expand on the strategies of the “Joint Distributions Tell Us Everything” section to make probabilistic statements from representative samples. For example, we might be curious to know $P(\theta \gt 50\%)$. Using indicator functions, simple functions that equal 1 when a criteria is met and 0 when it is not, we can get our estimate of $P(\theta \gt 50\%)$:

```{r}
## estimate posterior probability that theta > 50%
drawsDF |>
  mutate(indicatorFlag = 
           ifelse(theta > 0.5, TRUE, FALSE)) |>
  summarize(pctOfThetaVals = mean(indicatorFlag))
```

We would then state that “the probability that $\theta$ is greater than 50% is approximately 0.70”.

Similarly, we can answer more complicated queries. For example, what is the $P(40\% \lt \theta \lt 50\%)$:

```{r}
## estimate posterior probability that 40% < theta < 60%
drawsDF |>
  mutate(indicatorFlag = 
           ifelse(theta > 0.4 & theta < 0.6, 
                  TRUE, FALSE)) |>
  summarize(pctOfThetaVals = mean(indicatorFlag))
```

**The power of this workflow cannot be overstated** and you will use it frequently for making probabilistic statements. Statements that will come in handy when it is time to use data to inform decision making under uncertainty.

A more descriptive output, possibly used for plotting purposes, is shown here:

```{r}
drawsDF |>
  mutate(indicatorFlag = 
           ifelse(theta > 0.5,
                  "theta > 0.5",
                  "theta <= 0.5")) |>
  group_by(indicatorFlag) |>
  summarize(countInstances = n()) |>
  mutate(percentageOfInstances = 
           countInstances / sum(countInstances))
```

Why does this work? Let’s delve into the math for a moment just to see why.

An indicator function, denoted $1_A$ maps all values of a representative sample $X$ to either 0 or 1 depending on whether the values satisfy criteria to be in some set we label as $A$. For example, assume $A = \{x \ge 0.5\}$ is math set notation for all values in representative sample $X$ such that the draw satisfies $x \ge 0.5$. Then, the formal definition of an indicator function, denoted $1_A$, maps elements in $X$ to either 0 or 1 depending on whether they are in $A$:

$$
1_{A} \equiv\begin{cases}  1, & \textrm{if } x \in A\\  0, & \textrm{if } x \notin A\end{cases}
$$

Now, for the key math trick, known as the *fundamental bridge*. The probability of an event is the expected value (i.e. mean) of its indicator random variable. Mathematically,

$$
P(A) = \mathbb{E}[1_{A}]
$$

which is true since $\mathbb{E}[1_{A}] = 1 \times P(A) + 0 \times P(\bar{A}) = P(A)$ where $\bar A$ denotes not in set $A$. So, using this formula we can make probabilistic statements about a realization $x$ meeting the criteria to be in set $A$. Assuming $J$ draws in our data frame, each draw labelled $j$, then we estimate $P(A)$ by taking the average value of an indicator function over the $J$ draws:

$$
P(A) = \mathbb{E}[1_{A}] \approx \frac{1}{J} \sum_{j=1}^J 1_{x_j \in A}
$$

And despite the heavy math notation, your intuition can guide you in applying this formula. For example, imagine we have a representative sample of $X = [1,4,3,2,5]$ and we want to estimate $P(X \ge 3)$. You could answer this just by looking and say $\frac{3}{5}$ or 3 out of 5 chances for $x \ge 3$. Applying the formula, which we could easily do with code, is shown mathematically here:

$$
P(x \geq 3) = \mathbb{E}[1_{x \geq 3}] \approx \frac{1}{5} \sum_{j=1}^5 1_{x_j \geq 3} = \frac{1}{5} \times (0+1+1+0+1) = \frac{3}{5} = 0.6
$$

We have a probabilistic statement: “the probability that $x$ is greater than or equal to 3 is 0.6”.

### Credit card example

```{r}
graph = dag_create() |>
  dag_node("Get Card","x",
           rhs = bernoulli(theta),
           data = carModelDF$getCard) |>
  dag_node("Signup Probability","theta",
           rhs = uniform(0,1),
           child = "x") |>
  dag_plate("Car Model", "y",  
            data = carModelDF$carModel,  
            nodeLabels = "theta", 
            addDataNode = TRUE)  |>
  dag_plate("Observations", "i",
            nodeLabels = c("x","y"))
```

```{r}
graph |> dag_render()
```

```{r}
drawsDF = graph |> dag_numpyro()
```

```{r}
drawsDF |> dagp_plot()
```

## The binomial distribution

Making a model requires constructing a set of inter-related random variables. Some of these random variables are observed and others are latent (i.e. not observed). A common pattern is to:

1.  *Assume observed variables comes from a specified family of probability distributions.* So far, that known family has been the `Bernoulli` distribution.

2.  *Use latent variables to model uncertainty in the parameters of the distribution family in step 1.* So far, this uncertainty was modeled with the uniform distribution.

```{r}
dag_create() |>
  dag_node(descr = "Bernoulli RV", 
           label = "x",
           rhs = bernoulli(pi),
           data = c(1,1,0)) |>
  dag_node(descr = "Bernoulli parameter RV", 
           label = "pi",
           rhs = uniform(0,1),
           child = "x") |>
  dag_render()
```

The generative DAG we have been working with contains two inter-related random variables $X$ and $\Theta$. For any observed realization $x$, we assume it came from a $\textrm{Bernoulli}(\theta)$ generating process; i.e. there is zero uncertainty that the data is generated from a Bernoulli distribution. Given a specific parameter value $\theta$, the uncertainty in $X$ is purely due to the stochastic nature of a Bernoulli random variable. The takeaway is that for **observed** data, our statistical model restricts the generative recipe to be from the specified family - i.e. Bernoulli.

We also model uncertainty in the **unobserved** random variable $\Theta$. Modelling uncertainty in unobserved RV’s is different in that the posterior distribution of $\theta$ is **not** restricted to be from the same distribution family as its prior specification. In fact, for most cases our posterior distribution for unobserved random variables cannot be summarized using a known distribution family (e.g. normal, uniform, etc.); posteriors will only be approximated with representative samples as that is the best we can do.

When it comes to representing prior uncertainty, try to pick a distribution whose *support* The support of a random variable and its associated probability distribution is the set of possible realizations with non-zero probability density.covers all possible realizations of the RV. For $\Theta$, this would be a probability distribution whose support is all values such that $0 \le \theta \le 1$. The $\textrm{Uniform}(0,1)$ distribution provided an example where prior uncertainty mapped to all possible values of $\theta$, but after updating that uncertainty in light of data, we have already seen that the posterior distribution was no longer uniform.

If we do not have uniform beliefs about plausible values for $\theta$, there is another well-known and more-flexible distribution that can be used as a prior for $\theta$. In other words, if you were to ask a statistician “what other distributions have support that ensures $0 \le \theta \le 1$?”, a likely response you will get is the distribution. In the absence of a statistician, go to wikipedia, <https://en.wikipedia.org/wiki/List_of_probability_distributions>, and find a useful list of probability distributions. You’ll see the first one in the list is the Bernoulli distribution. To find others that might be useful in a certain case:

1.  Match the support of the distribution with what you think is feasible. The wikipedia list has multiple categories for support, we have only seen these two so far:

    -   Discrete with finite support (e.g. Bernoulli)

    -   Continuous on a bounded interval (e.g. uniform)

2.  Investigate how changing parameters can make a distribution more consistent with the assumptions you are willing to make about your generative recipe.

In this section, we investigate the Beta distribution whose support is continuous on a bounded interval and lends itself nicely to model our assumptions about an unknown probability parameter.

### Using a Beta prior for the Bernoulli parameter

A Beta distribution is a two-parameter distribution whose support is $[0,1]$. The two-parameters are typically called $\alpha$ (alpha) and $\beta$ (beta); yes, it is annoying and might feel confusing that the Beta distribution has a parameter of the same-name, $\beta$, but generally, it is clear from context which one you are talking about. Let’s assume random variable $\Theta$ follows a Beta distribution. Hence,

$$
\Theta \sim \textrm{beta}(\alpha,\beta)
$$

then we can plot a representative sample:

```{r}
n = 10000 # number of realizations to generate
# get vector of 10,000 random draws of theta
thetas = rbeta(n, shape1 = 6, shape2 = 2) 

## make a dataframe for plotting
thetas = tibble(thetas) 

## show some values of theta
head(thetas)
```

```{r}
mcmc_areas(thetas)
```

As a prior for a Bernoulli parameter, this distribution does seem valid in the sense that $0 \leq \theta \leq 1$. Clearly however, this is not a uniform prior anymore. This prior suggests that values greater than 50% are much more likely than smaller values (i.e. there is more draws of higher `theta` values in the representative sample than lower `theta` values). So, to use this prior suggests that you have a strong prior belief that success is more likely than failure.

### Matching Beta parameters to your beliefs

With named probability distributions, we can use the `stat_function` layer available with ggplot2 to plot the exact distribution instead of the approximated distribution using a representative sample. For example, the below code plot the exact $\textrm{Beta}(6,2)$ distribution.

```{r}
ggplot() +
  stat_function(fun = dbeta,
                args = c(shape1 = 6, shape2 = 2)) +
  xlim(0, 1)
```

```{r}
tibble(theta = c(0, 1)) |>
  ggplot(aes(x = theta)) +
  geom_area(stat = "function",
            fun = dbeta, 
            args = c(shape1 = 6, shape2 = 2),
            fill = "gray")
```

```{r}
gf_function(fun = dbeta,
            args = c(shape1 = 6, shape2 = 2),
            xlim = c(0, 1))
```

From the perspective of using the $\textrm{Beta}(6,2)$ distribution to represent our beliefs about probability of success for a Bernoulli RV, we again see that higher probability values are more plausible than lower ones (i.e. there is more blue filled area above the higher theta values). If this represented our uncertainty in a trick coin’s probability of heads, we are suggesting that it is most likely biased to land on heads (i.e. more area for `theta` values above 0.5 than below). That being said, there is visible area (i.e. probability) for the `theta` values below 0.5, so using this prior also suggests that the coin has the potential to be tails-biased; we would just need to flip the coin and see a bunch of tails to reallocate our plausibility beliefs to these lower values.

Beta distributions for various $\alpha$ and $\beta$ parameter values are shown below:

![](images/clipboard-1766480459.png)

We see the Beta distribution is flexible in showing a variety of representations of our uncertainty. When looking at the distribution for $\textrm{Beta}(0.5,0.5)$ distribution (top-left) we see that `theta` values closer to zero or one have higher density values than values closer to 0.5. At the other end, a $\textrm{Beta}(4,4)$ distribution distribution places more plausibility for values closer to 0.5. In between, it seems distributions that favor high or low values for `theta` can be constructed.

The Beta distribution has a very neat property which can aid your selection of parameters to model your uncertainty:

> you can roughly interpret $\alpha$ and $\beta$ as previously observed data where the $\alpha$ parameter is the number of successes you have observed and the $\beta$ parameter is the number of failures.

Hence, a $\textrm{Beta}(1,1)$ distribution (which is mathematically equivalent to a $\textrm{Uniform(0,1)}$ distribution) can be thought of as saying a single success and failure have been witnessed, but we are completely unsure as to the probability of each. Whereas, a $\textrm{Beta}(10,10)$ distribution is like suggesting you have seen 20 outcomes and they have been split down the middle - i.e. you have observed some small evidence that the two outcomes occur in equal proportions. Another distribution, say $\textrm{Beta}(10,20)$, can indicate a belief that would accompany having seen 30 outcomes where failures occur 2 times as frequently as successes. Finally, a distribution like $\textrm{Beta}(500,500)$ might be used to represent your uncertainty in the flip of a fair coin. These four distributions (i.e. probability density functions) are shown below side-by-side.

![](images/clipboard-2183603856.png)

Moving forward, we will often use the Beta distribution to represent our prior beliefs regarding a probability or a proportion. The support is $[0,1]$ and it is able to adopt some flexible shapes.

### Using a Beta prior for the Bernoulli parameter

Let's define a generative DAG declaring a $\textrm{Beta}(2,2)$ prior as representative of our uncertainty in $\theta$. We aren’t saying too much with this prior. This is a *weak prior* because. as we will see, it will be easily overwhelmed by data; its just saying that two successes and two failures have been seen. Let’s imagine that we observe 20 successes and only two failures. This data is highly consistent with a very large value for theta. We can intelligently combine prior and data using `dag_numpyro()` to get our posterior:

```{r}
# assume twenty successes and two failures
data = c(rep(1,20), rep(0,2))

# get representative sample of posterior
graph = dag_create() |>
  dag_node("Bernoulli RV","x",
           rhs = bernoulli(theta),
           data = data) |>
  dag_node("Bernoulli Parameter RV","theta",
           rhs = beta(2,2),
           child = "x")
drawsDF = graph |> dag_numpyro()
```

```{r}
mcmc_areas(drawsDF)
```

```{r}
ggplot(drawsDF, aes(x = theta)) +
  geom_area(stat = "function",
            fun = dbeta, 
            args = list(2,2),
            fill = 'grey') + 
  geom_density() +
  xlim(0,1)
```

The figure shows a dramatic shift from prior to posterior distribution. The weak prior suggested ot us that all values between zero and one had plausibility, but once observing 20 successes out of 22 trials, the higher values for $\theta$ became much more plausible.

If we want to change the prior to something stronger, say a $\textrm{Beta}(50,50)$, then we can rerun `dag_numpyro()` after just changing the one line for the prior:

```{r}
# assume twenty successes and two failures
data = c(rep(1,20), rep(0,2))

# get representative sample of posterior
graph = dag_create() |>
  dag_node("Bernoulli RV","x",
           rhs = bernoulli(theta),
           data = data) |>
  dag_node("Bernoulli Parameter RV","theta",
           rhs = beta(50,50),
           child = "x")
drawsDF = graph |> dag_numpyro()
```

```{r}
ggplot(drawsDF, aes(x = theta)) +
  geom_area(stat = "function",
            fun = dbeta, 
            args = list(50,50),
            fill = 'grey') + 
  geom_density() +
  xlim(0,1)
```

The figure shows a posterior distribution that is only mildly shifted from its prior. This is a direct result of a *strong prior* due to the larger $\alpha$ and $\beta$ parameters. In general, we will seek *weakly informative priors* that yield plausible prior generating processes, yet are flexible enough to let the data inform the posterior generating process. There is a bit of an art to this and we will learn more in subsequent sections.

### Example

Your company has been testing two flavors of coffee using free samples, let’s call them Flavor A and Flavor B. You are planning to only offer one flavor for sale and are interested in whether your customers prefer Flavor A or Flavor B.

You use a taste testing survey of 60 randomly selected people, and find that 36 people prefer Flavor B.

What's the the posterior probability that Flavor B is preferred to Flavor A? In other words, what percentage of your posterior draws (assuming the generative DAG shown below) have a `theta` that is above 0.5?

```{r}
data = c(rep(1,36), rep(0,24))
  
graph = dag_create() |>
  dag_node(descr = "Flavor B preferred",
           label = "x",
           rhs = bernoulli(theta),
           data = data) |>
  dag_node(descr = "Prob flavor B preferred",
           label = "theta",
           rhs = beta(2,2),
           child = "x") |>
  dag_plate(descr = "Observations",
            label = "i",
            nodeLabels = "x")

graph |> dag_render()
```

```{r}
drawsDF = graph |> dag_numpyro()
```

```{r}
drawsDF |> dagp_plot()
```

```{r}
summarise_draws(drawsDF)
```

```{r}
mean(drawsDF$theta > 0.5)
```

## Parameter estimation

We now look at multiple potential data distributions, comment on the prior distributions that can accompany them, and learn to update parameter uncertainty in response to data.

### Normal distribution

The Normal distribution also known as the Gaussian is perhaps the most well-known distribution. The Normal distribution is typically notated as $N(\mu,\sigma)$ and has two parameters:

1.  $\mu$: the mean or location/central tendency of either your data generating process or your prior uncertainty and,

2.  $\sigma$: the scale or spread/uncertainty around $\mu$.

![](images/clipboard-2877652671.png)

There is an astounding amount of data in the world that appears to be normally distributed.

Due to its prevalance and some nice mathematical properties, this is often the distribution you learn the most about in a statistics course. For us, it is often a good distribution when data or uncertainty is characterized by diminishing probability as potential realizations get further away from the mean. Given the small probability given to outliers, this is not the best distribution to use (at least by itself) if data far from the mean are expected. Even though mathematical theory tells us the support of the Normal distribution is $(-\infty,\infty)$, the probability of deviations far from the mean is practically zero. As such, do not use the Normal distribution by itself to model data with outliers.

The graphical model representing our uncertainty in the generating process for normally distributed data will have three random variables: 1) the observed data $X$, 2) the mean $\mu$, and 3) the standard deviation $\sigma$.

```{r}
dag_create() |>
  dag_node(descr = "Normal data",
           label = "x",
           rhs = normal(mu, sigma),
           data = data) |>
  dag_node(descr = "Location",
           label = "mu",
           child = "x") |>
  dag_node(descr = "Spread",
           label = "sigma",
           child = "x") |>
  dag_render()
```

and the statistical model with priors:

$$
\begin{aligned}X \sim& N(\mu,\sigma) \\\mu \sim& \textrm{ Some Prior Distribution} \\\sigma \sim& \textrm{ Some Prior Distribution}\end{aligned}
$$

where the prior distributions get determined based on the context of the problem under study.

For example, let’s say we are interested in modelling the heights of cherry trees. Even if we do not know much about cherry trees, we can probably be 95% confident that they are somewhere between 1 foot and 99 feet. Hence, we can set up a prior with 95% probability between 1 and 99, i.e. if $X \equiv \textrm{ Cherry Tree Height}$ , then we know that $P(\mu - 2\sigma \leq x \leq \mu + 2 \sigma) \approx 95\%$ . Hence, we can select $\mu \sim N(50,24.5)$. Note this is a prior on the average height, not the individual height of a cherry tree.

Choosing a prior on $\sigma$ is less intuitive. What do we know about the variation in heights of individual trees? Not much. We can probably bound it though. In fact, we can be 100% certain this is greater than zero; no two cherry trees will be the same height. And I am confident, that cherry tree heights will most definitely fall within say 50 feet of the average, so let’s go with a uniform distribution bounded by 0 and 50.

Hence, our statistical model becomes:

$$
\begin{aligned}X \sim &  N(\mu,\sigma) \\\mu \sim &  N(50,24.5) \\\sigma \sim & \textrm{ Uniform}(0,50)\end{aligned}
$$

After choosing our prior, we then use data to reallocate plausibility over all our model parameters. There is a built-in dataset called `trees`:

```{r}
as_tibble(trees)
```

which we will use in combination with the following generative DAG:

```{r}
graph = dag_create() |>
  dag_node("Tree Height","x",
           rhs = normal(mu,sigma),
           data = trees$Height) |>
  dag_node("Avg Cherry Tree Height","mu",
           rhs = normal(50,24.5),
           child = "x") |>
  dag_node("StdDev of Observed Height","sigma",
           rhs = uniform(0,50),
           child = "x") |>
  dag_plate("Observation","i",
            nodeLabels = "x")

graph |> dag_render()
```

Calling `dag_numpyro()`, we get our representative sample of the posterior distribution:

```{r}
drawsDF = graph |> dag_numpyro()
```

And then, plot the credible values which for our purposes is the 90% percentile interval of the representative sample:

```{r}
drawsDF |> dagp_plot()
```

```{r}
mcmc_areas(drawsDF, pars = "mu")
```

```{r}
mcmc_areas(drawsDF, pars = "sigma")
```

::: callout-note
REMINDER: the posterior distribution for the average cherry tree height is no longer normally distributed and the standard deviation is no longer uniform. This is a reminder that prior distribution families do not restrict the shape of the posterior distribution.
:::

We now have insight as to the height of cherry trees as our prior uncertainty is significantly reduced. REMINDER: the posterior distribution for the average cherry tree height is no longer normally distributed and the standard deviation is no longer uniform. This is a reminder that prior distribution families do not restrict the shape of the posterior distribution.Instead of cherry tree heights averaging anywhere from 1 to 99 feet as accomodated by our prior, we now say (roughly speaking) the average height is somewhere in the 70-80 foot range. Instead of the variation in height of trees being plausibly as high as 50 feet, the height of any individual tree is most likely within 15 feet of the average height (i.e. $\approx \mu \pm 2\sigma$).

### Gamma distribution

The support of any random variable with a normal distribution is $(-\infty,\infty)$ which means just about any value is theoretically possible - although values far away from the mean are practically impossible. Sometimes, you want a similar distribution, but one that has constrained support of $(0,\infty)$, i.e. the data is restricted to being strictly positive (even 0 has no density). The Gamma distribution has such support. While the Gamma distribution is often used to fit real-world data like total insurance claims and total rainfall, we will often use this distribution as a prior for another distribution’s parameters. For example, the standard deviation of a normally distributed random variable is strictly positive, so perhaps the Gamma could be appropriate for modelling uncertainty in a standard deviation parameter. Let’s take a deeper look.

The Gamma distribution is a two-parameter distribution notated $\textrm{Gamma}(\alpha,\beta)$. While there other ways to specify the two parameters, we will use the convention that $\alpha$ is a shape parameter and $\beta$ is a rate parameter.

![](images/clipboard-1272439291.png)

When choosing $\alpha$ and $\beta$ so that our prior uncertainty is accurately represented, we will use a few mathematical properties of the Gamma distribution. Assuming $\textrm{Gamma}(\alpha,\beta)$, the following properties are true: $E[X] = \frac{\alpha}{\beta}$ and $\textrm{Var}[X] = \frac{\alpha}{\beta^2}$.

### Student $t$ distribution

For our purposes, the Student $t$ distribution is a normal distribution with fatter tails - i.e. it places more plausibility to values far from the mean than a similar Normal distribution would. Whereas estimates for the mean of a distribution can get pulled towards outliers, the Student $t$ distribution is less susceptible to that issue. Hence, for just about all practical questions, the Student $t$ is a more robust distribution both as likelihood and for expressing prior uncertainty.

The Student $t$ distribution is a three parameter distribution notated $\textrm{Student-t}(\nu,\mu,\sigma)$.

The last two parameters, $\mu$ and $\sigma$, can be interpreted just as they are with the Normal distribution. The first parameter, $\nu$ (greek letter pronounced “new” and phonetically spelled “nu”), refers to the degrees of freedom. Interestingly, as $\nu \rightarrow \infty$ (i.e. as $\nu$ gets large) the Student $t$ distribution and the Normal distribution become identical; in other words, the fatter tails go away. For all of our purposes, we use a gamma prior for representing our uncertianty in as recommended in <https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations>.

![](images/clipboard-3417554164.png)

If we wanted to model the cherry tree heights using a Student $t$ distribution instead of a Normal distribution, the following generative DAG can be used:

```{r}
graph = dag_create() |>
  dag_node("Tree Height","x",
           rhs = student(nu,mu,sigma),
           data = trees$Height) |>
  dag_node("Degrees Of Freedom","nu",
           rhs = gamma(2,0.1),
           child = "x") |>
  dag_node("Avg Cherry Tree Height","mu",
           rhs = normal(50,24.5),
           child = "x") |>
  dag_node("StdDev of Observed Height","sigma",
           rhs = uniform(0,50),
           child = "x") |>
  dag_plate("Observation","i",
            nodeLabels = "x")

graph |> dag_render()
```

Calling `dag_numpyro()`, we get our representative sample of the posterior distribution:

```{r}
drawsDF = graph |> dag_numpyro()
```

And then, plot the credible values to observe very similar results to those using the Normal distribution:

```{r}
drawsDF |> dagp_plot()
```

The wide distribution for $\nu$ can be interpreted as saying we do not know how fat the tails for this distribution should be. Which makes sense! You need alot of data to potentially observe outliers, so with only 31 observations our generative model remains uncertain about this degrees of freedom parameter.

To highlight the robustness of the Student $t$ distribution to outliers and the lack of robustness of the Normal distribution, let’s add a mismeasured cherry tree height to the data. While most heights are between 60-80 feet, a mismeasured and truly unbelievable height of 1,000 is added to the data (i.e. `dataWithOutlier = c(trees$Height,1000)`). The figure below shows recalculated posterior densities for both the Normal and Student $t$ priors.

![](images/clipboard-1599969966.png)

The posterior estimate of the mean when using a Normal prior is greatly affected by the outlier data point - the posterior average height gets concentrated around an absurd 100 feet. Whereas, the Student $t$ prior seems to correctly ignore that mismeasured data and concentrates the posterior expectation of average cherry tree height around 76 (as discovered previously). While in this case, ignoring the outliers is desirable and the student t distribution provides a robust method for discounting outliers, all modelling choices should be made so that the posterior distribution accurately reflects your prior expectations of how data should be mixed with prior knowledge.

### Poisson distribution

The Poisson distribution is useful for modelling the number of times an event occurs within a specified time interval. It is a one parameter distribution where if $K \sim \textrm{Poisson}(\lambda)$, then any realization $k$ represents the number of times an event occurred in an interval. The parameter, $\lambda$, has a nice property in that it is a rate parameter representing the average number of times an event occurs in a given interval. Given this interpretability, estimating this parameter is often of interest. A graphical model for estimating a Poisson rate from count data is shown below:

```{r}
dag_create() |>
  dag_node(descr = "Count data",
           label = "k",
           rhs = poisson(lambda),
           data = data) |>
  dag_node(descr = "Rate parameter",
           label = "lambda",
           child = "k") |>
  dag_render()
```

and a prior for $\lambda$ will be determined by the use case.

::: callout-note
There are several assumptions about Poisson random variables that dictate the appropriateness of modelling counts using a Poisson random variable. Three of those assumptions are: 1) $k$ is a non-negative integer of counts in a specified time interval, 2) the rate $\lambda$ at which events occur is constant, and 3) the occurrence of one event does not affect the probability of additional events occurring (i.e. events are independent).
:::

Let’s create a dataframe of tickets issued on Wednesday’s in New York City, and then model the rate of ticketing on Wednesdays using the Poisson likelihood with appropriate prior.

```{r}
tickets <- ticketsDF |> 
  mutate(dayOfWeek = wday(date, label = TRUE)) |>
  filter(dayOfWeek == "Wed") |>
  group_by(date) |>
  summarize(numTickets = sum(daily_tickets))

gf_point(numTickets ~ date, data = tickets)
```

Now, we model these two years of observations using a Poisson likelihood and a prior that reflects beliefs that the average number of tickets issued on any Wednesday in NYC is somewhere between 3,000 and 7,000; for simplicity, let’s choose a $\textrm{Uniform}(3000,7000)$ prior as in the following generative DAG:

```{r}
graph = dag_create() |>
  dag_node("Daily # of Tickets","k",
           rhs = poisson(rate),
           data = tickets$numTickets) |>
  dag_node("Avg # Daily Tickets","rate",
           rhs = uniform(3000,7000),
           child = "k")

graph |> dag_render()
```

And extracting our posterior for the average rate of ticket issuance, (NOTE: we use `rate` in code instead of `lambda` due to the latter being a reserved word in Python that will cause issues with using `dag_numpyro()`), yields the following posterior:

```{r}
drawsDF = graph |> dag_numpyro()
drawsDF |> dagp_plot()
```

where we see that that the average rate of ticketing in NYC is somewhere slightly more than 5,000 tickets per day.

## Posterior predictive checks

![](images/clipboard-382068582.png){width="20%"}

> Cobra snakes are known for hypnotizing their prey. Like cobras, Bayesian posteriors can fool you into submission - thinking you have a good model with small uncertainty. The seductiveness of getting results needs to be counter-balanced with a good measure of skepticism. For us, that skepticism manifests as a posterior predictive check - a method of ensuring the posterior distribution can simulate data that is similar to the observed data. We want to ensure our BAW leads to actionable insights, not intoxicating and venomous results.

When modelling real-world data, your generative DAG **never** captures the *true* generating process - the real-world is too messy. However, if your generative DAG can approximate reality, then your model might be useful. Modelling with generative DAGs provides a good starting place from which to confirm, deny, or refine business narratives.

Whether your generative DAG proves successful or not, the modelling process by itself puts you on a good path towards learning more from both the domain expertise of business stakeholders and observed data.

In the last section, we were modelling the daily number of tickets issued in New York City on Wednesdays. We made a dataframe, `tickets`, that had our observed data.

```{r}
head(tickets)
```

The generative DAG, which we thought was successful, yielded a posterior distribution with a huge reduction in uncertainty. As we will see, this DAG will turn out to be the hypnotizing cobra I was warning you about. Let’s learn a way of detecting models that are inconsistent with observation.

```{r}
graph = dag_create() |>
  dag_node("Daily # of Tickets","k",
           rhs = poisson(rate),
           data = tickets$numTickets) |>
  dag_node("Avg # Daily Tickets","rate",
           rhs = uniform(3000,7000),
           child = "k")

graph |> dag_render()
```

```{r}
drawsDF = graph |> dag_numpyro()
drawsDF |> dagp_plot()
```

Prior uncertainty gave equal plausibility to any number between 3,000 and 7,000. The plausible range for the posterior spans a drastically smaller range, about 5,005 - 5,030. So while this might lead us to think we have a good model, do not be hypnotized into believing it just yet.

### Posterior predictive check(s)

A *posterior predictive check* compares simulated data using a draw of your posterior distribution to the observed data you are modelling - usually represented by the data node at the bottom of your generative DAG. This means we simulate 105 observations of tickets issued, $K$, and compare the simulated data to the 105 real-world observations (two years worth of Wednesday tickets).

Here we use just one draw from the posterior for demonstrating a posterior predictive check. It is actually more appropriate to use dozens of draws to get a feel for the variability within the entire sample of feasible posterior distributions.

Simulating 105 observations requires us to convert the DAGs joint distribution recipe into computer code - we do this going from top to bottom of the graph. At the top of the DAG is `lambda`, so we get a single random draw from the posterior:

```{r}
rate = drawsDF |>
  slice_sample(n=1) |>
  pull(rate)

rate
```

Continuing the recipe conversion by moving from parent to child, we simulate 105 realizations of using the appropriate `rfoo` functions (`causact` does not support posterior predictive checks yet, so we must use R’s built-in random variable samplers, namely `rpois` for a Poisson random variable):

```{r}
simData = rpois(n = 105, lambda = rate)
```

And then, we can compare the histograms of the simulated data and the observed data:

```{r}
d <- tibble(k_observed = tickets$numTickets, k_simulated = simData)

# make df in tidy format (use tidyr::pivot_longer)
# so fill can be mapped to observed vs simulated data
d <- d |> 
  pivot_longer(cols = c(k_observed,k_simulated),
               names_to = "type",
               values_to = "numTickets")

head(d)
```

```{r}
gf_density(~ numTickets, data = d, fill = ~ type)
```

The figure above shows two very different distributions of data. The observed data seemingly can vary from 0 to 8,000 while the simulated data never strays too far from 5,000. The real-world dispersion is not being captured by our generative DAG. Why not?

Our generative DAG wrongly assumes that every Wednesday has the exact same conditions for tickets being issued. In research done by Auerbach (2017) \[“Are New York City Drivers More Likely to Get a Ticket at the End of the Month?” *Significance* 14 (4): 20–25\] based off the same data, they consider holidays and ticket quotas as just some of the other factors driving the variation in tickets issued. To do better, we would need to account for this variation.

Let’s now look at how a posterior predictive check for a good generative DAG might work. Consider the following graphical model from the previous chapter which modeled cherry tree heights:

```{r}
graph = dag_create() |>
  dag_node("Tree Height","x",
           rhs = student(nu,mu,sigma),
           data = trees$Height) |>
  dag_node("Degrees Of Freedom","nu",
           rhs = gamma(2,0.1),
           child = "x") |>
  dag_node("Avg Cherry Tree Height","mu",
           rhs = normal(50,24.5),
           child = "x") |>
  dag_node("StdDev of Observed Height","sigma",
           rhs = uniform(0,50),
           child = "x") |>
  dag_plate("Observation","i",
            nodeLabels = "x")

graph |> dag_render()
```

We get the posterior as usual:

```{r}
drawsDF = graph |> dag_numpyro()
```

And then compare data simulated from a random posterior draw to the observed data. *Actually, we will compare simulated data from several draws, say 20, to get a fuller picture of what the posterior implies for observed data.* By creating multiple simulated datasets, we can see how much the data distributions vary. Observed data is subject to lots of randomness, so we just want to ensure that the observed randomness falls within the realm of our plausible narratives.

Getting the twenty random draws, which is technically a random sample of the posterior representative sample, we place them in `paramsDF`:

```{r}
paramsDF = drawsDF |> slice_sample(n=20)

paramsDF
```

Then, for each row of `paramsDF`, we will simulate 31 observations. Since we are going to do the same simulation 20 times, once for each row of parameters, I write a function which returns a vector of simulated tree heights. Again, we convert the generative DAG recipe to code that enables our posterior predictive check:

```{r}
simData = function(nu,mu,sigma) {
  # use n = 31 because 31 observed heights in the data
  return(extraDistr::rlst(n = 31, df = nu, mu = mu, sigma = sigma))
}
```

N.B.: The Student-t distribution in `causact` is based on the `rlst()` function in the `extraDistr` package.

```{r}
d <- paramsDF |>
  mutate(simID = row_number()) |>
  rowwise() |> 
  mutate(simData = list(simData(nu, mu, sigma))) |> 
  ungroup()

d
```

```{r}
d |>
  select(simID, simData) |> 
  unnest_longer(simData) |> # flatten list-column to numeric values
  gf_dens(~ simData, color = ~ factor(simID)) |> 
  gf_dens(~ Height, data = trees, linewidth = 2, inherit = FALSE)
```

This type of spaghetti plot (so-called for obvious reasons), shows 21 different density lines. The twenty light-colored lines each represent a density derived from the 31 points of a single posterior draw. The thicker dark-line is the observed density based on the actual 31 observations. As can be seen, despite the variation across all the lines, the posterior does seem capable of generating data like that which we observed. While this is not a definitive validation of the generative DAG, it is a very good sign that your generative DAG is on the right track.

Despite any posterior predictive success, remain vigilant for factors not included in your generative DAG. Investigating these can lead to substantially more refined narratives of how your data gets generated.

*Do not be a business analyst who only looks at data*, get out and talk to domain experts! See this Twitter thread for a real-world example of why this matters: <https://twitter.com/oziadias/status/1221531710820454400>. It shows how data generated in the real-world of emergency physicians can only be modeled properly when real-world considerations are factored in.

## Decision making

Objectives, decisions, and uncertainty are foundational elements of a field known as *decision theory*.

> Boardwalk Bathhouse’s business plan is to bring a private shower facility to the Ocean City, MD boardwalk. Currently, beachgoers visiting for the day have no way to wash off the saltwater and sand from a day at the beach. Nobody wants to get into their car or go out for dinner without a way of freshening up. We just need your help in deciding a location along the 2.5-mile long boardwalk (see Figure [20.1](https://www.causact.com/intro#fig:boardwalk)) to maximize our chances of success. Where should we put our bathhouse?

### Objectives

For Boardwalk Bathhouse, we will keep things simple and claim the business to be profit-maximizing. In more complex situations, we will often find a strong strategy statement provides time-saving clarity around which objectives or performance measures are important to look at.

### Decisions

The following is a first pass at depicting the Boardwalk Bathhouse decision in a model. Decision nodes, shown as rectangles, are just like random variable nodes (ovals) with the exception that their value is controlled by an internal decision-maker. Hence, we get to choose the realization for a decision node and use that to feed information to the children nodes. In this case, we see how the location decision drives revenue, expenses, and subsequently, profit.

```{r}
graph = dag_create() |>
  dag_node("# of Hot Day Beachgoers") |>
  dag_node(c("Rent by Location",
             "Beachgoer Location Probability"),
           obs = TRUE) |>
  dag_node(c("Revenue","Expenses","Profit"),
           det = TRUE) |>
  dag_node("Bathhouse Location",
           dec = TRUE) |>
  dag_edge(from = c("# of Hot Day Beachgoers",
                    "Beachgoer Location Probability",
                    "Bathhouse Location"),
           to = "Revenue") |>
  dag_edge(from = c("Bathhouse Location",
                    "Rent by Location"),
           to = "Expenses") |>
  dag_edge(from = c("Revenue","Expenses"),
           to = "Profit")

graph |> dag_render(shortLabel = TRUE)
```

The top-down narrative is as follows. Boardwalk Beachhouse’s target customers are hot day beachgoers whose exposure to surf, salt, and sand will make them good candidates for wanting to get cleaned up. Since future weather and human behavior are unpredictable, the *\# of Hot Day Beachgoers* is a random variable. These beachgoers spread-out across the 2.5 mile-long beach in a well-studied pattern represented as an observed data node named *Beachgoer Location Probability*. Using the *\# of Hot Day Beachgoers* and *Beachgoer Location Probability* inputs, one can calculate *Revenue* as a function of these inputs and a given bathhouse location. Presumably, beachgoers closer to the *Bathhouse Location* will be more likely to contribute to revenue, but these details are currently omitted from the DAG. Similarly, *Bathhouse Location* also determines expenses, but further details are omitted from the DAG to keep things simpler.

The DAG reveals how we are mapping our location decision and other model inputs to the objectives we care about (revenue, expenses, and profit). Our strategy is similar to that of any other generative DAG in that we seek representative samples of unknown values; only this time, these representative samples are a function of our decision, e.g. *Bathhouse Location*. With representative samples in hand, we will pick the location that offers the best mix of objectives aligned with the firm’s strategy!

### Uncertainty

The only uncertainty in the DAG is how many hot-day beachgoers will visit the beach during the year. Although often possible and sometimes preferable, we will ignore exact mathematical specification of this uncertainty and simply look at uncertainty expressed as a representative sample, e.g. from a marginal, a joint distribution or some past data that we feel is representative of the future. For now, let’s pretend we are interpeting this as a representative sample from a posterior distribution. We can obtain a representative sample of \# of hot-day beachgoers in a season from a vector built into the `causact` package named `totalBeachgoersRepSample`. A quick histogram of this representative sample is shown below:

```{r}
beachgoerDF = tibble(totalBeachgoers = totalBeachgoersRepSample)

gf_dens(~ totalBeachgoers, data = beachgoerDF)
```

The plot above tells us to expect around 9,000,000 beach visitors (plus or minus a few million) every year to Ocean City. Boardwalk Bathhouse’s profitability will likely be very dependent on this uncertain number. That should not stop us from making a decision though, we boldy accept this uncertainty and use a generative recipe for analyzing our performance measures of interest.

### Creating one draw of a generative decision DAG

The `causact` package is not developed enough to automate the simulation of generative models that include decisions. So, while we can use `causact` to go from prior to posterior and can use `causact` to create figures for communicating about models with decisions, we need to manually do the simulation from posterior to decision choice.

To aid our introduction, we make the following assumptions:

-   1% of beachgoers within 0.1 miles will become a customer of the bathhouse

-   Each customer will pay \$30.

In a more accurate model, these assumptions would also be turned into distributions that reflect any uncertainty in those assumptions.

Any changes to the above assumptions will alter our expectations and even alter the ideal action. While they should be explored in a real-world analysis, we will again ignore these additional details in favor of focusing on the translation from posterior uncertainty to decision making.

[Generating a potential realization of revenue]{.smallcaps} requires specification of the three parent nodes: 1) *\# of Hot Day Beachgoers*, 2) *Beachgoer Location Probability*, and 3) *Bathhouse Location*. So, let’s create a recipe for calculating each of these parent nodes and then, subsequently calculating *Revenue*.

*\# of Hot Day Beachgoers* is modelled using our representative sample (described above). To get a sample draw, we simply sample a single value from the representative sample as follows:

```{r}
getNumBeachgoers = function() {
  sample(totalBeachgoersRepSample,1)
}
```

*Beachgoer Location Probability* is considered known and a table of location probability (`beachgoerProb`) by milemarker (`mileMarker`) is in the following built-in `causact` data frame:

```{r}
beachLocProbDF = beachLocDF |>
  select(mileMarker, beachgoerProb)

beachLocProbDF  
```

*Bathhouse Location* is our decision, we get to choose this. Since the possible locations are already listed in `beachLocProbDF`, we will consider `beachLocProbDF$mileMarker` as our vector of possible locations choices. Hence, we assume there are 26 possible alternatives, one location possibility every tenth of a mile.

In decision theory, you would call the set of possible locations either *alternatives* or *actions*. When reading more about decision theory, knowing these terms will be useful.

*Revenue* is now a function of the three parents. Let’s make the function:

```{r}
revenueFun = function(numBeachgoers,
                      locProbDF,
                      bathhouseLocation) {
  ## get number of potential customers exiting 
  ## beach within 0.1 miles of bathhouse
  numPotentialCustomers = locProbDF |>
    filter(abs(mileMarker - bathhouseLocation) <= 0.1) |>
    mutate(custByMileMark = beachgoerProb * numBeachgoers) |>
    summarize(potentialCustomers = sum(custByMileMark)) |>
    pull(potentialCustomers)
  
  ## capturing 1% of potential customers on average
  ## means actual customers are a binomial rv
  ## hence, use the rbinom() function (see the
  ##representing uncertainty chapter for more info).
  numCust = rbinom(n = 1,
                   size = as.integer(numPotentialCustomers),
                   prob = 0.01)
  
  ## $30 per customer
  revenue = numCust * 30
  
  return(revenue)
}
```

Testing the revenue function above, we use numbers that let us easily verify the math. Assume there are 1,000,000 beachgoers, we know from `beachLocDF` that mile marker 0 will see around 13% of those beachgoers and 1% of those exiting beachgoers turn into customers (about 1,300 customers). At \$30 per customer, revenue of around \$39,000 is expected. Let’s see if the function works:

```{r}
revenueFun(numBeachgoers = 1000000,
           locProbDF = beachLocProbDF,
           bathhouseLocation = 0)
```

Close enough for me. Yay!

*Rent by Location* is considered known and a table of rent expenses (`beachgoerProb`) by milemarker (`mileMarker`) is in the following built-in `causact` data frame:

```{r}
beachExpDF = beachLocDF |>
  select(mileMarker,expenseEst)

beachExpDF
```

*Expenses* is a function of its two parents. Let’s make the function:

```{r}
expenseFun = function(locExpenseDF,bathhouseLocation) {
  ## get number of customers exiting at each mile marker
  ## by taking total # of beachgoers and using location
  ## probability to break the number down by mileMarker
  beachExpDF = locExpenseDF |>
    filter(mileMarker == bathhouseLocation)
  
  return(beachExpDF$expenseEst)
}
```

Let’s make sure this function works. We know from `beachLocDF` that mile marker 0 will have expenses of \$160,000. Testing this:

```{r}
expenseFun(locExpenseDF = beachExpDF,
           bathhouseLocation = 0)
```

And now this works too.

Last node to simulate in our top-down approach to computing a joint realization of the generative decision DAG is the profit node. We use a typical formula for profit:

$$
\textrm{Profit} = \textrm{Revenue} - \textrm{Expenses}
$$

which we will model computationally in the next section where we simulate a draw from the generative decision DAG.

### Simulating a possible outcome

With all the building blocks in place, we create a function that takes a potential decision and simulates one realization of revenue, expenses, and profit. It will return a single row as a data frame representing the draw.

```{r}
generativeRecipe = function(bathhouseLocation) {
  numBeachgoers = getNumBeachgoers()
  locProbDF = beachLocDF |>
    select(mileMarker,beachgoerProb)
  # bathhouseLocation is passed as input to function
  locExpenseDF = beachLocDF |> 
    select(mileMarker,expenseEst)
  rev = revenueFun(numBeachgoers,
                   locProbDF,
                   bathhouseLocation) 
  exp = expenseFun(locExpenseDF,
                   bathhouseLocation)
  profit = rev - exp
  decDrawsDF = tibble(locDec = bathhouseLocation,
                      numBeachgoers = numBeachgoers,
                      rev = rev,
                      exp = exp,
                      profit = profit)
  return(decDrawsDF)
}
```

Let's test it:

```{r}
generativeRecipe(bathhouseLocation = 1)
```

This also looks good. Yay again!!

### Simulating a range of outcomes for all possible decisions

Here we will use two `for` loops to get a representative sample of performance measures for each of the possible bathouse locations. The first loop, the *outer loop*, loops over possible decisions. The second loop, the *inner loop*, will then get 30 sample draws for a given decision. This code may take a minute or two; we have dramatically sacrificed computational speed in this chapter to keep the code intuitive (the end of this chapter offers guidance on making this simulation faster):

```{r}
numSimsPerDec = 30 # number of sims per decision

## create empty dataframe to store results
decDrawsDF = tibble()

for (bathLoc in beachLocDF$mileMarker) {
  for (simNum in 1:numSimsPerDec) {
    drawDF = generativeRecipe(
      bathhouseLocation = bathLoc)
    decDrawsDF = bind_rows(decDrawsDF, drawDF)
  } # end inner loop
} # end outer loop
```

And then, we do a quick visual to inspect our decisions:

```{r}
gf_point(profit ~ locDec, data = decDrawsDF)
```

From visual inspection of the figure above, there are certain locations that seem really good. However, be very skeptical of yourself - “can 30 points really capture a representative sample?” I would argue NO. We will see how to more quickly generate 1,000’s of samples in a subsequent section.

To demonstrate incorporating additional complexity into our simulation, let’s assume we only have \$100,000 to invest, that disqualifies many of the locations. The change to incorporate this is common; manipulate the dataframe to eliminate the infeasible choices and then view the results.

```{r}
gf_point(profit ~ locDec, data = decDrawsDF |> filter(exp < 100000))
```

Under the budget constraint, locating at mile markers 1.9 (expenses of \$96,000) or at mile marker 2.0 (expenses are \$76,000) now provide the best outcomes.

### Faster code

```{r}
# take nSims as argument
fasterGenRecipe = function(nSims) {
  fastGenDF = tibble(
    simNum = 1:nSims,
    numBeachgoers =
      sample(totalBeachgoersRepSample,nSims)) |>
    crossing(beachLocDF) |>
    mutate(numAtMileMarker = 
             numBeachgoers * beachgoerProb) |>
    select(simNum,numAtMileMarker,
           mileMarker,expenseEst,numAtMileMarker) |>
    group_by(simNum) |> arrange(mileMarker) |>
    mutate(numPotentialCust = 
             lag(numAtMileMarker, default = 0) +
             numAtMileMarker + 
             lead(numAtMileMarker, default = 0)) |>
    ungroup() |> arrange(simNum,mileMarker) |>
    mutate(estRevenue = 
             30 * numPotentialCust * 0.01) |>
    mutate(profit = estRevenue - expenseEst) |>
    select(simNum,mileMarker,profit,
           estRevenue,expenseEst)
  return(fastGenDF)
}
```

```{r}
# separate out caption for readability of code
plotCaption = 
  "Median Profit (dark point) and 90% Credible Interval"

fasterGenRecipe(4000) |> group_by(mileMarker) |>
        summarize(q05 = stats::quantile(profit,0.05),
                  q50 = stats::quantile(profit,0.50),
                  q95 = stats::quantile(profit,0.95),
                  expense = first(expenseEst)) |>
        arrange(mileMarker) |>
        ggplot(aes(y = mileMarker, yend = mileMarker)) +
        geom_linerange(aes(xmin = q05, xmax = q95), 
                     size = 4, color = "#5f9ea0") +
        geom_point(aes(x = q50), 
                   size = 4, color = "#11114e") +
        geom_text(aes(x = (expense - 120000), 
                  label = 
                    paste0("Annual Expense = ",
                           scales::dollar(expense))),
                  size = 3, color = "darkred") +
        labs(y = "Bathhouse Location Milemarker",
             x = "Annual Profit($)",
             caption = plotCaption) + 
  scale_x_continuous(limits = c(-50000,450000),
                     labels = scales::dollar_format())
```

I encourage you to avoid presenting point estimates whenever possible. They are so incomplete in the information provided and will make your predictions look foolish as the most likely scenario is still rarely the scenario one sees play out in the business world. In the previous figure, declaring mile marker 1.5 to bring profit of \$150,000 does not convey the real uncertainty where very different profit outcomes are possible - anywhere from \$120,000 to over \$200,000 are all very plausible outcomes.

## A simple linear model

> BuildIt Inc. flips houses - they buy houses to quickly renovate and then sell. Their specialty is building additions on to existing houses in established neighborhoods and then selling the home at prices above their total investment. BuildIt’s decision to move into a neighborhood is based on how sales price fluctuates with square footage. If sales price seems to increase by more than \$120 per additional square foot, then they consider that neighborhood to be a good candidate for buying houses. BuildIt is eyeing a new neighborhood and records the square footage and prices of some recent sales transactions.

The following code creates a dataframe with BuildIt’s findings (note: `salesPrice` is in thousands of dollars):

```{r}
dataDF = tibble(
  salesPrice = c(160, 220, 190, 250, 290, 240),
  sqFootage = c(960, 1285, 1350, 1600, 1850, 1900))
```

Visually, we can confirm what appears to be a linear relationship between square footage and sales prices by creating a scatterplot with a linear regression line drawn in blue: The so-called best line fit to six points. It is the line that minimizes the squared deviations between the line’s predictions of price and the observed prices. We will seek to draw multiple plausible lines, not just one.

```{r}
gf_lm(salesPrice ~ sqFootage, data = dataDF) |> gf_point()
```

BuildIt is interested in the slope of this line which gives the estimated change in mean sales price for each unit change in square footage. For BuildIt, they want to know if this slope is above 120 per square foot?

Letting,

$$
\begin{aligned}x_i \equiv &\textrm{ The square footage for the } i^{th} \textrm{ house.} \\y_i \equiv &\textrm{ The observed sales price, in 000's, for the } i^{th} \textrm{ house.} \\\alpha &\equiv \textrm{ The intercept term for the regression line.} \\\beta &\equiv \textrm{ The slope coefficient representing change in expected price per square footage.} \\\mu_i \equiv &\textrm{ The expected sales price, in 000's, for any given square footage where } \\& \hspace{0.2cm} \mu_i = E(y_i | x_i) \textrm{ and } \mu_i = \alpha + \beta \times x_i.\end{aligned}
$$

the regression output can be extracted using the `lm()` function in R. $\alpha$ is the `(intercept)` and `sqFootage` is the slope coefficient, $\beta$, for square footage:

```{r}
lm(salesPrice ~ sqFootage, data = dataDF)
```

Based on the output, the following linear equation is the so-called “best” line:

$$
\mu_i = 58.91 + 0.1114 \times x_i
$$

Using this model, BuildIt can anticipate being able to sell additional square footage for about \$111 per square foot (i.e. $1000 \times \beta$ because price is in 000’s). This would not earn them acceptable profit as it is less than \$120 per square foot. However, with only 6 data points, there is obviously going to be tremendous uncertainty in this estimate. A Bayesian model can capture this uncertainty.

### A simple Bayesian linear model

From previous coursework, you are probably familiar with simple linear regression in a non-Bayesian context. Extending this model to our Bayesian context, we will create an observational model with the assumption that our observed data is normally distributed around some line. Let’s use the following notation to describe the line:

$$
\mu_i = \alpha + \beta x_i
$$

where,

$$
\begin{aligned}x_i &\equiv \textrm{The value of an explanatory variable for the } i^{th} \textrm{ observation.} \\\alpha &\equiv \textrm{ The intercept term for the line.} \\\beta &\equiv \textrm{ The slope coefficient for the line.} \\\mu_i &\equiv \textrm{ The expected value (or mean) for the } i^{th} \textrm{ observation.}\end{aligned}
$$

In our generative DAG workflow, we represent the Bayesian version of simple linear regression:

```{r}
graph = dag_create() |>
  dag_node("Sales Price","y",
           rhs = normal(mu,sigma),
           data = dataDF$salesPrice) |>
  dag_node("Expected Sales Price","mu",
           child = "y",
           rhs = alpha + beta * x) |>
  dag_node("Intercept","alpha",
           child = "mu",
           rhs = uniform(-100,175)) |>
  dag_node("Square Footage","x",
           data = dataDF$sqFootage,
           child = "mu") |>
  dag_node("Price Std. Dev.","sigma",
           rhs = gamma(4,0.1),
           child = "y") |>
  dag_node("Price Per SqFt Slope", "beta",
           rhs = normal(0.120,0.80),
           child = "mu") |>
  dag_plate("Observations","i",
            nodeLabels = c("y","mu","x"))

graph |> dag_render()
```

The statistical model of the generative DAG has reasonable priors that we will analyze later - for now, let’s digest the implied narrative. Starting at the bottom:

-   *Sales Price* Node(`y`): We observe *Sales Price* data where each realization is normally distributed about an *Expected Sales Price*, $\mu$.

-   *Expected Sales Price* Node(`mu`): Each realization $\mu$ is actually a deterministic function of this node’s parents. Graphically, the double perimeter around the node signals this. This node’s presence on the observation plate alerts us that this expectation varies with each observation. The only way this can happen is that it has a parent that varies with each observation; namely *Square Footage*.

    -   *Square Footage* Node(`x`): The *Square Footage* is observed (as noted by the darker fill) and its lack of `rhs` means we are not modelling any of the variation in realizations ($x$); we just take it as a given.

    -   All other yet-to-be discussed nodes are outside the *Observations* plate. Therefore, the model assumes these three other nodes each have one *true* value. The one we are most interested in *Price Per SqFt Slope*, $\beta$, as this will serve as an estimate of how home prices change when Build-It adds square footage. *Intercept* just sets some base-level home value and *Price Std. Dev.* gives a measure of how much home prices vary about the calculated expected price, $\mu$.

A main deficiency of non-Bayesian linear regression lines is that they do not measure uncertainty in slope coefficients as well. Ultimately, we would rather make decisions that do not fall victim to the hubris of a single estimate and instead make informed decisions with measured uncertainty. For us, we seek to investigate all **plausible** parameter values for the square footage coefficient, not just one.

To highlight the idea of having multiple plausible lines, consider capturing the relationship between sales price and square footage using one of these 8 alternative lines (shown using dashed lines):

![](images/clipboard-2200813982.png)

**Might these alternative lines also describe the relationship between square footage and the expected sales price of a house?** Actually, they do seem reasonable - by golly, there should be multiple plausible lines. So instead of just defining one plausible line, let’s use Bayesian inference to get a posterior distribution over all plausible lines consistent with our model.

To get our posterior, the `numpyro` model is executed with additional arguments so that it runs longer to yield the posterior distribution (extra samples are needed to explore the posterior realm because both our measly 6 points of data and our weak prior leave a very large plausible space to explore):

```{r}
drawsDF = graph |> 
  dag_numpyro(num_warmup = 2000, num_samples = 5000)
```

::: callout-note
Our interest is in the slope of the plausible lines. Because different slope terms can dramatically affect where the line crosses the y-axis, the intercept term should be allowed to fluctuate. Instead of trying the difficult task of placing a prior on the intercept, we can just use a wide uniform prior where we know the line will be positively sloped and not too far from 0. A better way to do this is to transform x to represent the deviation from the average x. In this case, the intercept prior just represents our belief about the price of an average square footage house.
:::

When analyzing the posterior distribution, we are really just interested in the slope coefficient (i.e. 000’s per square foot), we just want a posterior distribution over $\beta$. Visually, that posterior distribution can be retrieved by:

```{r}
drawsDF |> 
  select(beta) |> 
  dagp_plot()
```

The plot shows estimates around 0.120 are plausible, but does not clearly put the majority of plausibility above 0.120. While the estimates are most likely positive (most of the posterior density is where ), the percent above 0.120 will have to be calculated via a query of the posterior distribution:

```{r}
drawsDF |>
  mutate(above120 = ifelse(beta > 0.120,1,0)) |>
  summarize(pctAbove120 = mean(above120))
```

Yielding an approximate 41.9% probability for the company to at least break-even.

Interestingly, because of the uncertainty in the estimate, there is still meaningful probability that the company can earn much more than \$120 per square foot, For example, here is the probability of being above \$140 per square foot.

```{r}
critValue = 0.140
drawsDF |>
  mutate(aboveCriticalValue = 
           ifelse(beta > critValue,1,0)) |>
  summarize(pctAbove = mean(aboveCriticalValue))
```

yielding an approximate 21.9% probability for the company to earn over a very favorable \$140 per square foot.

In conclusion, there is still alot of uncertainty in the estimate of $\beta$. Yet, the company needs to make a decision. If all relevant information has already been collected and BuildIt wants to be confident they will not lose money, then the best decision is to not invest in houses from this neighborhood. However, given the uncertainty in the estimate, this neighborhood certainly has the potential to be profitable. More data and/or more modelling assumptions will be the only ways to reduce that uncertainty and hence, be more confident in this opportunity. This makes sense - we only have six data points to go off of.

### Adding robustness

When modelling, assumptions should reflect the reality you are trying to model. Use care traversing the business analytics bridge from real-world to math-world. For example, when it comes to housing prices, there might be alot of outliers in your data. Hence, it might be better to say that housing prices are Student-$t$ distributed about some expected sales price rather than normally distributed - this will make estimates much more robust to outliers. In a Bayesian context, model assumption changes are readily accomodated, we do not need new or special techniques. In the generative DAG we change the observed data distribution from Normal to Student-$t$.

```{r}
graph = dag_create() |>
  dag_node("Sales Price","y",
           rhs = student(nu,mu,sigma),  ###NEW DIST.
           data = dataDF$salesPrice) |>
  dag_node("Deg. of Freedom","nu",    ###NEW NODE
           child = "y",
           rhs = gamma(2,0.1)) |>
  dag_node("Expected Sales Price","mu",
           child = "y",
           rhs = alpha + beta * x) |> 
  dag_node("Price Std. Dev.","sigma",
           rhs = gamma(4,0.1),
           child = "y") |>
  dag_node("Square Footage","x",
           data = dataDF$sqFootage,
           child = "mu") |>
  dag_node("Intercept","alpha",
           child = "mu",
           rhs = uniform(-100,175)) |>
  dag_node("Price Per SqFt Slope", "beta",
           rhs = normal(0.120,0.80),
           child = "mu") |>
  dag_plate("Observations","i",
            nodeLabels = c("y","mu","x"))   

graph |> dag_render()
```

### Explanatory variable centering

There are always mathematical tricks to do something a little better. In this book, we mostly shy away from the tricks to focus on the core creation process of generative DAGs. However, as you advance on in your BAW journey, you’ll see models leveraging these tricks as if they are common knowledge.

![](images/clipboard-2232385158.png)

The trick we will use now is *centering an explanatory variable*. The plot above expands the regression line shown in the previous figure to show the y-intercept at the point `(0,58.9)` and two other lines that plausibly describe the data. Notice the wild swings in intercepts of these lines. We captured knowledge about these wild swings with the node below, but it does not feel natural.

```{r}
dag_create() |>
  dag_node("Intercept","alpha",
           rhs = uniform(-100,175)) |> dag_render()
```

After adding a centered column of square footage to `dataDF`:

```{r}
dataDF = dataDF |>
  mutate(centeredSqFootage = 
           sqFootage - mean(sqFootage))
```

We can create the figure below:

![](images/clipboard-2118476944.png)

By centering the *Square Footage* variable, we are effectively shifting the y-axis to be drawn in the middle of our *Square Footage* data, $x$. More importantly, this transforms our interpretation of the y-intercept. Any line predicting expected sales price now intersects the y-axis at the average square footage. Hence, our y-intercept is the expected sales price of an average house. It is much easier for us to elicit assumptions about the average price of an average house than it is to imagine the meaning of the y-intercept for a zero square foot house. For our problem, we will say the average house gets sold in the \$200K range, $\alpha \sim N(200,50)$. The updated generative DAG is shown below:

```{r}
graph = dag_create() |>
  dag_node("Sales Price","y",
           rhs = student(nu,mu,sigma),  
           data = dataDF$salesPrice) |>
  dag_node("Deg. of Freedom","nu",    
           child = "y",
           rhs = gamma(2,0.1)) |>
  dag_node("Expected Sales Price","mu",
           child = "y",
           rhs = alpha + beta * x) |> 
  dag_node("Price Std. Dev.","sigma",
           rhs = gamma(4,0.1),
           child = "y") |>
  dag_node("Cent. Square Footage","x",  
           data = dataDF$centeredSqFootage, ## CHANGE
           child = "mu") |>
  dag_node("Intercept","alpha",
           child = "mu",
           rhs = normal(200,50)) |>  ## CHANGE PRIOR
  dag_node("Price Per SqFt Slope", "beta",
           rhs = normal(0.120,0.80),
           child = "mu") |>
  dag_plate("Observations","i",
            nodeLabels = c("y","mu","x"))   

graph |> dag_render()
```

```{r}
drawsDF = graph |> 
  dag_numpyro(num_warmup = 2000, num_samples = 5000)

drawsDF |> dagp_plot()
```

```{r}
## get 20 draws from posterior
plotDF = drawsDF |>
  slice_sample(n=20)

gf_lm(salesPrice ~ centeredSqFootage, data = dataDF) |> gf_point() |> 
  gf_abline(intercept = ~ alpha, slope = ~ beta, data = plotDF, alpha = .25)
```

and realize that there are lots of lines that look plausible through our data. Build-it is going to need more data before they can be confident about the relationship between price and square footage.

More generally, please start to think of Bayesian models as things you, as a Business Analyst, get to create. As long as you are mimicking reality with your models, you will find great success taking the building blocks you know and building models of increasing complexity that capture further nuances of the real-world. These models are yours to build - go boldly and make tailored models of the world you work in!

### Getting more help

Richard McElreath’s course uses `Stan` instead of `causact` and `numpyro`, yet outside of the computational engine, the material is relevant to your study. I highly recommend his lesson on linear models (see <https://youtu.be/0biewTNUBP4>). If you have the time, watch from the beginning. If you want to zero in on the simple linear model, he creates the first linear model trying to predict height as a function of weight at 43:00.

## Linear predictors and inverse link functions

We are learning building blocks for making models of data-generating processes. Each block is used to make some mathematical representation of the real-world. The better our representations, the better our insights. We have almost all the building blocks we need, latent nodes, observed nodes, calculated nodes, edges, plates, linear models, and probability distributions, but this section introduces one last powerful building block - the inverse link function.

### Linear predictors

We focus on restricting the *range* of linear predictors. The *range* of a function is the set of values that the function can give as output. For a linear predictor with non-zero slope, this range is any number from $-\infty$ to $\infty$.

A linear predictor for data observation, $i$ , is any function expressible in this form:

$$
f(x_{i1},x_{i2},\ldots,x_{in}) = \alpha + \beta_1 * x_{i1} + \beta_2 * x_{i2} + \cdots + \beta_n * x_{in}
$$

where $x_{i1},x_{i2},\ldots,x_{in}$ is the $i^{th}$ observation of a set of $n$ explanatory variables, $\alpha$ is the base-level output when all the explanatory variables are zero (e.g. y-intercept when $n=1$), and $\beta_j$ the coefficient for the $j^{th}$ explanatory variable ($j \in {1,2,\ldots,n}$). When $n = 1$, this is just the equation of a line as in the last section. When there is more than one explanatory variable, we are making a function with *high-dimensional* input - meaning the input includes multiple explanatory RV realizations per observed row. High-dimensional functions are no longer easily plotted, but the interpretation of the coefficients remain consistent with our intuition.

Explanatory variable effects are fully summarized in the corresponding coefficients, $\beta$. If an individual coefficient $\beta$ is positive, the linear prediction increases by $\beta$ units for each unit change in the explanatory variable. For example, we thought it plausible for the expected sales price of a home to go up by \$120 for every additional square foot; 10 additional square feet, then the home value increases \$1,200; 100 additional square feet, then the home value increases \$12,000. You can continue this logic ad-nauseum until you have infinitely big houses with infinite home prices. The takeaway is that linear predictors, in theory, can take on values anywhere from $-\infty$ to $\infty$.

### Inverse link functions

An inverse link function takes linear predictor output, which ranges from $-\infty$ to $\infty$, and confines it in some way to a different scale. For example, if we want to use many explanatory variables to explain success probability, our method will be to estimate a linear predictor and then, transform it so its value is forced to lie between zero and 1 (i.e. match the domain over which probabilities exist). More generally, inverse link functions are used to make linear predictors map to predicted values that are on a different scale. For our purposes, we will look at two specific inverse link functions:

1.  *Exponential*: The exponential function converts a linear predictor of the form $\alpha + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n$ into a curve that is restricted to values between 0 and $\infty$. This is useful for converting a linear predictor into a non-negative value. For example, the rate of tickets issued in New York city can be modeled by taking a linear predictor for tickets and turning it into a non-negative rate of ticket issuance. If we label the linear predictor value $y$ and the transformed value $\lambda$, the exponential function converting $y$ to $\lambda$ is defined here:

$$
\lambda = \exp(y) = \exp(\alpha + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n)
$$

2.  *Inverse Logit* (aka logistic): This function provides a way to convert a linear predictor of the form $\alpha + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n$ into a curve that is restricted to values between 0 and 1. This is useful for converting a linear predictor to a probability. If we label the linear predictor value $y$ and the transformed value $\theta$, the inverse logit function converting $y$ to $\theta$ is defined here (note the negative sign):

$$
\theta = \frac{1}{1+\exp(-y)}= \frac{1}{1+\exp(-(\alpha + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n))}
$$

While the beauty of these functions is that it allows us to use the easily-understood linear model form and still also have a form that is useful in a generative DAG. The downside is we lose interpretability of the coefficients. The only thing we get to say easily is that higher values of the linear predictor correspond to higher values of the transformed output.

::: callout-note
When communicating the effects of explanatory variables that are put through inverse link functions, you should either: 1) simulate observed data using the prior or posterior’s generative recipe, or 2) consult one of the more rigorous texts on Bayesian data analysis for some mathematical tricks to interpreting generative recipes with these inverse link functions (see references at end of book).
:::

#### Exponential function

The generative DAG below is a generic example of a Poisson count variable and makes the expected rate of occurrence a function of an explanatory variable:

```{r}
dag_create() |>
  dag_node("Count Data","k",
           rhs = poisson(rate),
           obs = TRUE) |>
  dag_node("Exp Rate","rate",
           rhs = exp(y),
           child = "k") |>
  dag_node("Linear Predictor","y",
           rhs = alpha + beta * x,
           child = "rate") |>
  dag_node("Intercept","alpha",
           child = "y") |>
  dag_node("Explantory Var Coeff","beta",
           child = "y") |>
  dag_node("Observed Expl Var","x",
           child = "y",
           obs = TRUE) |>
  dag_plate("Observation","i",
            nodeLabels = c("k","rate","y","x")) |>
  dag_render()
```

The inverse link function transformation takes place in the node for `rate`. The linear predictor, $y$, can take on any value from $-\infty$ to $\infty$, but as soon as it is transformed with the exponential function, it can only be a positive number. This transformation is shown in the figure below where positive and negative x-axis values become solely positive y-axis values.

![Graph of the exponential function. The linear predictor in our case is alpha + beta \* x. The role of the exp function is to map this linear predictor to a scale that is non-negative. This essentially takes any number from -infinity to infinity and provides a positive number as an output.](images/clipboard-3504728207.png)

From this figure, we see that negative values of the linear predictor are transformed into values of rate between 0 and 1 and positive values of the linear predictor get transformed into rate values greater than 1. Notice this transformation is non-linear, and hence caution must be used interpreting the slope coefficients of the linear predictor.

#### Inverse logit

The generative DAG below is a generic example which leverages the inverse logit link function:

```{r}
dag_create() |>
  dag_node("Bernoulli Data","z",
           rhs = bernoulli(theta),
           obs = TRUE) |>
  dag_node("Success Probability","theta",
           rhs = 1 / (1+exp(-y)),
           child = "z") |>
  dag_node("Linear Predictor","y",
           rhs = alpha + beta * x,
           child = "theta") |>
  dag_node("Intercept","alpha",
           child = "y") |>
  dag_node("Explantory Var Coeff","beta",
           child = "y") |>
  dag_node("Observed Expl Var","x",
           child = "y",
           obs = TRUE) |>
  dag_plate("Observation","i",
            nodeLabels = c("z","theta","y","x")) |>
  dag_render()
```

Note the inverse link function transformation takes place in the node for `theta`. To start to get a feel for what this transformation does, observe the figure below. When the linear predictor is zero, the associated probability is 50%. Increasing the linear predictor will increase the associated probability, but with diminishing effect. When the linear predictor is increased by one unit from say 1 to 2, the corresponding probability goes from about 73% to 88% (i.e. from to ). This is a 15% jump. However, increasing the linear predictor by one additional unit has probability go from 88% to 95% - only a 7% jump. Further increasing the linear predictor has diminishing effect. Likewise, large negative values in the linear predictor lead to ever-closer to zero values for probability.

![Graph of the inverse logit function (aka the logistic function). The linear predictor in our case is alpha + beta \* x. The role of the inverse logit function is to map this linear predictor to a scale bounded by zero and one. This essentailly takes any number from -infinity to infinty and provides a probability value as an output.](images/clipboard-3967822332.png)

Almost anytime you are modelling a probability as a function of many explanatory variables, using the inverse logit-link function is an obvious choice to make the mathematics work.

The use of the inverse logit function is done inside a method called logistic regression. Check out this sequence of videos that begin here (<https://youtu.be/zAULhNrnuL4>) on logistic regression for some additional insight.

You have officially been exposed to all the building blocks you need for executing Bayesian inference of ever-increasing complexity. These include latent nodes, observed nodes, calculated nodes, edges, plates, probability distributions, linear predictors, and inverse-link functions. While you have not seen every probability distribution or every inverse-link function, you have now seen enough that you should be able to digest new instances of these things. In the next chapter, we seek to build confidence by increasing the complexity of the business narrative and the resulting generative DAG to yield insights. Insights you might not even have thought possible!

## Multi-level modeling

> Crossfit Gyms is growing - they started the year with 5 locations and have more than doubled in size in the last five months. Additionally, Crossfit Gyms has secured funding to open 100 more gyms over the next year. Their key recruitment mechanism is to offer free trial memberships that last a calendar month, but management is unsure how to measure the success of this trial. Additionally, some gyms have been randomly chosen to increase the stretching, called “Yoga Stretch”, done during these trial classes. Crossfit Gyms is unsure whether this has helped convert trial customers into memberships. The theory is that new members, typically less athletic than current clients, benefit from the additional stretching and are more likely to enjoy the additional stretch time.

Our task is to help Crossfit Gyms understand the effect of its trial membership program and whether additional stretching leads to better conversion of free-trial customers into paying members. As we go through this example, we will explore three different ways to model the problem:

1.  **Complete Pooling**: This modelling method will *pool* or aggregate all the data for all the gyms. It will model two conversions rate for the entire franchise - one with yoga stretching and one without.

2.  **No Pooling**: This method will treat each gym individually with the underlying assumption that the two conversion rates (with and without yoga) at one gym are completely independent of conversion rates at every other gym.

3.  **Partial Pooling**: Under partial pooling, we assume each gym’s two conversion rates have both a unique component (like the no pooling case) as well as a component that is dictated by company-wide policies (like the complete pooling case). Hint: This is the case we will always use as the other two cases are subsets of these types of models.

### The gym data

Crossfit Gyms does not have alot of data, so let’s look at it.

```{r}
data("gymDF")

gymDF
```

When thinking about visualizing this dataset, we need to understand that each row represents a particular gym (`gymID`) running one month of free trial classes.

```{r}
gymDF %>% group_by(gymID) %>%
  summarize(numberOfObservations = n())
```

We discover that we have twelve unique gyms; the first 5 gyms give us five months of data each, while the next seven have less data (ranging from one to four free-trial months).

Since the small `gymdDF` data frame consists of only five variables, I am confident that I can map all five to aesthetics and show all the data using one plot command. After several iterations of plots (excluded here), here is one way to do that:

```{r}
plotDF = gymDF %>%
  mutate(stretchType = 
           ifelse(yogaStretch == 1, 
                  "Yoga Stretch", 
                  "Traditional"))

ggplot(plotDF, 
       aes(x = timePeriod, #map time to x-axis
           y = nTrialCustomers, #map numTrials to y-axis
           fill = stretchType #map fill to stretch type
                  )) + 
  geom_col(width = 0.75, 
           alpha = 0.55) +  # use transparency
  geom_col(width = 0.75, 
           aes(y = nSigned)) + # different y-mapping
  facet_wrap(vars(gymID), labeller = label_both) + 
  theme_minimal(11) + 
  theme(plot.caption = 
          element_text(size = 9, face = "italic"),
        legend.position = "bottom") +
  ylab("# of Trial Customers") +
  labs(caption = 
         "Darker fill for # of sign-ups.")
```

You can see from the figure above that the recruitment rate at each gym, represented by the proportion of dark fill to lighter fill, seems pretty consistent. However, there is apparent high variability across gyms. For example, `gymID: 2` seems to be a consistently poor performer while `gymID: 5` is somehow able to consistently make more conversions from free trials to paying memberships.

::: callout-note
Talking to the managers of these gyms to understand the differences in performance should be high on your to-do list, but for now (and because we do not have access to these managers), we will stick with the data that is available.
:::

In terms of providing insight as to recruitment rates and the effect of extra yoga stretching (indicated by fill color), the visualization can only go so far. Yes, we can say `gymID: 5` outperforms `gymID: 2`, but with only five months of data, how can we quantify our uncertainty about the differences? Additionally, yoga stretching seems to lead to better conversion rates, but how confident should we be given the seemingly small amounts of data?

### Complete pooling

The complete pooling model considers all observations to come from identical probability distributions; it does not differentiate based on `gymID` or `timePeriod`. Hence, all 44 observations behave as if they come from a single gym’s generative model. For this generative model, the unobserved parameter `theta` will be the only distinguishing feature. It takes one value for a traditional stretching class and another value for a yoga stretching class.

Under the complete pooling assumption, there is enough data that empirical estimates provide good probability estimates for the conversion rates by class type:

```{r}
gymDF %>% group_by(yogaStretch) %>%
  summarize(nTrials = sum(nTrialCustomers),
            nSuccess = sum(nSigned)) %>%
  mutate(pctSuccess = nSuccess / nTrials)
```

Here, we see that conversion rates seem to rise with the additional stretching from a base rate of 23.9% to 34.8%.

The figure below shows the generative DAG for complete pooling. Despite not being a very good model for reasons we will see, it still provides a posterior with seemingly small uncertainty:

```{r}
graph = dag_create() %>%
  dag_node("Number of Signups","k",
           rhs = binomial(nTrials,theta),
           data = gymDF$nSigned) %>%
  dag_node("Signup Probability","theta",
           child = "k",
           rhs = beta(2,2)) %>%
  dag_node("Number of Trials","nTrials",
           child = "k",
           data = gymDF$nTrialCustomers) %>%
  dag_plate("Yoga Stretch Flag","x",
           data = gymDF$yogaStretch,
           nodeLabels = "theta",
           addDataNode = TRUE) %>%
  dag_plate("Observation","i",
            nodeLabels = c("k","x","nTrials"))

graph %>% dag_render()
```

Notice that the graphical model has four random variables. Three of them are observed and enclosed in a plate labelled `Observation i`. This plate means that for each observation in our data (i.e. one of the 44 unique gym/time period rows in `gymDF`) , there are three observed random variables: 1) Number of signups (`k`), 2) Number of Trial Customers (`nTrials`), and 3) a flag indicating whether yoga stretch ws used or not (`x`). In contrast, the plate around Signup Probability, labelled *Yoga Stretch Flag*, indicates we have two random variables for Signup Probability: 1) a Signup Probability for traditional Stretching (i.e. probability when `x=0`) and 2) a Signup Probability for yoga Stretching (i.e. probability when `x=1`). In other words, all gyms have the same two success probabilities associated with them; we call this *complete pooling* of the gyms as according to the model, each gym is identical. We know this is a ridiculous assumption from visualizing the data, but we use this to illustrate that there are very meaningful assumptions tied to each graphical model we make.

Computationally, we go through the standard process to get our posterior and then we look at our posterior draws. Estimated signup probability for yoga stretching (about 35%) seems to range higher than without yoga (less than 25%).

```{r}
drawsDF = graph %>% dag_numpyro()
```

```{r}
drawsDF %>% dagp_plot()
```

Perhaps, the fact that these probability estimates align with the emprical estimates (above) should seem comforting, but there is something sinister lurking in our model choice. Recall that our visual sensibilities have already noticed differences among the gyms. So, it should be expected that we are not accurately modeling the data generation process when ignoring these differences. By performing a posterior predictive check, we see how looking at this overly simple *complete pooling* model fails to capture some of the patterns observed in the actual data.

```{r}
paramsDF = drawsDF |> slice_sample(n=20)

paramsDF
```

```{r}
simData <- function(theta_0, theta_1, data) {
  simData <- data %>%
    mutate(signupProb = ifelse(yogaStretch == 0, theta_0, theta_1)) %>%
    mutate(nSigned = rbinom(
      n = n(),
      size = nTrialCustomers,
      prob = signupProb
    )) |>
    pull(nSigned)
}
```

```{r}
d <- paramsDF |>
  mutate(simID = row_number()) |>
  rowwise() |> 
  mutate(simData = list(simData(theta_0, theta_1, gymDF))) |> 
  ungroup()

d
```

```{r}
d |>
  select(simID, simData) |> 
  unnest_longer(simData) |> # flatten list-column to numeric values
  gf_dens(~ simData, color = ~ factor(simID)) |> 
  gf_dens(~ nSigned, data = gymDF, linewidth = 2, inherit = FALSE)
```

The figure above shows the observed data density plot across gyms as represented by the dark blue line. This observed distribution is not captured by the spaghetti lines, i.e. predicted distributions from the posterior, none of the 20 sample predicted distributions come close to mimicking the behavior seen in the actual data. This is a clear signal we are not modelling something right; namely each gym will have some differences in recruiting probabilities, and hence, produce more varied amounts of success than suggested by the complete pooling model.

### No pooling

The no pooling model treats each gym as distinct completely independent entities, i.e. the Crossfit Gym franchise has no overarching effect on success probabilities. Thus, learning about one gym’s success in recruiting tells us nothing about the population of gyms or any other specific gym.

::: callout-note
The no pooling model is shown for the contrast it provides in relation to other models we will look at; it is an ill-advised model when you have small data and transferable learning. We will assume learning about one gym’s success rate should impact our opinion about another gym’s success rate - we do not want each gym to be distinct.
:::

On our path to modelling the effects of yoga, we are going to refine our previous generative DAG to get closer to the business narrative. Specifically, we want to ensure a direct modelling of the effect of yoga stretching.

We are going to assume there is a base-level of recruitment at each gym and that yoga stretching provides some deviation from the base. If we were to model the deviation as a probability deviation, say yoga increases recruitment probability 20%, than its possible that forecasting the effect of yoga at a gym with a base recruitment rate of 85% leads to 105% sign-up probability - this is a problem as probability of signup cannot be over 100%.

To remedy this, we use a linear predictor as input into an inverse logit link function; it ensures signup probability draws are actually probability values between 0 and 1. This new generative story with base recuitment probability and deviation for yoga is reflected in the following generative DAG:

```{r}
graph2 = dag_create() %>%
  dag_node("Number of Signups","k",
           rhs = binomial(nTrials,theta),
           data = gymDF$nSigned) %>%
  dag_node("Signup Probability","theta",
           child = "k",
           rhs = 1 / (1+exp(-y))) %>%
  dag_node("Number of Trials","nTrials",
           child = "k",
           data = gymDF$nTrialCustomers) %>%
  dag_node("Linear Predictor","y",
           rhs = alpha + beta * x,
           child = "theta") %>%
  dag_node("Yoga Stretch Flag","x",
           data = gymDF$yogaStretch,
           child = "y") %>%
  dag_node("Intercept","alpha",
           rhs = normal(-1,1.5),
           child = "y") %>%
  dag_node("Yoga Slope Coeff","beta",
           rhs = normal(0,0.75),
           child = "y") %>%
  dag_plate("Gym","j",
            nodeLabels = c("alpha","beta"),
            data = gymDF$gymID,
            addDataNode = TRUE) %>%
  dag_plate("Observation","i",
            nodeLabels = c("k","x","j",
                           "nTrials","theta","y"))

graph2 %>% dag_render()
```

This generative DAG has slope and intercept terms that are not directly understandable - they go through an inverse logit function before we see their impact on a node that we can understand. To understand the prior on these terms (or any prior), we simulate how its values are transferred to a parameter of interest, in this case that parameter is signup probability.

#### Using prior predictive checks to help choose priors

Unfortunately, I am always writing custom code to explore the effect of priors on the children of transformed linear predictors. This part of the BAW process will be accelerated in future versions of `causact`. Until then, here is some code to help find a good `alpha`:

```{r}
numSims = 10000

## guess at distribution and params for alpha
## let's say alpha ~ normal(0,10) was my first guess
## then simulate effects on theta.  Do they mimic
## your uncertainty in theat without data?  If no,
## iterate with a different guess. After 
## many iterations I settled on normal(-1,1.5)
## below code gets repsample of theta for this prior
simDF = tibble(alpha = rnorm(n = numSims, 
                             mean = -1., 
                             sd = 1.5))

simDF = simDF %>%
  mutate(y = alpha) %>% ##follows DAG
  mutate(theta = 1 / (1 + exp(-y)))  ## inv logit

## plot implied theta distribution
gf_density(~ theta, data = simDF)
```

The figure above is the result of many iterations of looking at the effect of changes in `alpha`’s prior on success probability `theta`. I stopped at because it reflected my mild belief that sign-up probability will be smaller than 50%, but should also remain open to any possible value. I am also comfortable with the hump of probability around 10%, that sounds like a good mode for recruiting.

Your prior should be open to changing its mind, but it should also reflect assumptions you are willing to make. Don’t be afraid to have mildly informative priors, they do not all need to be uniform. This will protect you from bad decisions when there is not alot of data to inform the posterior.

A similar exercise is done for the prior on . Recognizing that stretching is most likely going to have a small effect. I run the following code to find my prior:

```{r}
numSims = 10000

## guess at distribution and params for beta
## since beta is deviation from base rate alpha
## assume a base rate and then, see effects
## mean should absolutely be zero to indicate no prior
## preference for whether yoga is helpful
## let the data tip the scales
## sd should be smaller than that used for alpha
baseRate = 0.20  ## assumed for now

## logit function 
## input: probability, 
## output: linear Predictor
## inverse of the inverse logit function
## linearPredictor = log(probability / (1-probability))
alpha = log(baseRate / (1 - baseRate))

simDF = tibble(alpha = alpha,
               beta = rnorm(n = numSims,
                            mean = 0,  
                            sd = 0.75))

simDF = simDF %>%
  mutate(yNoYoga = alpha) %>%
  mutate(yWithYoga = alpha + beta) %>%
  mutate(thetaNoYoga = 1 / (1 + exp(-yNoYoga))) %>%
  mutate(thetaWithYoga = 1 / (1 + exp(-yWithYoga))) %>%
  mutate(probDiff = thetaWithYoga - thetaNoYoga)

## plot implied theta distribution
gf_density(~ probDiff, data = simDF)
```

#### Parameter estimation

```{r}
drawsDF = graph2 %>% dag_numpyro()
```

```{r}
drawsDF %>% dagp_plot()
```

Recall that these are components of a linear predictor (i.e. $\alpha + \beta x$). If the linear predictor is zero, that corresponds to a probability of 50%. Hence, values for $\alpha < 0$ would imply base conversion rates less than 50%. Values for $\beta$ indicate whether that base probability should be adjusted up or down; $\beta \lt 0$ implies a negative effect for the yoga stretching routine and $\beta \gt 0$ indicates that the effect is positive.

```{r}
plotDF = gymDF %>%
  mutate(stretchType = 
           ifelse(yogaStretch == 1, 
                  "Yoga Stretch", 
                  "Traditional"))

ggplot(plotDF, 
       aes(x = timePeriod, #map time to x-axis
           y = nTrialCustomers, #map numTrials to y-axis
           fill = stretchType #map fill to stretch type
                  )) + 
  geom_col(width = 0.75, 
           alpha = 0.55) +  # use transparency
  geom_col(width = 0.75, 
           aes(y = nSigned)) + # different y-mapping
  facet_wrap(vars(gymID), labeller = label_both) + 
  theme_minimal(11) + 
  theme(plot.caption = 
          element_text(size = 9, face = "italic"),
        legend.position = "bottom") +
  ylab("# of Trial Customers") +
  labs(caption = 
         "Darker fill for # of sign-ups.")
```

Comparing the estimates above with the gym data from the top (copied above for convenience), notice how the posterior estimates correspond with your expectations. The $\alpha$ values with the smallest credible intervals are for gyms that have the largest number of trial customers participating in trials with traditional stretching (i.e. gyms 2, 5, and 8). Similarly, the $\beta$ values with the smallest credible intervals are for gyms that have the largest number of trial customers participating in Yoga stretching classes (i.e. gyms 1 and 4). The opposite is also observable. Gyms lacking data for traditional stretching have wide credible intervals for $\alpha$ (e.g. gym 10) and gyms lacking data for yoga stretching do not improve on the prior uncertainty in $\beta$ (e.g. gyms 11 and 12).

This no pooling model is not terrible in the sense that each gym gets statistically modeled. Let’s do some challenging data manipulation and do a posterior predictive check:

```{r}
## get DF of observed data and remove target measure
simDF = gymDF %>% select(-nSigned)

## get 20 random draws of posterior
paramsDF = drawsDF %>%
  slice_sample(n=20) %>%
  mutate(sampDrawID = row_number()) 

## when data like gym_id is in the name of column 
## headings, e.g. alpha_4,
## we use the tidyr package pivot_longer function 
## to turn headings into data
## and extract the gymID information from the heading
tidyParamDF = paramsDF %>% 
    pivot_longer(cols = -sampDrawID) %>%
    mutate(gymID = str_extract(name, '[0-9]+')) %>%
    mutate(gymID = as.integer(gymID)) %>%
    mutate(paramName = str_extract(name, '[:alpha:]+'))

## now we get explanatory variable info 
## combined with relevant params
simDF = simDF %>% left_join(tidyParamDF, by = "gymID") 
## 44 observations * 2 params per row * 20 sims 
## = 1,760 rows

## get relevant alpha and beta on same row
simPredDF = simDF %>%
  pivot_wider(id_cols = gymID:sampDrawID,
              names_from = paramName,
              values_from = value)

## if no yoga, then y=alpha.  If yoga, then y=alpha+beta
simPredDF = simPredDF %>%
  mutate(y = ifelse(yogaStretch == 0, 
                    alpha, alpha+beta)) %>%
  mutate(theta = 1 / (1 + exp(-y))) %>%
  mutate(nSigned = rbinom(n = n(),
                          size = nTrialCustomers,
                          prob = theta))

obsDF = tibble(obs = gymDF$nSigned)

colors = c("simulated" = "cadetblue", 
           "observed" = "navyblue")

## make plot
ggplot(simPredDF) +
  stat_density(aes(x=nSigned, 
                   group = sampDrawID,
                   color = "simulated"),
               geom = "line",
               position = "identity") +
  stat_density(data = obsDF, aes(x=obs, 
                                 color = "observed"),
               geom = "line",  
               position = "identity",
               lwd = 2) +
  scale_color_manual(values = colors) +
  labs(x = "# of Monthly Signups",
       y = "Density Estimated from Data",
       color = "Data Type")
```

This posterior predictive check looks really good. Yet, the estimates above revealed there are credible intervals which can be made smaller; uncertainty bands can be narrowed for gym’s missing one type of class. We know, from previous observation at all the gyms, that yoga’s effect seems small. Hence, the wide uncertainty bands are not warranted. Going the other way, we should have more confidence in gym 10’s recruitment rate (). Gym 10 is a star performer and I do not think dropping the yoga-stretch will lead to enormous drops in rate. Overall, I think we should agree that learning about one gym might tell us something about another gym. In fact, I would argue that learning about 12 gym’s should tell us what we might expect when the next 100 gyms are opened. The next model explores this and shows us we can do better.

### Partial pooling

The partial pooling model imagines that the Crossfit Gyms business model produces gyms with similar characteristics. Some gyms will do better than average and some gyms will do worse, but there is some relationship between each gym’s performance. In this case, learning about one gym’s success in recruiting provides information about that gym as well as the entire population of Crossfit gyms.

To accomodate this structure (i.e. known as hierarchical or multi-level structure), we add parent nodes to the gym-level coefficients in the previous generative DAG (stored in `graph2`) and change the statistical model for those gym-level coefficients as well. The partial pooling model generative DAG (`graph3`) can be specified as:

```{r}
graph3 = dag_create() %>%
  dag_node("Number of Signups","k",
           rhs = binomial(nTrials,theta),
           data = gymDF$nSigned) %>%
  dag_node("Signup Probability","theta",
           child = "k",
           rhs = 1 / (1+exp(-y))) %>%
  dag_node("Number of Trials","nTrials",
           child = "k",
           data = gymDF$nTrialCustomers) %>%
  dag_node("Linear Predictor","y",
           rhs = alpha + beta * x,
           child = "theta") %>%
  dag_node("Yoga Stretch Flag","x",
           data = gymDF$yogaStretch,
           child = "y") %>%
  dag_node("Gym Intercept","alpha",
           rhs = normal(mu_alpha,sd_alpha),
           child = "y") %>%
  dag_node("Gym Yoga Slope Coeff","beta",
           rhs = normal(mu_beta,sd_beta),
           child = "y") %>%
  dag_node("Avg Crossfit Intercept","mu_alpha",
           rhs = normal(-1,1.5),
           child = "alpha") %>%
  dag_node("Avg Crossfit Yoga Slope","mu_beta",
           rhs = normal(0,0.75),
           child = "beta") %>%
  dag_node("SD Crossfit Intercept","sd_alpha",
           rhs = uniform(0,3),
           child = "alpha") %>%
  dag_node("SD Crossfit Yoga Slope","sd_beta",
           rhs = uniform(0,1.5),
           child = "beta") %>%
  dag_plate("Gym","j",
            nodeLabels = c("alpha","beta"),
            data = gymDF$gymID,
            addDataNode = TRUE) %>%
  dag_plate("Observation","i",
            nodeLabels = c("k","x","j",
                           "nTrials","theta","y"))

graph3 %>% dag_render()
```

::: callout-note
**Model with partially pooled observations** - all gyms share common parent nodes. The data informs both the parameters of the parent nodes as well as the parameters of their children, i.e. the gym-level coefficients. Think of gyms as children of the Crossfit chain - each child is different, but probably shares some underlying characterstics.
:::

Take note of the top row of random variables in the generative DAG above, which characterize the population of gyms. Think of the first two nodes as characterizing the base conversion rate for the population of crossfit gyms - one represents an expectation of any single gym’s intercept and the other represents how much deviation should be expected between the intercept’s of all the gyms. For example, a high `mu_alpha` and a small `sd_alpha` would suggest any randomly chosen Crossfit gym will have a high base-level probability for membership sign-ups and additionally, no matter which gym is chosen, its likely to have a similarly high base-level probability.

For priors, we used our previous prior for gym-level intercept to now be the prior for its parent, `mu_alpha` which is the expected base intercept for any Crossfit gym. The standard deviation component, `sd_alpha`, uses a uniform distribution that is twice the standard deviation prior for `mu_alpha`; chosen so the variation in base conversion rates will be on a similar scale to our own uncertainty in the corresponding mean, i.e. `mu_alpha`.

The next two nodes atop the generative DAG suggest that every gym’s slope coefficient for yoga is drawn from a distribution characterized by an average effect of yoga stretching for any Crossfit gym, `mu_beta`, and a standard deviation, `sd_beta` measuring how likely any individual gym’s effect might deviate from the average. Priors are chosen as was done for the first two nodes.

Through these population parameters, each gym’s $\alpha_j$ and $\beta_j$ are related; in cases of minimal gym data, the population parameters will be very important in estimating gym performance, conversely, in cases a gym shows strong deviation from the population parameters over a lot of trial customers, then these population parameters will get ignored.

With generative DAG in place, we can compute and visualize the posterior in the standard way:

```{r}
drawsDF = graph3 %>% dag_numpyro()
drawsDF %>% dagp_plot()
```

The figure below shows a slightly different snapshot of the posterior distribution. In addition to the partial pooling posterior, the previous *no-pooling* posterior is also put on the graph for comparison. This will help us see what gains are made (or not) by adding the four hyperparameters (i.e. `mu_alpha`,`mu_beta`,`sd_alpha`,`sd_beta`).

![](images/clipboard-4243588728.png)

Visually comparing the estimates of the two models helps us learn how to think about the multi-level model’s (i.e. partial pooling) interpretation:

-   Uncertainty interval widths have shrunk; especially for gym/stretching combinations that have yet to be seen. For example, gym10 has only two observations, both yoga, no traditional stretching observations. Now look at the change in , the base conversion rate parameter without yoga. Its a narrower interval than with no pooling. The narrowness is because we have learned about the effects of yoga from the other gyms and hence, can reason better about this unobserved base rate. This base rate remains high because we have also learned that differences due to yoga are never that dramatic.

-   There is increased confidence that yoga stretching would be effective at each gym. Look at the rightward shift of most of the partial pooling `beta` estimates. Also, `mu_beta`, the average effect of yoga is clearly greater than zero; hence, it seems like a good idea for Crossfit to promote yoga.

-   Not all gym’s are equal. Looking at gym 9 where there are 3 time periods of observations - one with traditional stretching that seemed better than the others. From the partial-pooling posterior, you see a rightward shift of `beta_9` as compared to the no-pooling model because we’ve learned that yoga works at other gyms. The uncertainty interval is clearly both positive and negative indicating we need more data to confidently dismiss or accept the idea of yoga.

-   Both `alpha` and `beta` intervals in the partial pooling model often (but not always) get pulled towards their means, `mu_alpha` and `mu_beta` respectively. Because we are sharing information through these parents, their impact gets felt at the gym level - especially in cases of gym’s without much data. For example, gym 12 only has one time period of observation, so both `alpha_12` and `beta_12` intervals are pushed towards the expected values of their parent nodes. Gym 5 has alot of data and as a result does not respond to these parental node influences.

-   The interval for `sd_alpha` being away from zero suggests that there is alot of deviation between gyms in terms of average expected base conversion rate. Recruitment rate is quite gym-specific. We can also see this in the original data as gym 10 seems exceptional and gym 2 needs new management.

Overall, the partial pooling model is mildly helpful in reducing our uncertainty. Perhaps the best conclusion enabled by partial pooiling is that `mu_beta` being greater than zero; yoga is helpful, a point that was not crystal clear when just looking at the gym data. More importantly, this partial-pooling model tells a story of related gyms where insights at one gym help inform our opinion about other gyms.

#### Posterior predictive checks

While these more confident and revealing estimates seem like a good thing, a posterior predictive check is in order to ensure that our model is still capable of producing data that might occassionally look like the actual data that we saw. Below is code for the posterior predictive check - its identical to the no-pooling model as we do NOT need to worry about the hyperparameters. As long as we have the parent-nodes of the linear-predictor, we do not need to worry about how they were generated by the grand-parent nodes.

```{r}
simDF = gymDF %>% select(-nSigned)

## get 20 random draws of posterior
paramsDF = drawsDF %>%
  slice_sample(n=20) %>%
  mutate(sampDrawID = row_number())

tidyParamDF = paramsDF %>% 
    pivot_longer(cols = -sampDrawID) %>%
    mutate(gymID = str_extract(name, '[0-9]+')) %>%
    mutate(gymID = as.integer(gymID)) %>%
    mutate(paramName = str_extract(name, '[:alpha:]+'))

## combine needed info
simDF = simDF %>% left_join(tidyParamDF, by = "gymID") 

## get relevant alpha and beta on same row
simPredDF = simDF %>%
  pivot_wider(id_cols = gymID:sampDrawID,
              names_from = paramName,
              values_from = value)

## if no yoga, then y=alpha. If yoga,then y=alpha+beta
simPredDF = simPredDF %>%
  mutate(y = ifelse(yogaStretch == 0, 
                    alpha, alpha+beta)) %>%
  mutate(theta = 1 / (1 + exp(-y))) %>%
  mutate(nSigned = rbinom(n = n(),
                          size = nTrialCustomers,
                          prob = theta))

obsDF = tibble(obs = gymDF$nSigned)

colors = c("simulated" = "cadetblue", 
           "observed" = "navyblue")

## make plot
ggplot(simPredDF) +
  stat_density(aes(x=nSigned, 
                   group = sampDrawID,
                   color = "simulated"),
               geom = "line", 
               position = "identity") +
  stat_density(data = obsDF, aes(x=obs, 
                                 color = "observed"),
               geom = "line",  
               position = "identity",
               lwd = 2) +
  scale_color_manual(values = colors) +
  labs(x = "# of Monthly Signups",
       y = "Density Estimated from Data",
       color = "Data Type")
```

The figure above is encouraging!! It looks like the actual data might get generated from our posterior distribution. The partial pooling model seems to work! Hooray! Next chapter we interpret the results of this model for decision making.

## Compelling decisions and actions under uncertainty

We now continue last chapter’s investigation of Crossfit Gym’s use of yoga. Our goal is to bring the mathematical estimates of our posterior distribution’s parameters back into the real-world with compelling and visually-based recommendations. To do this, we 1) define our *outcomes of interest*, 2) *compute a posterior distribution* for those outcomes, and 3) communicate our beliefs about those outcomes by *visualizing the outcomes of interest*.

### Outcomes of interest

Our main outcome of interest is signup probability. We will investigate signup probability by investigating that particular node in relation to our decision. Representing this is the deceptively simple generative decision DAG shown below.

```{r}
dag_create() %>%
  dag_node("Yoga Stretching?","yoga",
           dec = TRUE) %>%
  dag_node("Signup Probability","prob") %>%
  dag_edge("yoga","prob") %>%
  dag_plate("Stretch Type","",
            nodeLabels = c("yoga","prob")) %>%
  dag_render(shortLabel = TRUE, wrapWidth = 12)
```

The top-down narrative of the generative decision DAG above is as follows. Crossfit Gyms decides on whether to offer yoga stretching and the sign-up probability across their gyms changes as a result. Our job is to quantify this change and form an opinion on whether to offer yoga stretching or not across the gyms; we will form our opinion using the posterior distribution.

### Computing posterior distributions for outcomes of interest

The last model in the previous chapter gets us the generative DAG of Figure [24.1](https://www.causact.com/compelling-decisions-and-actions-under-uncertainty#fig:partPoolingGM2). Running it through the `dag_numpyro()` function yields a posterior distribution, but our job as analysts is not over. We now have to make sense of the posterior distribution and communicate its implications to stakeholders.

Rerun the analysis at the end of the previous chapter concluding with:

```{r}
drawsDF = graph3 %>% dag_numpyro()
```

where `drawsDF` is a representative sample of the posterior distribution associated with partial pooling model. This data frame is a sample of 4,000 draws of 28 variables. Let’s list the 28 variables using `names(drawsDF)`.

```{r}
names(drawsDF)
```

Perusing our posterior’s 28 random variables, we might notice that `theta` is not one of them. Bummer! We are going to need to do some coding to get a representative sample for `theta`.

`theta` is omitted because it is a calculated node; its realization is a deterministic function of its parent `y` which is also a calculated node. Note that an oval’s double-perimeter is the visual clue for a calculated node. Its parents include both random and observed nodes. So to actually determine `theta`, we need follow the generative recipe from grandparents (`alpha`,`beta`,`j`, and `x`) to grandchild (`theta`) via linear predictor `y`.

```{r}
graph3 %>% dag_render()
```

Notice, we do not care about `mu_alpha` and the other parents of `alpha` and `beta`. Once we have a representative sample of `alpha` and `beta`, plus the observed nodes `j` and `x` , then we can calculate `theta`.

For example, let’s estimate the additional sign-up probability when using “Yoga Stretch” at gym number 12?

1.  Get a single draw of the required nodes from the representative sample:

```{r}
draw = drawsDF %>% 
  slice_sample(n=1) %>% # get a random draw
  select(alpha_12,beta_12)
```

2.  Compute a value for the linear predictor with and without yoga (i.e. `x=1` and `x=0`, respectively) using the formula shown for this node in the generative decision DAG. Plugging in for `x`, we get the two different values of linear preditor `y` that interest us: `x=1` $\rightarrow y_{yoga} = \alpha{12}+\beta{12}*1$) and without yoga (`x=0` $\rightarrow y_{trad} = \alpha{12}+\beta{12}*0 = \alpha_{12}$):

```{r}
draw = draw %>%
  mutate(y_yoga = alpha_12 + beta_12 * 1) %>%
  mutate(y_trad = alpha_12)
```

3.  Compute the values for `theta` with and without yoga at gym12 using the inverse-logit function (i.e. the link function formula shown for this node in the generative decision DAG):

```{r}
draw = draw %>%
  mutate(theta_yoga = 1 / (1+exp(-y_yoga))) %>%
  mutate(theta_trad = 1 / (1+exp(-y_trad)))
```

4.  Compute the increased probability of signup when using yoga at gym12:

```{r}
draw = draw %>%
  mutate(probIncDueToYoga = 
           theta_yoga - theta_trad)
```

And now, viewing these computed values:

```{r}
draw
```

we see that for this draw, around 21% of yoga trial customers end up signing up for a membership versus 15% of customers doing traditional stretching. According to this draw then, approximately 6% is the signup probability increase due to yoga stretching.

Let’s get a little mathematical and declare this difference in probabilities to be a new random variable `Z_{gymID}` like:

$$
Z_{12} \equiv \textrm{ Probability increase due to yoga stretching at gym 12}
$$

The following code scales the above four steps for creating one draw to creating a column of representative samples for $Z_{12}$:

```{r}
postDF = drawsDF %>%
  select(alpha_12,beta_12) %>%
  mutate(y_yoga = alpha_12 + beta_12 * 1) %>%
  mutate(y_trad = alpha_12) %>%
  mutate(theta_yoga = 1 / (1+exp(-y_yoga))) %>%
  mutate(theta_trad = 1 / (1+exp(-y_trad))) %>%
  mutate(z_12 = theta_yoga - theta_trad)
```

The column we just made, `z_12`, is our posterior distribution for the change in probability due to yoga stretching. We can visualize and summarize this posterior density:

```{r}
gf_density(~ z_12, data = postDF)
```

```{r}
summary(postDF$z_12)
```

and form only a mild opinion that yoga is probably helpful at gym 12 (i.e. most of the plausible values are above zero).

### Visual advocacy for decisions which improve outcomes of interest

To convert posterior probabilities into decisions, we want a visual that communicates a recommendation. In business, a random variable outcome of interest is usually made more compelling by converting it to some measure of money. Let’s assume that that the value of each new customer is estimated to be \$500 in *net present value* terms. We can then create a mathematical formula for value created by yoga stretching per trial customer:

$$
\textrm{ValueOfYogaStretchingForGym12} = 500 \times Z_{12}
$$

and also, represent it computationally:

```{r}
moneyDF = postDF %>% mutate(ValueCreated = 500 * z_12)
```

We now have a random variable of the per customer profit estimate if gym12 adopts yoga stretching for the next year versus not adopting yoga stretching. We can summarize this random variable graphically:

```{r}
gf_density(~ ValueCreated, data = moneyDF) |> gf_refine(scale_x_continuous(labels = dollar_format()))
```

which shows both the plausibility of losing money as well as making up to say \$100 per customer as a result of the decision. We can find some additional metrics:

```{r}
summary(moneyDF$ValueCreated)
```

telling us the median value for gym12 is about \$24 of extra value per customer. This means that we assign a 50/50 chance to being above or below this number in terms of value created per customer. Hence, if it costs extra money (e.g. licensing fees, additional labor expense, additional equipment expense, etc.), say \$25 per customer to offer the class, then this investment at gym12 might not be recovered.

Lastly, since continuous probability estimates are sometimes difficult for decision makers (and ourselves) to understand, we can create a discrete probability distribution by creating bins:

```{r}
breaks = c(-1000,-20,0,20,40,60,80,100,1000)
labels = c("<-$20","-$20 - $0","$0 - $20",
           "$20 - 40$","$40 - $60","$60 - $80",
           "$80-$100","$100+")
bins = cut(moneyDF$ValueCreated, 
           breaks, 
           include.lowest = T, 
           right=FALSE, 
           labels=labels)
moneyDF$bins = bins  ## add new column

## add label for percentage in each bin
plotDF = moneyDF %>%
  group_by(bins) %>%
  summarize(countInBin = n()) %>%
  mutate(pctInBin = countInBin / sum(countInBin)) %>%
  mutate(label = paste0(round(100*pctInBin,0),"%")) %>%
  mutate(makeMoney = ifelse(bins %in% levels(bins)[1:2],
                            "Not Profitable",
                            "Profitable"))

## Create more interpretable plot
plotDF %>%
  ggplot(aes(x = bins, y = pctInBin, 
             fill = makeMoney)) +
  geom_col(color = "black") +
  geom_text(aes(label=label), nudge_y = 0.015) +
  xlab("Value Added Per Trial Customer") +
  ylab("Probability of Outcome") +
  scale_fill_manual(values = c("lightcoral",
                               "cadetblue")) +
  theme(legend.position = "none") +
  coord_flip() +
  ggtitle("Making Yoga Stretching Mandatory for Gym 12")
```

From the above, we can easily talk to any decision maker about the possiblities for various outcomes. For example, summing the bottom two percentages tells us there is an approximately 14% chance of yoga stretching creating negative value. Additionally, if Crossfit anticipates a cost of \$20 per customer, then there is a 46% chance of not breaking even by using this policy.

## The journey continues

[CONGRATULATIONS!!!]{.smallcaps} You have journeyed through a challenging path and persevered. You are now equipped to “release the BAW”. In your journey, you learned to coordinate three languages – 1) graphical models to aid business communication , 2) probability theory for Bayesian inference, and 3) the R programming language to manipulate and visualize data as well as to give you access to statistical computation. The use of generative DAGs from the `causact` package enabled you to reduce the friction between these languages. As a tradeoff, however, your knowledge in any one of the languages only runs so deep. Now is the time to keep going and pick from the below resources to further develop your skills.

### Additional readings

-   Michael Betancourt (2018Michael Betancourt. 2018. *Towards a Principled Bayesian Workflow (RStan)*. <https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html>.) provides the *scientific philosophy of Bayesian statistics* and associated computational methods with mathematical rigor and beautiful pictures.

-   Gelman, Hill, and Vehtari (2020Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. *Regression and Other Stories*. Cambridge University Press.) dives much deeper into *multilevel regression* than is done in this text. These three authors are gurus of applied Bayesian statistics.

-   Grolemund and Wickham (2018Grolemund, Garrett, and Hadley Wickham. 2018. “R for Data Science.”) is a fundamental read on *R programming*, data manipulation, and using R in analytics workflows.

-   McElreath (2020McElreath, Richard. 2020. *Statistical Rethinking: A Bayesian Course with Examples in r and Stan*. CRC press.) made a pedagogical masterpiece that teaches more mathematically rigorous *Bayesian statistics* in a fun way. I highly recommend this as the next book in your journey.

-   Wilke (2019Wilke, Claus O. 2019. *Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures*. O’Reilly Media.) is a wonderful read for exploring *data visualization*. All the code for numerous beautiful `ggplots` is made available to the reader.
