---
title: "causact"
format:
  html:
    toc: true
    df-print: kable
---

```{r setup}
#| output: false
library(tidyverse)
library(ggformula)
library(mosaic)
library(posterior)
library(bayesplot)
library(causact) # causact::install_causact_deps()

theme_set(theme_minimal())

set.seed(123)
```

## Introduction

There are three languages at the core of any data analysis: narrative, math, and code.

-   **Narrative**: This is the language of your real-world understanding.

-   **Math**: This is the language describing a faithful mathematical representation of your real-world narrative whose solution or output might turn your data into data-driven insight.

-   **Code**: This is the programming language, the code required to generate output that exactly or approximately solves your math problem or generates the desired output.

Using Bayesian inference is the provably best method of combining data with domain knowledge to extract interpretable and insightful results that lead us towards better outcomes.

This is the job of the *business analyst*; to drive better outcomes by compelling actions that are aligned with strategy and informed by data.

![](images/clipboard-2076919182.png)

![](images/clipboard-613416536.png)

The business analyst workflow consists of transforming the real-world into a compact mathematical representation that we can use, along with data, to computationally search for insights. These computational insights must then be translated back into the real-world in such a way that they inspire action which leads to improved outcomes. This is your role: transform the real-world into the math/computer world, extract mathematical/statistical/computational insight, then transform that insight into a compelling real-world call for action.

![](images/clipboard-3434935492.png)

## Representing uncertainty

[Real-world uncertainty]{.smallcaps} makes decision making hard. Conversely, without uncertainty decisions would be easier. For example, if a cafe knew exactly 10 customers would want a bagel, then they could make exactly 10 bagels in the morning; if a factory’s machine could make each part exactly the same, then quality control to ensure the part’s fit could be eliminated; if a customer’s future ability to pay back a loan was known, then a bank’s decision to underwrite the loan would be quite simple.

In this section, we learn to represent our real-world uncertainty (e.g. demand for bagels, quality of a manufacturing process , or risk of loan default) in mathematical and computational terms. We start by defining the ideal mathematical way of representing uncertainty, namely by assigning a *probability distribution* to a *random variable*. Subsequently, we learn to describe our uncertainty in a *random variable* using *representative samples* as a pragmatic computational proxy to this mathematical ideal.

Think of a random variable as a mapping from potential outcomes to numerical values representing the probability we assign to each outcome.

| Outcome | Probability |
|---------|-------------|
| Heads   | 0.5         |
| Tails   | 0.5         |

While the table might be adequate to describe the mapping of coin flip outcomes to probability, as we make more complex models of the real-world, we will want to take advantage of the concise (and often terse) notation that mathematicians would use. In addition, we want to gain fluency in *math world* notation so that we can successfully traverse the bridge between *real world* and *math world*.

Above, a random variable was introduced as a mapping of outcomes to probabilities. And, this is how you should think of it most of the time. However, to start gaining fluency in the math-world definition of a random variable, we will also view this mapping process as not just one mapping, but rather a sequence of two mappings: 1) the first mapping is actually the “true” probability-theory definition of a **random variable** - it maps all possible outcomes to real numbers, and 2) the second mapping, known as a **probability distribution** in probability theory, maps the numbers from the first mapping to real numbers representing how plausibility is allocated across all possible outcomes - we often think of this allocation as assigning probability to each outcome.

The terse math-world representation of a mapping process like this is denoted:

$$
X:\Omega \rightarrow \mathbb{R}
$$

, where you interpret it as “random variable $X$ maps each possible outcome in sample space omega to a real number.”

The second mapping process then assigns a probability distribution to the random variable. By convention, lowercase letters, e.g. $x$, represent actual observed outcomes. We call $x$ the *realization* of random variable $X$ and define the mapping of outcomes to probability for every $x \in X$ (read as $x$ “in” $X$ and interpret it to mean “for each possible realization of random variable $X$”).

In this book, we will use $f$, to denote a function that maps each possible realization of a random variable to its corresponding plausibilty measure and use a subscript to disambiguate which random variable is being referred to when necessary. For the coin flip example:

$$
f_X: X \rightarrow [0,1]
$$

Despite all this fancy notation, for small problems it is sometimes the best course of action to think of a random variable as a lookup table as shown here:

| Outcome | Realization ($x$) | $f(x)$ |
|---------|-------------------|--------|
| Heads   | 1                 | 0.5    |
| Tails   | 0                 | 0.5    |

and where $f(x)$ can be interpreted as the plausability assigned to random variable $X$ taking on the value $x$.

The Bernoulli distribution (or a Bernoulli random variable):

| Outcome | Realization ($x$) | $f(x)$  |
|---------|-------------------|---------|
| Success | 1                 | $\pi$   |
| Failure | 0                 | $1-\pi$ |

With all of this background, we are now equipped to model uncertainty in any observable data that has two outcomes. The way we will represent this uncertainty is using two forms: 1) a graphical model and 2) a statistical model.

```{r}
dag_create() |>
  dag_node(
    descr = "Coin flip",
    label = "X") |>
  dag_render(shortLabel = TRUE)
```

$$
\begin{aligned}X &\equiv \textrm{Coin flip outcome with heads}=1 \textrm{  and tails}=0.\\X &\sim \textrm{Bernoulli}(p)\end{aligned}
$$

Instead of working with probability distributions directly as *mathematical* objects, we will most often seek a representative sample and treat them as *computational* objects (i.e. **data**). For modelling a coin flip, the representative sample might simply be a list of $0$ and $1$ generated by someone flipping a coin or by a computer simulating someone flipping a coin.

[Turning a mathematical object into a representative sample using R]{.smallcaps} is quite easy as R can be used to generate random outcomes from just about all well-known probability distributions.

```{r}
rbern(7, p = 0.5)
```

```{r}
set.seed(123) 

# Create dataframe of coinflip observations
numFlips = 50 ## flip the coin 50 times
df = data.frame(
  flipNum = 1:numFlips,
  coinFlip = rbern(n=numFlips,prob=0.5)
  ) %>%
  mutate(headsProportion = cummean(coinFlip))
  
# Plot results
ggplot(df, aes(x = flipNum, y = headsProportion)) + 
  geom_point() +
  geom_line() + 
  geom_hline(yintercept = 0.5, color = "red") +
  ggtitle("Running Proportion of Heads") +
  xlab("Flip Number") + 
  ylab("Proportion of Heads") +
  ylim(c(0,1))
```

$$
\begin{aligned}X_i &\equiv \textrm{If passenger } i \textrm{ shows up, then } X=1 \textrm{. Otherwise, } X = 0 \textrm{. Note: } i \in \{1,2,3\}.\\X_i &\sim \textrm{Bernoulli}(p = 0.85)\\Y &\equiv \textrm{Total number of passengers that show up.}\\Y &= X_1 + X_2 + X_3\end{aligned}
$$

```{r}
numFlights = 1000 ## number of simulated flights
probShow = 0.85 ## probability of passenger showing up

# choose random seed so others can
# replicate results
set.seed(111) 

pass1 = rbern(n = numFlights, prob = probShow)
pass2 = rbern(n = numFlights, prob = probShow)
pass3 = rbern(n = numFlights, prob = probShow)

# create data frame (use tibble to from tidyverse) 
flightDF = tibble(
  simNum = 1:numFlights,
  totalPassengers = pass1 + pass2 + pass3
)

# transform data to give proportion
propDF = flightDF %>% 
  group_by(totalPassengers) %>% 
  summarize(numObserved = n()) %>%
  mutate(proportion = numObserved / sum(numObserved))

# plot data with estimates
ggplot(propDF, 
       aes(x = totalPassengers, y = proportion)) +
  geom_col() +
  geom_text(aes(label = proportion), nudge_y = 0.03)
```

Simulation will always be your friend in the sense that if given enough time, a simulation will always give you results that approximate mathematical exactness. The only problem with this friend is it is sometimes slow to yield representative results. In these cases, sometimes mathematics provides a shortcut. The shortcut we study here is to define a probability distibution.

$$
\begin{aligned}Y &\equiv \textrm{Total number of passengers that show up.}\\Y &\sim \textrm{Binomial}(n = 3, p = 0.85)\end{aligned}
$$

```{r}
# transform data to give proportion
propExactDF = tibble(totalPassengers = 0:3) %>%
  mutate(proportion = 
           dbinom(x = totalPassengers,
                  size = 3,
                  prob = 0.85))

# plot data with estimates
ggplot(propExactDF, aes(x = totalPassengers, 
                        y = proportion)) +
  geom_col() +
  geom_text(aes(label = proportion), 
            nudge_y = 0.03)
```

The above code is both simpler and faster than the approximation code run earlier. In addition, it gives exact results. Hence, when we can take mathematical shortcuts, we will to save time and reduce the uncertainty in our results introduced by approximation error.

Our representation of uncertainty takes place in three worlds: 1) the real-world - we use graphical models (i.e. ovals) to convey the story of uncertainty, 2) the math-world - we use statistical models to rigorously define how random outcomes are generated, and 3) the computation-world - we use R functions to answer questions about exact distributions and representative samples to answer questions when the exact distribution is unobtainable.

## Joint distributions tell you everything

The most complete method of reasoning about sets of random variables is by having a *joint probability distribution*. A joint probability distribution, , assigns a probability value to all possible assignments or realizations of sets of random variables. The goal of this section is to 1) introduce you to the notation of joint probability distributions and 2) convince you that if you are given a joint probability distribution, then you would be able to answer some very useful questions using probability.

### Joint distributions

### Marginal distributions

### Conditional distributions

### Limitations of joint distributions

[Why don’t we just use joint probability distributions all the time?]{.smallcaps} Despite the expressive power of having a joint probability distribution, they are not that easy to directly construct due to the *curse of dimensionality*. As the number of random variables being considered in a dataset grows, the number of potential probability assignments grows too. Even in the era of big data, this curse of dimensionality still exists.Generally speaking, an exponential increase is required in the size of the dataset as each new descriptive feature is added.

Let’s assume we have $n$ random variables with each having $k$ values. Thus, the joint distribution requires $k^n$ probabilites. Even if $k=2$ and $n=34$, this leads to 17,179,869,184 possibilities (over 17 billion). To make this concrete, a typical car purchase decision might easily look at 34 different variables (e.g. make, model, color, style, financing, etc.). So, to model this decision would require a very large joint distribution which actually dwarfs the amount of data that is available. As a point of comparison, well under 100 million motor vehicles were sold worldwide in 2019 - i.e. less than one data point per possible combination of features. Despite this “curse”, we will learn to get around it with more compact representations of joint distributions. These representations will require less data, but will still yield the power to answer queries of interest; just as if we had access to the full joint distribution.

## Graphical models tell joint distribution stories

Graphical models serve as compact representations of joint probability distributions.

It’s okay to not draw a perfect model when you are first attacking a problem.

```{mermaid}
graph TD
    X[ X: It is raining ]
    Y[ Y: The curb is wet ]
    X --> Y
```

The diagram above is what mathematicians and computer scientists call a *graph*. To them, a *graph* is a set of related objects where the objects are called *nodes* and any pair of related nodes are known as an *edge*. For our purposes, a *node* is a visual depiction of a random variable (i.e. an oval) and the presence of a visible *edge* indicates a relationship between the connected random variables.

Our first vetting should be to ensure that nodes can be modelled as a random variable with each node representing a mapping of real-world outcomes to numerical values (see the “Representing uncertainty” section above).

Any variable we include in our models needs to be clearly defined and fulfill the requirements of a random variable; afterall, this is our key technique for representing the *real-world* in mathematical terms.

Mathematically, our goal is to have a joint distribution over the random variables of interest. Once we have that, we can answer any probability query we might have using marginal and conditional distributions (see the “Joint distributions tell you everything” section above).

For all but very simple models like the one above, a tabular representation of a joint distribution becomes unmanageable (computationally, cognitively, and statistically).

To overcome this, we use a different, more-compact structure - called a *Bayesian Network* (BN). Bayesian networks are compressed, easier-to-specify recipes for generating a full joint distribution. So, what is a BN? It is a type of *graph* (i.e. nodes and edges), specifically a *directed acyclic graph* (DAG), with the following requirements:

1.  All nodes represent random variables. They are drawn using ovals. By convention, a constant is also considered a random variable.

2.  All edges are pairs of random variables with a specified direction of ordering. By convention, the direction is from parent node to child node. While not always true, it is usually good to have edges reflecting a causal ordering. Edges are drawn using arrows where the tail originates from the parent and the tip points to a child.

3.  Edges are uni-directional, they must only flow in one direction between any two nodes (i.e. *directed*).

4.  The graph is *acyclic* meaning there are no cycles. In other words, if you were to put your finger on any node and trace a path following the direction of the edges, you would not be able to return to the starting node.

5.  Any child node’s probability distribution will be expressed as conditional solely on its parents’ values, i.e. $P(\text{child}\mid \text{parent(s)})$; this assumption is what enables more compact representations of joint distributions.

The one edge, $Y \rightarrow X$, means that our uncertainty in $X$ is measured using a probability function conditional on its parent - i.e. $P(X|Y)$. Since there are no edges leading into $Y$, it has no parents, its probability distribution is $P(Y)$ . With those two probabilities in place, we can recover the joint distribution, $P(X,Y)$, based on the definition of conditional probability $P(X,Y) = P(Y) \times P(X|Y)$.

If there are no edges between $X$ and $Y$, then the joint distribution is recovered via $P(X,Y)=P(X)\cdot P(Y)$. In this case, the two random variables are independent and that is not a model structure consistent with the scenario we are exploring. If the ordering of nodes for $X$ and $Y$ are reversed such that $Y$ is the parent of $X$, then the joint distribution would be recovered via $P(X,Y)=P(Y)\cdot P(X\mid Y)$. This math is not consistent with the story we are trying to tell. The story is that rain causes the curb to be wet, not the other way around.

The general rule for a probabilistic graphical model is that its joint distribution can be factored using the chain rule formula for DAGs where $P(X_1, X_2, \ldots, X_n) = \prod_I P(X_i|Parents(X_i))$. Hence, to model any one random variable, we only have to model its relationship to its parents instead of modelling its relationship to all of the random variables included in the model.

See [mathematicalmonk](www.youtube.com/@mathematicalmonk)’s youtube video on how joint distributions are compactly represented using DAGs here: <https://youtu.be/3XysEf3IQN4>.

## Bayesian inference on graphical models

*Plausible reasoning* - an allocation of credibility/plausibility to all possible explanations of the observations.

Plausible reasoning in the math world is called Bayesian inference - a methodology for updating the credibility/plausibility of the various explanations in our mathematical representation of the world as more data becomes available.

Let’s walk through a hypothetical argument between two scientists as to the effect of a new drug on cognition in Alzheimer's disease (AD).

Assume the two scientists have competing models for what effect the drug will have on the cognition of patients with AD:

1.  *The Optimist Model (Model1)* : This model is from a scientist who is very optimistic about the effect of the drug and argues that 70% of all patients will see at least a 5% increase in cognition.

2.  *The Pessimist Model (Model2)* : This model is from a scientist who is very pessimistic about the effect of the drug and argues that 20% of all patients will see at least a 5% increase in cognition.

These two scientists recognize and respect their differing opinions - they agree to test the new drug in one patient. They hire you as a data analyst and ask you to make the decision as to whose model is more credible in light of the test’s results. Your job is to allocate credibility to the two competing models both before seeing the test results and after seeing the test results. Initially, you might not have any reason to favor one model over another, but as data is collected, your belief in whose model is more plausible/believable/credible will change. For example, if cognition decreases, then the pessimist model would seem more credible. Your task is to allocate credibility (using probability theory) to the various models/explanations, before and after the data becomes available.

### Building a graphical model of the real-world

The first step is to create a graphical model/representation of the real world.

Starting simple, *let’s only imagine that we test the drug in one patient* and our single data point (i.e. whether cognition increases by at least 5% or not) follows a Bernoulli distribution.

The graphical model is simply:

```{r}
dag_create() |>
  dag_node(
    descr = "Cognition increases",
    label = "X") |>
  dag_render(shortLabel = TRUE)
```

And, the statistical model with the mathematical details is represented like this:

$$
\begin{aligned}X \equiv& \textrm{ Cognition increases: } \\        & \textrm{ If cognition increases more than 5}\% \textrm{, then }X=1 \textrm{ otherwise, }X=0.\\X \sim  & \textrm{ Bernoulli}(\theta)\\\end{aligned}
$$

We have seen this model before when representing coin flips. Our data is analogous to heads or tails of a coin flip. The data will be reduced to a zero or one for each patient. If given $\theta$, we could generate data using `rbern(n=1, prob=theta)`, but, we do not know $\theta$; the reason we are looking at data is to answer the question: “what is $\theta$?”

From the story above, $\theta$ can only take on two values. In the optimistic model $\theta = 70\%$ and in the pessimistic model $\theta = 20\%$. So, we have two models of the world and are uncertain as to which one is more plausible. Without data, we have no reason to believe one scientist over another, so $P(\theta=70\%)=50\%$ and $P(\theta=20\%)=50\%$ - i.e. each scientist is equally likely to be correct. This is just like saying $P(\text{model1})=P(\text{model2})=50\%$. Before any data is considered, this allocation of credibility assigning probability to all the models being considered is called the *prior*. The prior is the initial probability allocated among all the possible models.

See <https://youtu.be/nCRTuwCdmP0> for gaining some intuition about prior probabilities.

So now, we can more completely specify our data story using both a graphical and a statistical model with specified prior probabilities. The graphical model is now two ovals representing our uncertainty in the probability of success and the observed cognition increase (random variable math-labels for $\Theta$ and $X$ are included for extra clarity in connecting the graphical model and the statistical model):

```{r}
dag_create() |>
  dag_node(
    descr = "Cognition increases",
    label = "X") |>
  dag_node(
    descr = "Success probability",
    label = "Θ",
    child = "X") |>
  dag_render()
```

And, the statistical model is represented like this:

$$
\begin{aligned}
X \equiv& \textrm{ Cognition increases: } \\
        & \textrm{ If cognition increases more than 5} \% \textrm{, then }X=1 \textrm{ otherwise, }X=0.\\
X \sim  & \textrm{ Bernoulli}(\theta)\\
\Theta \equiv& \textrm{ Success probability: } \\
\end{aligned}
$$ $$
\Theta \sim
\begin{array}{ccc}
  \textrm{Outcome } & \textrm{ Realization }(\theta) & f(\theta) \\
  \hline
  \textrm{Model1}  & \textrm{   70}\% & \textrm{  50}\% \\
  \textrm{Model2}  & \textrm{   20}\% & \textrm{  50}\% \\
\end{array}
$$

where the prior probability distribution for $\Theta$ is given in tabular form.

The graphical model and statistical model are two different ways of representing the same story. The graphical model is more visual and intuitive, while the statistical model is more precise and mathematical. Both are useful for different purposes, but both represents a generative model, i.e., a recipe for simulating real-world data observations. In this case, following the top-down flow of the graphical model, the recipe is:

1.  Simulate a potential success probability by randomly picking, with equal probability, either the optimist or pessimist model.
2.  Simulate a drug’s success by using the probability from (1) and generating a Bernoulli random variable with that probability of success.

We can easily show how to simulate an observation by writing code for the recipe in R:

```{r}
# Step 1: Simulate a potential success probability
theta = sample(size = 1, x = c(0.7, 0.2), prob = c(0.5, 0.5))
# Step 2: Simulate a drug's success
x = rbern(n = 1, prob = theta)
theta
x
```

Much of your subsequent work will use this notion of generative models as recipes. You will 1) *create generative models* that serve as skeleton recipes - recipes with named probability distributions and **unknown** parameters - for how real-world data arises, and 2) *inform models with data* by reallocating plausibility to the recipe parameters that are most consistent with the data.

A guiding principle for creating good generative models is that the generated data should mimic data that you, as a data analyst, believe is plausible. If the generative model outputs implausible data in high frequency, then your model is not capturing the essence of the underlying data story; your modelling assumptions will need work. When the generative model seems correct in the absence of data, then data can feed the updating process which sensibly reallocates prior probability so that model parameters that tend to generate data similar to the observed data are deemed more plausible.

### From model to joint distribution

The graphical model shows that we have uncertainty about two related random variables: 1) *Success Probability* ($\Theta$) *Cognition Increases* ($X$). Our assumption - built into our prior - is that one of our two models, $\theta = 20\%$ or $\theta = 70\%$, is correct.

::: callout-note
This implicit assumption that one of the considered generative models is correct is sometimes referred to as the *small world* assumption. See this comparison by Richard McElreath of *small world* versus *large world* highlighting the implications of this assumption: <https://youtu.be/WFv2vS8ESkk>.
:::

We are also confident about what collecting data from one patient might yield: either a single *success* or a single *failure* in terms of the cognition increase. Prior to collecting data, there are four combinations of model and data that are potential truths. Each combination’s prior plausibility, represented by $a\%$, $b\%$, $c\%$ and $d\%$, are elements of the table:

$$
\begin{array}{cc|cc}         &                  & \textrm{  Possible}                & \textrm{Models  }                  \\         &                  & \theta = 70\% & \theta = 20\% \\         \hline\textrm{Possible} & \textit{Success} & a\%   & b\%                     \\\textrm{Data}     & \textit{Failure} & c\%   & d\%\end{array}
$$

```{r}
# Simulate many samples from the generative model
samples <- map(1:10000, function(x) { theta = sample(size = 1, x = c(0.7, 0.2), prob = c(0.5, 0.5)); x = rbinom(size=1, n=1, prob=theta); tibble(theta, x) }) |> list_rbind()

head(samples)
```

```{r}
# Calculate joint probability distribution before seeing the data, and the marginal of theta (prior)
joint <- samples %>%
  group_by(theta, x) %>%
  summarize(count = n(), .groups = "drop") %>%
  mutate(p = count / sum(count))

joint

joint |> 
  group_by(theta) |> 
  summarize(prior = sum(p))
```

```{r}
tally(~ x + theta, data = samples, format = 'proportion')
```

### Bayesian updating of the joint distribution

$$
P(Model | Data) = \frac{P(Data|Model) \times P(Model)}{P(Data)}
$$

The above formula is called *Bayes' Theorem* and it is the mathematical engine that drives Bayesian inference. It is a simple formula, but it is very powerful. It tells us how to update our beliefs about the plausibility of various models in light of new data. The formula has four components:

1.  $P(Model | Data)$: This is the *posterior* probability of the model given the data. It is what we are trying to compute. It tells us how plausible each model is after seeing the data.
2.  $P(Data|Model)$: This is the *likelihood* of the data given the model. It tells us how likely the observed data is under each model.
3.  $P(Model)$: This is the *prior* probability of the model.
4.  $P(Data)$: This is the *marginal likelihood* of the data. It is a normalizing constant that ensures the posterior probabilities sum to one.

To use Bayes' Theorem, we need to compute the likelihood of the data under each model. This is done using the probability distributions defined in our statistical model.

Assume we observe a success, i.e. the patient’s cognition increases by at least 5%. The likelihood of this data under each model is:

-   Under Model1 ($\theta = 70\%$): $P(X=1|\theta=70\%) = 0.7$
-   Under Model2 ($\theta = 20\%$): $P(X=1|\theta=20\%) = 0.2$

The marginal likelihood of the data is computed by summing the likelihoods weighted by the prior probabilities:

$$
P(Data) = P(X=1) = P(X=1|\theta=70\%) \times P(\theta=70\%) + P(X=1|\theta=20\%) \times P(\theta=20\%) = 0.7 \times 0.5 + 0.2 \times 0.5 = 0.45
$$ Now, we can plug these values into Bayes' Theorem to compute the posterior probabilities:

-   For Model1 ($\theta = 70\%$):

$$
P(\theta=70\%|X=1) = \frac{P(X=1|\theta=70\%) \times P(\theta=70\%)}{P(X=1)} = \frac{0.7 \times 0.5}{0.45} \approx 0.778
$$

-   For Model2 ($\theta = 20\%$):

$$
P(\theta=20\%|X=1) = \frac{P(X=1|\theta=20\%) \times P(\theta=20\%)}{P(X=1)} = \frac{0.2 \times 0.5}{0.45} \approx 0.222
$$

So, after observing a success, the posterior probabilities are approximately 77.8% for Model1 and 22.2% for Model2. This means that after seeing the data, we believe that Model1 is more plausible than Model2.

```{r}
# Calculate joint probability distribution after seeing the data (x=1), and the marginal of theta (posterior)
joint <- samples %>%
  filter(x == 1) %>%
  group_by(theta, x) %>%
  summarize(count = n(), .groups = "drop") %>%
  mutate(p = count / sum(count))

joint

joint |> 
  group_by(theta) |> 
  summarize(prior = sum(p))
```

## Generative DAGs as scientific and statistical models

Up to this point, we specified a prior joint distribution using information from both a graphical model and a statistical model. In this section, we combine the two model types, graphical models and statistical models, into one visualization called the generative DAG.

The advantage of this is that the generative DAG unites real-world measurements (e.g. cognition increases) with their math/computation world counterparts (e.g. $X \sim \textrm{Bernoulli}(\theta)$) without requiring the cognitive load of flipping back-and-forth between the graphical and statistical models.

Moreover, generative DAGs, when combined with data, enable Bayesian inference to be automated (yay! no more manual calculations using Bayes rule).

### Generative DAGs

To build a generative DAG, we combine a graphical model with a *statistical model* into a single unifying representation.

::: callout-note
A statistical model is a mathematically expressed recipe for creating a joint distribution of related random variables. Each variable is defined either using a probability distribution or a function of the other variables. Statistical model and generative DAG will be used interchangeably. The term statistical model is just the mathematically expressed recipe. Generative DAG is the graphically expressed recipe with an embedded statistical model.
:::

The objective in building generative DAGs is to capture how we might go about simulating real-world measurements using the mathematical language we’ve been learning. Good generative DAGs are recipes for simulating a real-world observation in a way that reflects our domain knowledge. Then, with a generative DAG and some actual data, we will ask the computer to do Bayesian inference for us and to output an updated joint distribution of the random variables in our DAG.

The creation of a generative DAG mimics the creation of graphical models with accompanying statistical models. Both the structure of the DAG and the definition of the random variables should reflect what a domain expert knows, or wants to assume, about the data generating process being investigated.

Here is the generative DAG for the scenario described before. We have seen some elements of this DAG before. For example, the *Cognition increase*s $X$ node, whose prominent feature is that it can only take on two-values, *yes* or *no*. We model these binary-valued real-world outcomes with the probabilistic mapping (Bernoulli PMF) $x \sim \textrm{Bernoulli}(\theta)$.

```{r}
dag_create() |>
  dag_node(
    descr = "Cognition increases",
    label = "x",
    rhs = bernoulli(theta)) |>
  dag_node(
    descr = "Success probability",
    label = "theta",
    child = "x",
    rhs = uniform(0,1)) |>
  dag_render()
```

Other elements of the DAG are new. Notice, `theta` is now spelled out phonetically as opposed to using the actual Greek letter math symbol of $\theta$. While this need not be done when drawing a generative DAG by hand, we phonetically spell out the Greek letter $\theta$ as `theta` here because computer code and computer-generated DAG visuals lack easy support for the Greek alphabet. Additionally, and more importantly, we switch to using lowercase notation for both `x` and `theta` as we want to highlight that ovals in generative DAG models represent a single realization of the random variable. Later we will introduce the representation of more than one realization.

### Building generative models

As we build these generative DAGs, it is often easiest to build and read them from bottom to top; start by converting your target measurement, in this case whether the cognition of patients with AD increases after treatment with a new drug, to a brief real-world description (e.g. *Cognition increases*). This description is the top-line of the oval. For rigor, a more formal definition should be stored outside of the generative DAG in a simple text document that can be referenced by others.

Every node will also have a mathematical line (e.g. $x \sim \textrm{Bernoulli}(theta)$). The mathematical line will always follow a three-part structure of 1) Left-hand side, 2) Relationship Type, and 3) Right-hand side:

Each line of a statistical model follows one of two forms: or

-   **Left-hand side (LHS)**: a mathematical label for a realization of the node (e.g. $x$ or $y$ or whatever symbol/word you choose). The purpose of the label is to have a very short way of referring to a node. To that end, you cannot include spaces in this label and try to keep it at five letters or less.

-   **Relationship Type**: There are two types of relationships that can be specified:

1.  a *probabilistic* relationship, denoted by the $\sim$ symbol, where the LHS node’s value is determined with some uncertainty/randomness as a function of its inputs, such as the outcome of a coin flip. $LHS \sim \textrm{Probability mass/density function}$ Or,

2.  a *deterministic* relationship denoted by an $=$ symbol. A deterministic relationship means the LHS node’s value is determined with zero uncertainty as a function of its inputs. $LHS \sim \textrm{Deterministic function}$

-   **Right-hand side (RHS)**: either a known probability distribution (i.e., probability mass or density function) governing the node’s random outcome (e.g. $\textrm{Bernoulli}(\theta)$) or a deterministic mathematical function defining the node’s value (e.g. $x + y$ ). In both cases, any parameters/variables/inputs in the RHS must be represented by a parent node to the current node in your model.

To model subsequent nodes, consider any parameters or variables on the right-hand side of the mathematical lines that remain undefined; all of these undefined parameters must be defined on the LHS of a parent node. Hence, for $x \sim \textrm{Bernoulli}(theta)$, $\theta$ should be (and is) the LHS of a parent node of $x$.

Now to have a complete statistical model, we need a recipe for how all the random variables of interest can be generated. Previously, when doing Bayes rule calculations by hand, we considered a recipe for with only two possible values. However, as we move towards using the computer to do Bayesian inference for us, we now consider all of the possibilities - any value between 0 and 1.

The generative DAG above introduces us to the uniform distribution. The $\textrm{uniform}(a, b)$ distribution is a two-parameter probability distribution with the property that all real numbers between $a$ and $b$ are generated with equal probability. Hence, for our application where represents a probability restricted to be between 0 and 1, we use $\textrm{uniform}(0, 1)$ to represent our consideration of the infinite possible values of success probability ranging from 0% to 100%.

Since all nodes/variables ($x$ and $\theta$) have a mathematical line, and all RHS parameters/variables are defined, the DAG above represents a generative model. The top line of each oval is a meaningful real-world description of the random variable and the bottom line gives the math.

Notice that the generative DAG is a recipe for simulating or generating a single sample/realization from the joint distribution - read it from top-to-bottom: 1) first, simulate a random sample value of $\theta$, say the realization is 10%, and then 2) use that sample value to simulate $x$, which assuming $\theta = 10\%$, is either 0 with 90% probability or 1 with 10% probability.

For illustrating the simulation aspect of a generative DAG, we can use simple R functions to get a joint realization of the two variables. We model $\theta$ as $\textrm{uniform}(0, 1)$ and $x$ as $\textrm{bernoulli}(\theta)$ computationally using the functions `runif` and `rbern` (i.e. `rfoo` for the uniform distribution and Bernoulli distribution, respectively). Hence, a random sample from the joint distribution of $\Theta$ and $X$ can be generated with the following code:

```{r}
set.seed(1234)
# generate random theta: n is # of samples we want
theta = runif(n=1,min=0,max=1)
theta # print value

# generate random X
x = rbern(n = 1, prob = theta)
x # print value
```

For the particular model above, the recipe picked a random $\theta$ of 11.4% and then, using that small probability of success, generated a random $x$ with 11.4% chance of giving a one. Unsurprisingly, the random sample from $x$ was zero.

A generative models is a mathematical abstraction of a real-world data generating process (DGP); they represent, in the formal language of probability theory, our knowledge of the DGP. We will use these abstractions and Bayesian inference to combine prior information in the form of a generative DAG with observed data. The combination will yield us a posterior distribution; a joint distribution over our RV’s of interest that we can use to answer questions and generate insight. For most generative DAGs, Bayes rule is not analytically tractable (i.e. it can’t be reduced to simple math), and we need to define a computational model using a probabilistic programming language.

### Representing observed data with fill and a plate

A generative DAG represents a family of recipes (i.e., hypothesis space, where each hypothesis/recipe is indexed by a parameter value or combination of parameter values) for simulating one real-world observation. We can then use data to inform us about which of the recipes seem more consistent with the data. For example, we know that observing cognition increase not in one but three patients with AD after drug treatment would be less consistent with the recipe based on the pessimist model $\theta = 20\%$ and more consistent with the recipe based on the optimist model $\theta = 70\%$. Thus, reallocating plausibility in light of this type of data becomes a milestone during our data analysis.

In the generative DAG above, the reallocation of probability in light of the data will be done in such a way as to give more plausibility to $\theta$ values consistent with the observations and less plausibility to the other $\theta$ values.

```{r}
dag_create() |>
  dag_node(
    descr = "Cognition increases",
    label = "x",
    rhs = bernoulli(theta),
    obs = TRUE
    ) |>
  dag_node(
    descr = "Success probability",
    label = "theta",
    child = "x",
    rhs = uniform(0,1)) |>
  dag_render()
```

```{r}
dag_create() |>
  dag_node(
    descr = "Cognition increases",
    label = "x",
    rhs = bernoulli(theta),
    obs = TRUE) |>
  dag_plate(
    descr = "Observation",
    label = "i",
    nodeLabels = "x") |>
  dag_node(
    descr = "Success probability",
    label = "theta",
    child = "x",
    rhs = uniform(0,1)) |>
  dag_render()
```

In the generative DAG above, there are some additional elements. *Observed* data - in contrast to latent parameters/variables or unobserved data - gets represented by using fill shading of the ovals (e.g. the darker fill of *Cognition increases*). Also, rectangles called *plates* are used to indicate repetition of the enclosed random variable (or group/subgraph of random variables). In this case, the plate indicates we have observed multiple \[modeled as independent in the corresponding statistical model\] outcomes/realizations of $x$ from treating more than one patient with the drug.

Text in the lower right-hand corner of the plate indicates how variables inside the plate are repeated and indexed. In this case, there will be one realization of $x$ for each observation. The letter $i$ represents a short-hand label used to index the observations. For example, `x[i]` is the $i^{\text{th}}$ observation of `x` and therefore, `x[2]` would the $2^{\text{nd}}$ observation of `x`. The `[3]` in the lower-right hand corner represents the number of repetitions of the RV, in this case there are 3 observations of stores: `x[1]`, `x[2]`, and `x[3]`.

Since the $\theta$ node lacks a plate, the generative recipe implied by this generative DAG calls for just one realization of $\theta$ to be used in generating all 3 observations of $x$. In other words, our recipe assumes there is just one "true" $\theta$ value and all observations are Bernoulli trials with the same $\theta$ value. The question we will soon answer computationally is “how to reallocate our plausibility among all possible "true" values of $\theta$ given that we have observed 3 observations of $x$?”

More rigorous mathematical notation and definitions can be found in the lucid recommendations of Michael Betancourt. See his work *Towards a Principled Bayesian Workflow (Rstan)* for a more in-depth treatment of how generative models and prior distributions are only models of real-world processes. <https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html>. Additionally, Betancourt’s *Generative Modelling* (<https://betanalpha.github.io/assets/case_studies/generative_modeling.html>) is excellent in explaining how models with narrative interpretations tell a set of stories on how data are generated. For us, we will combine our generative DAGs with observed data to refine our beliefs about which stories (i.e. which parameters) seem more plausible than others.

## Computational Bayesian inference workflows

This section illustrates how to use the `causact` package to specify and run computational Bayesian inference workflows. The package is an easy-to-use, intuitive, and visual interface to `numpyro`. In fact, it automates the creation of `numpyro` code based on a user creating a generative DAG. `causact` advocates and enables generative DAGs to serve as a business analytics workflow (BAW) platform upon which to launch business discussions, create statistical models, and automate computational Bayesian inference.

### Getting started with `dag_foo()` functions

In the code below, the two lines beginning with `dag_` output R list objects consisting of six data frames. Let’s not worry too much about the details, but notice that one of the data frames is for storing node information (i.e. `nodesDF`) and one for edge information (i.e. `edgesDF`).

```{r}
## returns a list of data frames to store DAG info
dag_create()
```

```{r}
## adds a node to the list with given description
dag_create() %>% dag_node("BernoulliRV")
```

The function dag_create() is used to create an empty list object that we will refer to as a *causact_graph*. Subsequently functions like `dag_node()` and `dag_edge()` will take a *causact_graph* object as input, modify them, and provide a *causact_graph* object as output. This feature allows for the chaining (i.e. using `%>%`) of these functions to build up your observational model. Once done building with `dag_create()`, `dag_node()`, and `dag_edge()`, a call to the `dag_render()` function outputs a visual depiction of your *causact graph*.

```{r}
dag_create() %>% 
  dag_node("BernoulliRV") %>% 
  dag_render()   ## visualize graph
```

### Four steps to creating graphical models

Let’s use the `dag_foo()` functions to make a generative DAG with Bernoulli data, a uniform prior, and some observed data:

$$
\begin{aligned}X &\sim \textrm{Bernoulli}(\theta) \\\theta &\sim \textrm{uniform}(0,1)\end{aligned}
$$

Assume two successes and one failure such that:

$$
x_1 = 1, x_2 = 1, x_3 = 0
$$

The following code uses the `descr` and `label` arguments of the `dag_node()` function to, respectively, provide a long label for your random variable that is easily understood by business stakeholders and a short label that is more for notational/mathematical convenience:

```{r}
dag_create() %>%  # just get one node to show
  dag_node(descr = "Cognition increases", label = "x") %>%
  dag_render()
```

Since, we actually observe this node as data, we use the `data` argument of the `dag_node()` function to specify the observed values:

```{r}
dag_create() %>%  # make it an observed node by adding data
  dag_node(descr = "Cognition increases", label = "x", 
           data = c(1,1,0)) %>%
  dag_render()
```

The node’s darker fill is automated because of the supplied observed data and the `[3]` means there are three observed realizations in the supplied data vector. You could also use a plate to represent the multiple realizations, but I recommend plate creation to be the last step in creating generative DAGs via the `causact` package.

One goal we often have is to take a sample of data and find plausible parameters within a distribution family (e.g. normal, gamma, etc.) governing how that data might have been generated - at its core, Bayesian inference is a search for plausible parameters of a specified generative DAG. To specify the distribution family governing the observed data, use the `rhs` argument of the `dag_node()` function.

::: callout-note
`rhs` stands for right-hand side. The `rhs` of a node is reserved for indicating how the node is generated as a function of its parent nodes’ values. Alternatively, when not used as a distribution specification, the `rhs` is an algebraic expression or other function of data converting parent node values into a realization of the node’s value.
:::

There is an implicit assumption that all unknown variables will eventually be defined by parent nodes - the `causact` package will not check for this. In the case below, we are specifying a parameter `theta` that will need to eventually be added as a parent node.

```{r}
dag_create() %>%  # specify data generating distribution
  dag_node(descr = "Cognition increases", label = "x", 
           rhs = bernoulli(theta),  ##add distribution 
           data = c(1,1,0)) %>%
  dag_render()
```

Note, a random variable’s distribution must be part of `causact`. The full list of `causact` distributions can be found by running the following line:

```{r}
#| eval: false
?causact::distributions
```

For each unknown argument remaining on the `rhs` of any nodes in your causact graph, one must eventually All unknown rhs arguments for nodes must be defined prior to running any Bayesain computations.define how the unknown argument can be generated. In our data node above, the only unknown argument on the `rhs` is `theta`. We define the generative model for `theta` by making an additional node representing its value:

```{r}
dag_create() %>%  
  dag_node(descr = "Cognition increases", 
           label = "x",
           rhs = bernoulli(theta), # no quotation marks
           data = c(1,1,0)) %>%
  dag_node(descr = "Success probability", 
           label = "theta",  # labels needs quotes
           rhs = uniform(0,1)) %>%
  dag_render()
```

Notice, the parameters of the `rhs` distribution for `theta` are not unknown - they are actually constants (i.e. 0 and 1) and hence, no additional parent nodes are required to be created. Take note that `theta` on the `rhs` for the *Cognition increases* node does not require quotes as it refers to an R object; specifically, this refers to the R object created by:

```{r}
#| eval: false
dag_node(descr = "Success probability", 
         label = "theta",  # label needs quotes
         rhs = uniform(0,1))
```

where the `theta` object is given its name by the `label = "theta"` argument; when creating node labels via this argument, quotes are typically required.

Without a link between `theta` and `x`, one is not creating a properly factorizable directed acyclic graph as is required for specifying a joint distribution. Using the `child` argument of `dag_node`, one can now create the required link between `theta` as a parent node and `theta` as an argument to the distribution of its child node `x`:

```{r}
dag_create() %>%  # connect parent to child
  dag_node(descr = "Cognition increases", label = "x",
           rhs = bernoulli(theta),
           data = c(1,1,0)) %>%
  dag_node(descr = "Success probability", label = "theta",
           rhs = uniform(0,1),
           child = "x") %>%  ## ADDED LINE TO CREATE EDGE
  dag_render()
```

::: callout-note
When using the child argument, please ensure the child was previously defined as a node. Generative DAGs are designed to be built from bottom-to-top reflecting the way a business analyst would create these DAGs. Note that computer code for Bayesian inference, like numpyro code, requires the exact opposite order - parent nodes get defined prior to their children. One way the causact package accelerates the BAW is by facilitating a bottom-up workflow that can be automatically translated to top-down computer code.
:::

For more complicated modelling, repeat this process of adding parent nodes until there are no uncertain parameters on the right hand side of the top nodes.

### Running Bayesian inference on a generative DAG

Now we use `dag_numpyro()` function to get a posterior distribution based solely on the `causact_graph` that we just made. A call to `dag_numpyro()` expects a `causact_graph` as the first argument, so make sure you are not passing the output of the `dag_render()` function by mistake. Use `dag_render()` to get a picture and `dag_numpyro()` for Bayesian inference - just do not mix them in the same chain of functions. The second argument of `dag_numpyro()` is the `mcmc` argument and its default value is `TRUE`. When `mcmc = FALSE`, `dag_numpyro(mcmc = FALSE)` gives a print-out of the `numpyro` code that would be run by `causact` to get a posterior without running the code. You can cut and paste this code into a new R-script if you wish and run it there (ESPECIALLY USEFUL FOR DEBUGGING WHEN YOU GET AN ERROR AT THIS STAGE). Alternatively and preferably, the `numpyro` code should run automatically in the background by setting `mcmc = TRUE` or simply omitting this argument because it is the default value. Usage is shown below:

```{r}
# running Bayesian inference
# remove dag_render() and save graph object
graph = dag_create() %>%
  dag_node(descr = "Cognition increases", 
           label = "x",
           rhs = bernoulli(theta),
           data = c(1,1,0)) %>%
  dag_node(descr = "Success probability", 
           label = "theta",
           rhs = uniform(0,1),
           child = "x")
# pass graph to dag_numpyro
draws = graph %>% dag_numpyro()
```

After some time, during which `numpyro` computes our posterior Bayesian distribution, we get that distribution as representative sample in an object I typically name `draws` - a data frame of posterior draws that is now useful for computation and plotting. For a quick visual-check that all went well, pass the `draws` data frame to the `dagp_plot()` function to get a quick look at the credible (posterior) values of `theta`:

```{r}
draws %>% dagp_plot()  # eyeball P(theta>0.65)
```

the lighter fill indicates a 90% percentile interval where 5% of plausible values are excluded from the left- and right-sides of the colored interval. Consider this range your credilble values for `theta`; hence, our posterior belief is that `theta` is somewhere in the 27% to 90% range. The darker fill within the colored interval indicates a 10% percentile interval; hence, the most likely values of `theta` are centered around 60%. For more customized graphs, please use `bayesplot` or `ggplot2` with `ggdist` with `draws` as the input data.

### Investigating the posterior distribution

The object we named `draws` is our posterior distribution after `numpyro` automated Bayes rule for us. The posterior distribution is expressed as a representative sample of all unobserved nodes/parameters; it is not a named probability distribution. Each row of `draws` is a single individual sample or realization of the posterior distribution. Each row is referred to as a **draw** from the posterior.

To see a representative sample of the posterior distribution, we access the R object, `draws`, created above. The `theta` column of `draws` contains our representative sample of the posterior distribution. In this case, that representative sample includes 4,000 samples of `theta`. Recall that our prior belief about `theta`, the probability of success, was uniformly distributed between 0 and 1 - all values equally likely. Now, after observing 2 successes and 1 failure, our plausibility beliefs should favor $\theta$ values away from 0 and 1 and more towards middle values as we learned both success and failure are possible; but we let Bayes rule (via `numpyro`) do this plausibility updating for us.

```{r}
head(draws)
```

Using `bayesplot`, we can visualize how plausibility was reallocated from our uniform prior of $\theta$ in light of the observed data; in other words, we can see the posterior distribution for $\theta$ after observing 2 out of 3 stores increase sales:

```{r}
mcmc_areas(draws)
```

Notice how values of $\theta$ are no longer uniformly distributed; values from 50% to 75% are represented more frequently.

Some other takeaways from the plot regarding plausibility reallocation:

-   Low values of $\theta$ are now deemed less plausible; two out of three successes is simply inconsistent with low values of $\theta$.

-   Very high values of $\theta$ are less plausbile; one failure in three tries is not likely to occur if $\theta$ were to be super-high.

-   Our best guess of $\theta$ went from 0.5 (i.e. the mean of theta when uniformly distributed) to `mean(draws$theta) =` 0.60.

-   We are still very unsure about the "true" value of $\theta$ after only 3 observations. We can see this by looking at quantiles of the posterior distribution suggesting a 90% percentile interval from 0.25 to 0.91:

```{r}
summarize_draws(draws)
```

This continued large band of uncertainty is a good thing. We only have three data points, we should not be very confident in our point estimate of $\theta$. If we want a tighter interval of uncertainty, then we simply need to get more data.

### Making probabilistic statements with indicator functions

With a representative sample of the posterior joint distribution available to us, namely `draws`, we can expand on the strategies of the “Joint Distributions Tell Us Everything” section to make probabilistic statements from representative samples. For example, we might be curious to know $P(\theta \gt 50\%)$. Using indicator functions, simple functions that equal 1 when a criteria is met and 0 when it is not, we can get our estimate of $P(\theta \gt 50\%)$:

```{r}
## estimate posterior probability that theta > 50%
draws %>%
  mutate(indicatorFlag = 
           ifelse(theta > 0.5, TRUE, FALSE)) %>%
  summarize(pctOfThetaVals = mean(indicatorFlag))
```

We would then state that “the probability that $\theta$ is greater than 50% is approximately 0.70”.

Similarly, we can answer more complicated queries. For example, what is the $P(40\% \lt \theta \lt 50\%)$:

```{r}
## estimate posterior probability that 40% < theta < 60%
draws %>%
  mutate(indicatorFlag = 
           ifelse(theta > 0.4 & theta < 0.6, 
                  TRUE, FALSE)) %>%
  summarize(pctOfThetaVals = mean(indicatorFlag))
```

**The power of this workflow cannot be overstated** and you will use it frequently for making probabilistic statements. Statements that will come in handy when it is time to use data to inform decision making under uncertainty.

A more descriptive output, possibly used for plotting purposes, is shown here:

```{r}
draws %>%
  mutate(indicatorFlag = 
           ifelse(theta > 0.5,
                  "theta > 0.5",
                  "theta <= 0.5")) %>%
  group_by(indicatorFlag) %>%
  summarize(countInstances = n()) %>%
  mutate(percentageOfInstances = 
           countInstances / sum(countInstances))
```

Why does this work? Let’s delve into the math for a moment just to see why.

An indicator function, denoted $1_A$ maps all values of a representative sample $X$ to either 0 or 1 depending on whether the values satisfy criteria to be in some set we label as $A$. For example, assume $A = \{x \ge 0.5\}$ is math set notation for all values in representative sample $X$ such that the draw satisfies $x \ge 0.5$. Then, the formal definition of an indicator function, denoted $1_A$, maps elements in $X$ to either 0 or 1 depending on whether they are in $A$:

$$
1_{A} \equiv\begin{cases}  1, & \textrm{if } x \in A\\  0, & \textrm{if } x \notin A\end{cases}
$$

Now, for the key math trick, known as the *fundamental bridge*. The probability of an event is the expected value (i.e. mean) of its indicator random variable. Mathematically,

$$
P(A) = \mathbb{E}[1_{A}]
$$

which is true since $\mathbb{E}[1_{A}] = 1 \times P(A) + 0 \times P(\bar{A}) = P(A)$ where $\bar A$ denotes not in set $A$. So, using this formula we can make probabilistic statements about a realization $x$ meeting the criteria to be in set $A$. Assuming $J$ draws in our data frame, each draw labelled $j$, then we estimate $P(A)$ by taking the average value of an indicator function over the $J$ draws:

$$
P(A) = \mathbb{E}[1_{A}] \approx \frac{1}{J} \sum_{j=1}^J 1_{x_j \in A}
$$

And despite the heavy math notation, your intuition can guide you in applying this formula. For example, imagine we have a representative sample of $X = [1,4,3,2,5]$ and we want to estimate $P(X \ge 3)$. You could answer this just by looking and say $\frac{3}{5}$ or 3 out of 5 chances for $x \ge 3$. Applying the formula, which we could easily do with code, is shown mathematically here:

$$
P(x \geq 3) = \mathbb{E}[1_{x \geq 3}] \approx \frac{1}{5} \sum_{j=1}^5 1_{x_j \geq 3} = \frac{1}{5} \times (0+1+1+0+1) = \frac{3}{5} = 0.6
$$

We have a probabilistic statement: “the probability that $x$ is greater than or equal to 3 is 0.6”.

### Credit card example

```{r}
graph = dag_create() %>%
  dag_node("Get Card","x",
           rhs = bernoulli(theta),
           data = carModelDF$getCard) %>%
  dag_node("Signup Probability","theta",
           rhs = uniform(0,1),
           child = "x") %>%
  dag_plate("Car Model", "y",  
            data = carModelDF$carModel,  
            nodeLabels = "theta", 
            addDataNode = TRUE)  %>%
  dag_plate("Observations", "i",
            nodeLabels = c("x","y"))
```

```{r}
graph %>% dag_render()
```

```{r}
draws = graph %>% dag_numpyro()
```

```{r}
draws %>% dagp_plot()
```
